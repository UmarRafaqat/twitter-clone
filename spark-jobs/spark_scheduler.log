2025-05-11 15:26:09,490 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:26:12,973 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:26:15,922 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:26:18,380 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:26:21,697 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:26:24,385 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:26:28,528 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:26:37,277 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:26:55,864 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:27:23,478 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:28:22,632 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:29:30,153 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:30:37,431 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:31:44,840 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:32:51,693 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:33:59,402 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:35:06,667 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:36:13,902 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:37:21,160 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:38:28,574 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:39:36,032 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:40:43,416 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:41:50,641 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:42:58,051 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:44:05,605 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:45:13,165 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:46:20,683 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:47:28,328 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:48:35,903 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:49:44,991 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:50:50,711 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:51:58,140 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:53:05,534 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:54:12,803 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:55:19,957 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:56:27,152 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:57:34,408 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:58:41,657 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 15:59:48,894 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:00:56,549 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:02:03,461 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:03:10,934 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:04:18,124 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:05:25,321 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:06:32,675 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:07:39,891 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:08:47,118 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:09:54,793 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:11:05,594 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:12:12,940 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:13:20,323 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:14:28,443 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:15:36,387 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:16:44,702 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:17:52,032 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:18:59,263 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:20:06,653 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:21:14,245 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:22:21,609 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:23:29,114 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:24:36,734 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:25:43,983 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:26:51,200 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:27:58,463 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:29:05,753 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:30:12,972 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:31:17,923 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:32:27,279 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:33:34,942 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:34:42,346 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:35:49,646 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:36:56,966 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:38:04,386 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:39:11,739 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:40:19,023 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:41:26,192 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:42:33,533 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:43:42,466 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:44:48,102 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:45:55,293 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:47:02,655 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:48:09,890 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:49:17,206 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:50:25,047 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:51:32,545 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:52:39,888 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:53:47,194 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:54:54,642 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:56:05,246 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:57:12,459 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:58:19,743 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 16:59:26,961 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:00:34,194 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:01:41,688 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:02:49,346 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:03:56,854 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:05:04,079 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:06:11,517 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:07:18,848 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:08:26,223 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:09:33,551 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:10:40,989 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:11:48,327 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:12:55,788 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:14:03,371 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:15:11,094 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:16:18,410 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:17:25,926 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:18:33,416 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:19:40,742 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:20:47,949 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:21:55,062 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:23:02,626 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:24:10,263 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:25:17,582 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:26:25,337 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:27:32,758 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:28:39,991 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:29:47,233 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:30:54,433 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:32:01,652 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:33:08,208 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:34:15,793 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:35:23,150 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:36:31,042 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:37:37,508 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:38:44,742 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:39:51,971 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-11 17:40:59,065 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 12:52:37,683 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 12:52:37,684 - SparkScheduler - INFO - Waiting for services to be ready...
2025-05-12 12:53:07,721 - SparkScheduler - INFO - Setting up job schedules
2025-05-12 12:53:07,722 - SparkScheduler - INFO - Scheduled trend_analysis to run every 30 minutes
2025-05-12 12:53:07,723 - SparkScheduler - INFO - Scheduled user_recommender to run every 3 hours
2025-05-12 12:53:07,725 - SparkScheduler - INFO - Scheduled content_analyzer to run daily at 02:00
2025-05-12 12:53:07,725 - SparkScheduler - INFO - All jobs scheduled
2025-05-12 12:53:07,726 - SparkScheduler - INFO - Running initial job executions...
2025-05-12 12:53:07,727 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 12:53:07,727 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 12:53:16,917 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 12:56:31,232 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 12:56:31,234 - SparkScheduler - INFO - Waiting for services to be ready...
2025-05-12 12:57:03,998 - SparkScheduler - INFO - Setting up job schedules
2025-05-12 12:57:04,004 - SparkScheduler - INFO - Scheduled trend_analysis to run every 30 minutes
2025-05-12 12:57:04,004 - SparkScheduler - INFO - Scheduled user_recommender to run every 3 hours
2025-05-12 12:57:04,007 - SparkScheduler - INFO - Scheduled content_analyzer to run daily at 02:00
2025-05-12 12:57:04,008 - SparkScheduler - INFO - All jobs scheduled
2025-05-12 12:57:04,009 - SparkScheduler - INFO - Running initial job executions...
2025-05-12 12:57:04,009 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 12:57:04,010 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 12:57:07,890 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 12:57:17,949 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 12:57:20,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:20 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 12:57:20,762 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:20 INFO ResourceUtils: ==============================================================
2025-05-12 12:57:20,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:20 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 12:57:20,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:20 INFO ResourceUtils: ==============================================================
2025-05-12 12:57:20,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:20 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 12:57:20,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 12:57:20,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:20 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 12:57:20,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 12:57:21,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:21 INFO SecurityManager: Changing view acls to: spark
2025-05-12 12:57:21,095 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:21 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 12:57:21,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:21 INFO SecurityManager: Changing view acls groups to:
2025-05-12 12:57:21,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:21 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 12:57:21,108 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 12:57:21,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:21 INFO Utils: Successfully started service 'sparkDriver' on port 39045.
2025-05-12 12:57:22,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:22 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 12:57:22,193 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:22 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 12:57:22,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 12:57:22,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 12:57:22,272 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 12:57:22,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ddea15f2-865b-400e-af00-fcea7ab8e12d
2025-05-12 12:57:22,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 12:57:22,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:22 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 12:57:23,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 12:57:23,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://80006b2cb50e:39045/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747054640681
2025-05-12 12:57:23,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://80006b2cb50e:39045/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747054640681
2025-05-12 12:57:23,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://80006b2cb50e:39045/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747054640681
2025-05-12 12:57:23,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://80006b2cb50e:39045/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747054640681
2025-05-12 12:57:23,129 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://80006b2cb50e:39045/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747054640681
2025-05-12 12:57:23,133 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-070ff2ca-e592-4542-b0e1-907cb5bf76c0/userFiles-8d4759a2-fa85-4d4f-9563-72ba9cbf59ad/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 12:57:23,177 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://80006b2cb50e:39045/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747054640681
2025-05-12 12:57:23,179 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-070ff2ca-e592-4542-b0e1-907cb5bf76c0/userFiles-8d4759a2-fa85-4d4f-9563-72ba9cbf59ad/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 12:57:23,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://80006b2cb50e:39045/files/org.mongodb_bson-4.0.5.jar with timestamp 1747054640681
2025-05-12 12:57:23,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-070ff2ca-e592-4542-b0e1-907cb5bf76c0/userFiles-8d4759a2-fa85-4d4f-9563-72ba9cbf59ad/org.mongodb_bson-4.0.5.jar
2025-05-12 12:57:23,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://80006b2cb50e:39045/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747054640681
2025-05-12 12:57:23,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-070ff2ca-e592-4542-b0e1-907cb5bf76c0/userFiles-8d4759a2-fa85-4d4f-9563-72ba9cbf59ad/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 12:57:23,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 12:57:23,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.6:7077 after 53 ms (0 ms spent in bootstraps)
2025-05-12 12:57:23,830 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512125723-0000
2025-05-12 12:57:23,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34551.
2025-05-12 12:57:23,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO NettyBlockTransferService: Server created on 80006b2cb50e:34551
2025-05-12 12:57:23,855 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 12:57:23,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 80006b2cb50e, 34551, None)
2025-05-12 12:57:23,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO BlockManagerMasterEndpoint: Registering block manager 80006b2cb50e:34551 with 434.4 MiB RAM, BlockManagerId(driver, 80006b2cb50e, 34551, None)
2025-05-12 12:57:23,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 80006b2cb50e, 34551, None)
2025-05-12 12:57:23,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 80006b2cb50e, 34551, None)
2025-05-12 12:57:23,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512125723-0000/0 on worker-20250512125632-172.23.0.8-35921 (172.23.0.8:35921) with 2 core(s)
2025-05-12 12:57:23,919 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512125723-0000/0 on hostPort 172.23.0.8:35921 with 2 core(s), 1024.0 MiB RAM
2025-05-12 12:57:24,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512125723-0000/0 is now RUNNING
2025-05-12 12:57:24,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 12:57:25,764 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 12:57:25,790 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 12:57:25,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:25 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 12:57:29,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 12:57:29,544 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 12:57:29,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 80006b2cb50e:34551 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 12:57:29,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:29 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 12:57:31,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:58742) with ID 0,  ResourceProfileId 0
2025-05-12 12:57:31,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:31 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:38399 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 38399, None)
2025-05-12 12:57:35,337 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:35 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:35,425 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:35 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:35,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:35 INFO cluster: Exception in monitor thread while connecting to server mongodb:27017
2025-05-12 12:57:35,446 - SparkScheduler - INFO - [trend_analysis] com.mongodb.MongoSocketOpenException: Exception opening socket
2025-05-12 12:57:35,447 - SparkScheduler - INFO - [trend_analysis] at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:70)
2025-05-12 12:57:35,448 - SparkScheduler - INFO - [trend_analysis] at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:127)
2025-05-12 12:57:35,449 - SparkScheduler - INFO - [trend_analysis] at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:117)
2025-05-12 12:57:35,450 - SparkScheduler - INFO - [trend_analysis] at java.base/java.lang.Thread.run(Thread.java:840)
2025-05-12 12:57:35,453 - SparkScheduler - INFO - [trend_analysis] Caused by: java.net.ConnectException: Connection refused
2025-05-12 12:57:35,454 - SparkScheduler - INFO - [trend_analysis] at java.base/sun.nio.ch.Net.pollConnect(Native Method)
2025-05-12 12:57:35,456 - SparkScheduler - INFO - [trend_analysis] at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
2025-05-12 12:57:35,457 - SparkScheduler - INFO - [trend_analysis] at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
2025-05-12 12:57:35,458 - SparkScheduler - INFO - [trend_analysis] at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
2025-05-12 12:57:35,458 - SparkScheduler - INFO - [trend_analysis] at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
2025-05-12 12:57:35,459 - SparkScheduler - INFO - [trend_analysis] at java.base/java.net.Socket.connect(Socket.java:633)
2025-05-12 12:57:35,460 - SparkScheduler - INFO - [trend_analysis] at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:63)
2025-05-12 12:57:35,460 - SparkScheduler - INFO - [trend_analysis] at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:79)
2025-05-12 12:57:35,461 - SparkScheduler - INFO - [trend_analysis] at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:65)
2025-05-12 12:57:35,462 - SparkScheduler - INFO - [trend_analysis] ... 3 more
2025-05-12 12:57:35,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:36,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:36 INFO connection: Opened connection [connectionId{localValue:3, serverValue:1}] to mongodb:27017
2025-05-12 12:57:36,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=14265058}
2025-05-12 12:57:36,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:36 INFO connection: Opened connection [connectionId{localValue:4, serverValue:2}] to mongodb:27017
2025-05-12 12:57:37,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:37 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 12:57:38,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO CodeGenerator: Code generated in 319.627226 ms
2025-05-12 12:57:38,341 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:38,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:38,345 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO connection: Opened connection [connectionId{localValue:5, serverValue:3}] to mongodb:27017
2025-05-12 12:57:38,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1358620}
2025-05-12 12:57:38,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO connection: Opened connection [connectionId{localValue:6, serverValue:4}] to mongodb:27017
2025-05-12 12:57:38,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 12:57:38,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 12:57:38,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 12:57:38,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 12:57:38,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:38,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 12:57:38,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 12:57:38,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 12:57:38,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 80006b2cb50e:34551 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:38,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:38,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:38,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 12:57:38,655 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 12:57:39,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:38399 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:40,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:38399 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 12:57:40,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1940 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:40,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 12:57:40,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:40 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 2.081 s
2025-05-12 12:57:40,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:40 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 12:57:40,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:40 INFO DAGScheduler: running: Set()
2025-05-12 12:57:40,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:40 INFO DAGScheduler: waiting: Set()
2025-05-12 12:57:40,600 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:40 INFO DAGScheduler: failed: Set()
2025-05-12 12:57:44,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 12:57:44,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 0.004096 s
2025-05-12 12:57:44,111 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 12:57:44,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 12:57:44,141 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 80006b2cb50e:34551 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 12:57:44,143 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO SparkContext: Created broadcast 2 from broadcast at MongoSpark.scala:530
2025-05-12 12:57:44,145 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:44,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO connection: Closed connection [connectionId{localValue:4, serverValue:2}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:44,246 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:44,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:44,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:44,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO connection: Opened connection [connectionId{localValue:7, serverValue:7}] to mongodb:27017
2025-05-12 12:57:44,259 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2332431}
2025-05-12 12:57:44,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 80006b2cb50e:34551 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:44,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO connection: Opened connection [connectionId{localValue:8, serverValue:8}] to mongodb:27017
2025-05-12 12:57:44,267 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:38399 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:44,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 12:57:44,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO CodeGenerator: Code generated in 76.855977 ms
2025-05-12 12:57:44,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Registering RDD 17 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 12:57:44,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 12:57:44,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (rdd at MongoSpark.scala:169)
2025-05-12 12:57:44,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 12:57:44,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:44,562 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[17] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 12:57:44,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 12:57:44,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 12:57:44,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 80006b2cb50e:34551 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:44,588 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:44,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[17] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:44,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
2025-05-12 12:57:44,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 80006b2cb50e:34551 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 12:57:44,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:44,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 12:57:44,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO connection: Closed connection [connectionId{localValue:8, serverValue:8}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:44,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:44,602 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO connection: Closed connection [connectionId{localValue:6, serverValue:4}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:44,659 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.8:38399 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:44,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 241 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:44,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
2025-05-12 12:57:44,844 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: ShuffleMapStage 1 (rdd at MongoSpark.scala:169) finished in 0.274 s
2025-05-12 12:57:44,845 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 12:57:44,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: running: Set()
2025-05-12 12:57:44,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: waiting: Set()
2025-05-12 12:57:44,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: failed: Set()
2025-05-12 12:57:44,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 12:57:44,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.000481 s
2025-05-12 12:57:44,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 12:57:44,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 12:57:44,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 80006b2cb50e:34551 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 12:57:44,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO SparkContext: Created broadcast 4 from broadcast at MongoSpark.scala:530
2025-05-12 12:57:44,924 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 80006b2cb50e:34551 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:44,935 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.23.0.8:38399 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:44,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:44,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:45,000 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:44 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:45,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Opened connection [connectionId{localValue:9, serverValue:9}] to mongodb:27017
2025-05-12 12:57:45,005 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1247693}
2025-05-12 12:57:45,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Opened connection [connectionId{localValue:10, serverValue:10}] to mongodb:27017
2025-05-12 12:57:45,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:45,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Closed connection [connectionId{localValue:10, serverValue:10}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:45,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 12:57:45,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 12:57:45,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO CodeGenerator: Code generated in 100.105184 ms
2025-05-12 12:57:45,574 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:45,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:45,577 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:45,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Opened connection [connectionId{localValue:11, serverValue:11}] to mongodb:27017
2025-05-12 12:57:45,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1404692}
2025-05-12 12:57:45,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Opened connection [connectionId{localValue:12, serverValue:12}] to mongodb:27017
2025-05-12 12:57:45,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:45,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Closed connection [connectionId{localValue:12, serverValue:12}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:45,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:45,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:45,595 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:45,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Opened connection [connectionId{localValue:13, serverValue:13}] to mongodb:27017
2025-05-12 12:57:45,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2123732}
2025-05-12 12:57:45,606 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Opened connection [connectionId{localValue:14, serverValue:14}] to mongodb:27017
2025-05-12 12:57:45,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:45,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO connection: Closed connection [connectionId{localValue:14, serverValue:14}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:45,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:45,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:45,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:45,628 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO DAGScheduler: Registering RDD 29 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 12:57:45,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 12:57:45,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (rdd at MongoSpark.scala:169)
2025-05-12 12:57:45,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 12:57:45,631 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:45,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[29] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 12:57:45,641 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 12:57:45,661 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 12:57:45,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 80006b2cb50e:34551 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:45,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:45,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 80006b2cb50e:34551 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 12:57:45,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[29] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:45,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 12:57:45,680 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 12:57:45,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:38399 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:46,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 327 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:46,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 12:57:46,008 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: ShuffleMapStage 2 (rdd at MongoSpark.scala:169) finished in 0.373 s
2025-05-12 12:57:46,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 12:57:46,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: running: Set()
2025-05-12 12:57:46,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: waiting: Set()
2025-05-12 12:57:46,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: failed: Set()
2025-05-12 12:57:46,041 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 12:57:46,044 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.000602 s
2025-05-12 12:57:46,049 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 12:57:46,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 12:57:46,076 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 80006b2cb50e:34551 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 12:57:46,079 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 12:57:46,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 80006b2cb50e:34551 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:46,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:38399 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:46,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 12:57:46,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:46,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:46,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:46,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO connection: Opened connection [connectionId{localValue:16, serverValue:16}] to mongodb:27017
2025-05-12 12:57:46,353 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1285896}
2025-05-12 12:57:46,356 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO connection: Opened connection [connectionId{localValue:17, serverValue:17}] to mongodb:27017
2025-05-12 12:57:46,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:46,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO connection: Closed connection [connectionId{localValue:17, serverValue:17}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:46,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:46,361 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:46,362 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:46,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO connection: Opened connection [connectionId{localValue:18, serverValue:18}] to mongodb:27017
2025-05-12 12:57:46,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1564279}
2025-05-12 12:57:46,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO connection: Opened connection [connectionId{localValue:19, serverValue:19}] to mongodb:27017
2025-05-12 12:57:46,380 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:46,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO connection: Closed connection [connectionId{localValue:19, serverValue:19}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:46,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:46,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:46,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:46,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Registering RDD 41 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 12:57:46,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 12:57:46,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 12:57:46,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 12:57:46,392 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:46,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[41] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 12:57:46,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 12:57:46,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 12:57:46,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 80006b2cb50e:34551 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:46,412 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:46,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 80006b2cb50e:34551 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 12:57:46,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[41] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:46,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 12:57:46,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 12:57:46,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:38399 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:46,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 119 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:46,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 12:57:46,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.146 s
2025-05-12 12:57:46,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 12:57:46,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: running: Set()
2025-05-12 12:57:46,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: waiting: Set()
2025-05-12 12:57:46,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: failed: Set()
2025-05-12 12:57:46,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 12:57:46,662 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO CodeGenerator: Code generated in 23.096556 ms
2025-05-12 12:57:46,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO CodeGenerator: Code generated in 29.533903 ms
2025-05-12 12:57:46,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 12:57:46,753 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO CodeGenerator: Code generated in 26.819394 ms
2025-05-12 12:57:46,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 12:57:46,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 12:57:46,854 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 12:57:46,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 12:57:46,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:46,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[46] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 12:57:46,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 12:57:46,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 12:57:46,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 80006b2cb50e:34551 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:46,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 80006b2cb50e:34551 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:46,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:46,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[46] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:46,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 12:57:46,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (172.23.0.8, executor 0, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 12:57:46,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:38399 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:46,964 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:46 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:38399 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:47,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:58742
2025-05-12 12:57:47,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 613 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:47,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 12:57:47,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.643 s
2025-05-12 12:57:47,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 12:57:47,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 12:57:47,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.677172 s
2025-05-12 12:57:47,627 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 12:57:47,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:47,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:47,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:47,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO connection: Opened connection [connectionId{localValue:21, serverValue:21}] to mongodb:27017
2025-05-12 12:57:47,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1261701}
2025-05-12 12:57:47,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO connection: Opened connection [connectionId{localValue:22, serverValue:22}] to mongodb:27017
2025-05-12 12:57:47,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:47,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO connection: Closed connection [connectionId{localValue:22, serverValue:22}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:47,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:47,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:47,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:47,735 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO connection: Opened connection [connectionId{localValue:23, serverValue:23}] to mongodb:27017
2025-05-12 12:57:47,737 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1480310}
2025-05-12 12:57:47,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO connection: Opened connection [connectionId{localValue:24, serverValue:24}] to mongodb:27017
2025-05-12 12:57:47,753 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:47,754 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO connection: Closed connection [connectionId{localValue:24, serverValue:24}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:47,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:47,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:47,759 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:47,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Registering RDD 51 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 12:57:47,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 12:57:47,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 12:57:47,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 12:57:47,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:47,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[51] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 12:57:47,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 47.8 KiB, free 434.3 MiB)
2025-05-12 12:57:47,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 12:57:47,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 80006b2cb50e:34551 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:47,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:47,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[51] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:47,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 12:57:47,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 12:57:47,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 80006b2cb50e:34551 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:47,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:38399 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:47,926 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.23.0.8:38399 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:48,074 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 254 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:48,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 12:57:48,079 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.304 s
2025-05-12 12:57:48,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 12:57:48,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: running: Set()
2025-05-12 12:57:48,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: waiting: Set()
2025-05-12 12:57:48,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: failed: Set()
2025-05-12 12:57:48,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 12:57:48,134 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 12:57:48,184 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 12:57:48,186 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 12:57:48,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 12:57:48,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 12:57:48,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:48,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[56] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 12:57:48,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 12:57:48,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 12:57:48,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 80006b2cb50e:34551 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:48,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:48,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[56] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:48,219 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 12:57:48,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (172.23.0.8, executor 0, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 12:57:48,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 80006b2cb50e:34551 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:48,250 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.23.0.8:38399 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 12:57:48,288 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:38399 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:48,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:58742
2025-05-12 12:57:48,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 170 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:48,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 12:57:48,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.204 s
2025-05-12 12:57:48,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 12:57:48,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 12:57:48,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.217535 s
2025-05-12 12:57:48,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 12:57:48,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 12:57:48,715 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:48,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:48,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:48,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO connection: Opened connection [connectionId{localValue:26, serverValue:26}] to mongodb:27017
2025-05-12 12:57:48,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6325970}
2025-05-12 12:57:48,781 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO connection: Opened connection [connectionId{localValue:27, serverValue:27}] to mongodb:27017
2025-05-12 12:57:48,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:48,787 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO connection: Closed connection [connectionId{localValue:27, serverValue:27}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:48,789 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:48,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:48,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:48,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO connection: Opened connection [connectionId{localValue:28, serverValue:28}] to mongodb:27017
2025-05-12 12:57:48,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1540624}
2025-05-12 12:57:48,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO connection: Opened connection [connectionId{localValue:29, serverValue:29}] to mongodb:27017
2025-05-12 12:57:48,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:48,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO connection: Closed connection [connectionId{localValue:29, serverValue:29}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:48,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:48,831 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:48,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:48,841 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Registering RDD 61 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 12:57:48,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 12:57:48,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 12:57:48,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 12:57:48,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:48,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[61] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 12:57:48,853 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 12:57:48,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 12:57:48,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 80006b2cb50e:34551 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:48,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:48,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[61] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:48,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 12:57:48,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 12:57:48,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:38399 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:48,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 80006b2cb50e:34551 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:49,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:38399 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:49,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 300 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:49,179 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 12:57:49,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.333 s
2025-05-12 12:57:49,184 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 12:57:49,185 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: running: Set()
2025-05-12 12:57:49,186 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: waiting: Set()
2025-05-12 12:57:49,187 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: failed: Set()
2025-05-12 12:57:49,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 12:57:49,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 12:57:49,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO CodeGenerator: Code generated in 137.8803 ms
2025-05-12 12:57:49,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 12:57:49,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 12:57:49,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 12:57:49,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 12:57:49,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:49,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[66] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 12:57:49,536 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 58.7 KiB, free 434.3 MiB)
2025-05-12 12:57:49,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.2 MiB)
2025-05-12 12:57:49,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 80006b2cb50e:34551 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 12:57:49,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:49,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[66] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:49,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 12:57:49,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 80006b2cb50e:34551 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:49,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:38399 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 12:57:49,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (172.23.0.8, executor 0, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 12:57:49,686 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:38399 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 12:57:49,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:58742
2025-05-12 12:57:49,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 283 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:49,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 12:57:49,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.339 s
2025-05-12 12:57:49,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 12:57:49,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 12:57:49,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.364810 s
2025-05-12 12:57:49,879 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 0, 'weekly': 0, 'hourly': 0}
2025-05-12 12:57:49,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 12:57:49,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 12:57:49,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 80006b2cb50e:34551 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 12:57:49,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO SparkContext: Created broadcast 13 from broadcast at MongoSpark.scala:530
2025-05-12 12:57:49,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:49,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:49,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 12:57:49,951 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO connection: Opened connection [connectionId{localValue:31, serverValue:32}] to mongodb:27017
2025-05-12 12:57:49,954 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2037911}
2025-05-12 12:57:49,957 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO connection: Opened connection [connectionId{localValue:32, serverValue:33}] to mongodb:27017
2025-05-12 12:57:49,960 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:49,962 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:49 INFO connection: Closed connection [connectionId{localValue:32, serverValue:33}] to mongodb:27017 because the pool has been closed.
2025-05-12 12:57:50,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 12:57:50,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 12:57:50,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 12:57:50,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO connection: Opened connection [connectionId{localValue:33, serverValue:34}] to mongodb:27017
2025-05-12 12:57:50,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 12:57:50,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 12:57:50,111 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: Final stage: ResultStage 12 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 12:57:50,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 12:57:50,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: Missing parents: List()
2025-05-12 12:57:50,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[71] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 12:57:50,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 12:57:50,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.3 MiB)
2025-05-12 12:57:50,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 80006b2cb50e:34551 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 12:57:50,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 80006b2cb50e:34551 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 12:57:50,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:38399 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 12:57:50,166 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 12:57:50,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[71] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 12:57:50,168 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 12:57:50,173 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 12:57:50,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:38399 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 12:57:50,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:38399 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 12:57:50,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 255 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 12:57:50,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 12:57:50,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: ResultStage 12 (treeAggregate at MongoInferSchema.scala:88) finished in 0.324 s
2025-05-12 12:57:50,431 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 12:57:50,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
2025-05-12 12:57:50,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.332129 s
2025-05-12 12:57:50,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO SparkUI: Stopped Spark web UI at http://80006b2cb50e:4040
2025-05-12 12:57:50,737 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 12:57:50,738 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 12:57:50,781 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 12:57:50,912 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO MemoryStore: MemoryStore cleared
2025-05-12 12:57:50,914 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO BlockManager: BlockManager stopped
2025-05-12 12:57:50,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 12:57:50,959 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 12:57:51,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:51 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 12:57:51,144 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 12:57:51,145 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 12:57:51,150 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 12:57:51,151 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 196, in main
2025-05-12 12:57:51,155 - SparkScheduler - INFO - [trend_analysis] activity_results = analyze_user_activity(spark)
2025-05-12 12:57:51,156 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 116, in analyze_user_activity
2025-05-12 12:57:51,160 - SparkScheduler - INFO - [trend_analysis] .withColumn("date", to_date("created_at"))
2025-05-12 12:57:51,161 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 3036, in withColumn
2025-05-12 12:57:51,162 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 12:57:51,163 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 12:57:51,177 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Column 'created_at' does not exist. Did you mean one of the following? [];
2025-05-12 12:57:51,179 - SparkScheduler - INFO - [trend_analysis] 'Project [to_date('created_at, None, Some(Etc/UTC)) AS date#119]
2025-05-12 12:57:51,180 - SparkScheduler - INFO - [trend_analysis] +- Relation [] MongoRelation(MongoRDD[67] at RDD at MongoRDD.scala:51,Some(StructType()))
2025-05-12 12:57:51,181 - SparkScheduler - INFO - [trend_analysis] 
2025-05-12 12:57:51,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:51 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 12:57:51,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-75ab08ed-2340-4b67-9c8c-524ef282f7e0
2025-05-12 12:57:51,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-070ff2ca-e592-4542-b0e1-907cb5bf76c0/pyspark-622376d4-a3cc-4086-865a-6218e0e7650e
2025-05-12 12:57:51,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 12:57:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-070ff2ca-e592-4542-b0e1-907cb5bf76c0
2025-05-12 12:57:51,438 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 12:57:51,439 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 12:57:51,440 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 12:57:51,443 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-0fd914bf-5110-4b1c-9088-0fdeba9b3263;1.0
2025-05-12 12:57:51,444 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 12:57:51,445 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 12:57:51,446 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 12:57:51,446 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 12:57:51,446 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 12:57:51,447 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...
2025-05-12 12:57:51,447 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (1080ms)
2025-05-12 12:57:51,448 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...
2025-05-12 12:57:51,449 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (307ms)
2025-05-12 12:57:51,450 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...
2025-05-12 12:57:51,450 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (428ms)
2025-05-12 12:57:51,451 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...
2025-05-12 12:57:51,452 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (723ms)
2025-05-12 12:57:51,452 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 6444ms :: artifacts dl 2575ms
2025-05-12 12:57:51,453 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 12:57:51,454 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 12:57:51,455 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 12:57:51,456 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 12:57:51,457 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 12:57:51,458 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 12:57:51,459 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 12:57:51,459 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 12:57:51,460 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 12:57:51,461 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   4   |   4   |   0   ||   4   |   4   |
2025-05-12 12:57:51,462 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 12:57:51,463 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-0fd914bf-5110-4b1c-9088-0fdeba9b3263
2025-05-12 12:57:51,463 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 12:57:51,464 - SparkScheduler - ERROR - [trend_analysis] 4 artifacts copied, 0 already retrieved (2728kB/43ms)
2025-05-12 12:57:51,466 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 12:57:51,468 - SparkScheduler - INFO - Job trend_analysis duration: 47.46 seconds
2025-05-12 12:57:51,473 - SparkScheduler - INFO - Starting job: user_recommender - Generate user recommendations
2025-05-12 12:57:51,477 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/user_recommender.py
2025-05-12 12:57:56,088 - SparkScheduler - INFO - [user_recommender] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 12:57:57,740 - SparkScheduler - INFO - [user_recommender] 25/05/12 12:57:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 12:57:59,412 - SparkScheduler - INFO - [user_recommender] Traceback (most recent call last):
2025-05-12 12:57:59,413 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 3, in <module>
2025-05-12 12:57:59,416 - SparkScheduler - INFO - [user_recommender] from pyspark.ml.feature import CountVectorizer
2025-05-12 12:57:59,416 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py", line 22, in <module>
2025-05-12 12:57:59,418 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py", line 40, in <module>
2025-05-12 12:57:59,419 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py", line 32, in <module>
2025-05-12 12:57:59,421 - SparkScheduler - INFO - [user_recommender] ModuleNotFoundError: No module named 'numpy'
2025-05-12 12:57:59,478 - SparkScheduler - INFO - [user_recommender] 25/05/12 12:57:59 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 12:57:59,479 - SparkScheduler - INFO - [user_recommender] 25/05/12 12:57:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-10f097a7-8c49-406c-8130-07690d1d2ce5
2025-05-12 12:57:59,522 - SparkScheduler - ERROR - [user_recommender] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 12:57:59,523 - SparkScheduler - ERROR - [user_recommender] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 12:57:59,524 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 12:57:59,524 - SparkScheduler - ERROR - [user_recommender] :: resolving dependencies :: org.apache.spark#spark-submit-parent-013f4db1-8e3a-47f6-937c-21057e0fc1c7;1.0
2025-05-12 12:57:59,525 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 12:57:59,525 - SparkScheduler - ERROR - [user_recommender] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 12:57:59,526 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 12:57:59,526 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#bson;4.0.5 in central
2025-05-12 12:57:59,527 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 12:57:59,527 - SparkScheduler - ERROR - [user_recommender] :: resolution report :: resolve 548ms :: artifacts dl 38ms
2025-05-12 12:57:59,528 - SparkScheduler - ERROR - [user_recommender] :: modules in use:
2025-05-12 12:57:59,528 - SparkScheduler - ERROR - [user_recommender] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 12:57:59,529 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 12:57:59,529 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 12:57:59,530 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 12:57:59,530 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 12:57:59,531 - SparkScheduler - ERROR - [user_recommender] |                  |            modules            ||   artifacts   |
2025-05-12 12:57:59,531 - SparkScheduler - ERROR - [user_recommender] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 12:57:59,532 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 12:57:59,533 - SparkScheduler - ERROR - [user_recommender] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 12:57:59,533 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 12:57:59,533 - SparkScheduler - ERROR - [user_recommender] :: retrieving :: org.apache.spark#spark-submit-parent-013f4db1-8e3a-47f6-937c-21057e0fc1c7
2025-05-12 12:57:59,534 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 12:57:59,534 - SparkScheduler - ERROR - [user_recommender] 0 artifacts copied, 4 already retrieved (0kB/27ms)
2025-05-12 12:57:59,535 - SparkScheduler - ERROR - Job user_recommender failed with exit code 1
2025-05-12 12:57:59,535 - SparkScheduler - INFO - Job user_recommender duration: 8.06 seconds
2025-05-12 12:57:59,536 - SparkScheduler - INFO - Starting job: content_analyzer - Analyze tweet content and topics
2025-05-12 12:57:59,537 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/content_analyzer.py
2025-05-12 12:58:01,540 - SparkScheduler - INFO - [content_analyzer] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 12:58:02,204 - SparkScheduler - INFO - [content_analyzer] 25/05/12 12:58:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 12:58:03,142 - SparkScheduler - INFO - [content_analyzer] Traceback (most recent call last):
2025-05-12 12:58:03,142 - SparkScheduler - INFO - [content_analyzer] File "/opt/spark-jobs/content_analyzer.py", line 4, in <module>
2025-05-12 12:58:03,145 - SparkScheduler - INFO - [content_analyzer] from pyspark.ml.feature import StopWordsRemover, Tokenizer
2025-05-12 12:58:03,145 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py", line 22, in <module>
2025-05-12 12:58:03,146 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py", line 40, in <module>
2025-05-12 12:58:03,146 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py", line 32, in <module>
2025-05-12 12:58:03,147 - SparkScheduler - INFO - [content_analyzer] ModuleNotFoundError: No module named 'numpy'
2025-05-12 12:58:03,173 - SparkScheduler - INFO - [content_analyzer] 25/05/12 12:58:03 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 12:58:03,174 - SparkScheduler - INFO - [content_analyzer] 25/05/12 12:58:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-09bcc337-bd7f-4076-9a55-3f15dd51fd74
2025-05-12 12:58:03,225 - SparkScheduler - ERROR - [content_analyzer] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 12:58:03,225 - SparkScheduler - ERROR - [content_analyzer] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 12:58:03,226 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 12:58:03,226 - SparkScheduler - ERROR - [content_analyzer] :: resolving dependencies :: org.apache.spark#spark-submit-parent-59c99059-421b-4106-b335-bb699c39bf78;1.0
2025-05-12 12:58:03,227 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 12:58:03,228 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 12:58:03,228 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 12:58:03,229 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#bson;4.0.5 in central
2025-05-12 12:58:03,229 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 12:58:03,229 - SparkScheduler - ERROR - [content_analyzer] :: resolution report :: resolve 241ms :: artifacts dl 15ms
2025-05-12 12:58:03,230 - SparkScheduler - ERROR - [content_analyzer] :: modules in use:
2025-05-12 12:58:03,231 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 12:58:03,231 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 12:58:03,232 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 12:58:03,232 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 12:58:03,233 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 12:58:03,233 - SparkScheduler - ERROR - [content_analyzer] |                  |            modules            ||   artifacts   |
2025-05-12 12:58:03,234 - SparkScheduler - ERROR - [content_analyzer] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 12:58:03,234 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 12:58:03,235 - SparkScheduler - ERROR - [content_analyzer] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 12:58:03,235 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 12:58:03,235 - SparkScheduler - ERROR - [content_analyzer] :: retrieving :: org.apache.spark#spark-submit-parent-59c99059-421b-4106-b335-bb699c39bf78
2025-05-12 12:58:03,236 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 12:58:03,236 - SparkScheduler - ERROR - [content_analyzer] 0 artifacts copied, 4 already retrieved (0kB/6ms)
2025-05-12 12:58:03,237 - SparkScheduler - ERROR - Job content_analyzer failed with exit code 1
2025-05-12 12:58:03,237 - SparkScheduler - INFO - Job content_analyzer duration: 3.70 seconds
2025-05-12 12:58:03,238 - SparkScheduler - INFO - Scheduler running. Press Ctrl+C to exit.
2025-05-12 13:02:51,791 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 13:02:51,791 - SparkScheduler - INFO - Waiting for services to be ready...
2025-05-12 13:03:24,573 - SparkScheduler - INFO - Setting up job schedules
2025-05-12 13:03:24,576 - SparkScheduler - INFO - Scheduled trend_analysis to run every 30 minutes
2025-05-12 13:03:24,578 - SparkScheduler - INFO - Scheduled user_recommender to run every 3 hours
2025-05-12 13:03:24,581 - SparkScheduler - INFO - Scheduled content_analyzer to run daily at 02:00
2025-05-12 13:03:24,582 - SparkScheduler - INFO - All jobs scheduled
2025-05-12 13:03:24,583 - SparkScheduler - INFO - Running initial job executions...
2025-05-12 13:03:24,585 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 13:03:24,586 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 13:03:30,188 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 13:03:38,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 13:03:39,954 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:39 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 13:03:40,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO ResourceUtils: ==============================================================
2025-05-12 13:03:40,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 13:03:40,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO ResourceUtils: ==============================================================
2025-05-12 13:03:40,043 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 13:03:40,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 13:03:40,156 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 13:03:40,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 13:03:40,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO SecurityManager: Changing view acls to: spark
2025-05-12 13:03:40,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 13:03:40,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO SecurityManager: Changing view acls groups to:
2025-05-12 13:03:40,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 13:03:40,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 13:03:40,754 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO Utils: Successfully started service 'sparkDriver' on port 35103.
2025-05-12 13:03:40,862 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:40 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 13:03:44,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:44 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 13:03:44,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 13:03:44,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 13:03:44,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 13:03:44,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6220f17d-3048-4d2f-a9ee-267cc4ee36a3
2025-05-12 13:03:44,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:44 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 13:03:44,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:44 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 13:03:45,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 13:03:45,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://2542ef3b9551:35103/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747055019939
2025-05-12 13:03:45,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://2542ef3b9551:35103/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747055019939
2025-05-12 13:03:45,364 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://2542ef3b9551:35103/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747055019939
2025-05-12 13:03:45,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://2542ef3b9551:35103/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747055019939
2025-05-12 13:03:45,373 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://2542ef3b9551:35103/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747055019939
2025-05-12 13:03:45,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-338b93eb-c1af-4955-9da4-8122a0bbdef9/userFiles-670c93e4-6c94-45a1-9769-6d2147f1c69d/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 13:03:45,408 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://2542ef3b9551:35103/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747055019939
2025-05-12 13:03:45,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-338b93eb-c1af-4955-9da4-8122a0bbdef9/userFiles-670c93e4-6c94-45a1-9769-6d2147f1c69d/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 13:03:45,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://2542ef3b9551:35103/files/org.mongodb_bson-4.0.5.jar with timestamp 1747055019939
2025-05-12 13:03:45,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-338b93eb-c1af-4955-9da4-8122a0bbdef9/userFiles-670c93e4-6c94-45a1-9769-6d2147f1c69d/org.mongodb_bson-4.0.5.jar
2025-05-12 13:03:45,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://2542ef3b9551:35103/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747055019939
2025-05-12 13:03:45,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-338b93eb-c1af-4955-9da4-8122a0bbdef9/userFiles-670c93e4-6c94-45a1-9769-6d2147f1c69d/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 13:03:45,641 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 13:03:45,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:45 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 51 ms (0 ms spent in bootstraps)
2025-05-12 13:03:46,015 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512130345-0000
2025-05-12 13:03:46,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36997.
2025-05-12 13:03:46,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO NettyBlockTransferService: Server created on 2542ef3b9551:36997
2025-05-12 13:03:46,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 13:03:46,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2542ef3b9551, 36997, None)
2025-05-12 13:03:46,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO BlockManagerMasterEndpoint: Registering block manager 2542ef3b9551:36997 with 434.4 MiB RAM, BlockManagerId(driver, 2542ef3b9551, 36997, None)
2025-05-12 13:03:46,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512130345-0000/0 on worker-20250512130300-172.23.0.8-42543 (172.23.0.8:42543) with 2 core(s)
2025-05-12 13:03:46,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2542ef3b9551, 36997, None)
2025-05-12 13:03:46,084 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2542ef3b9551, 36997, None)
2025-05-12 13:03:46,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512130345-0000/0 on hostPort 172.23.0.8:42543 with 2 core(s), 1024.0 MiB RAM
2025-05-12 13:03:46,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512130345-0000/0 is now RUNNING
2025-05-12 13:03:46,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 13:03:47,391 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 13:03:47,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 13:03:47,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:47 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 13:03:50,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 13:03:50,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 13:03:50,355 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2542ef3b9551:36997 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:03:50,370 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:50 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 13:03:53,208 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:44470) with ID 0,  ResourceProfileId 0
2025-05-12 13:03:53,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:53 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:38107 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 38107, None)
2025-05-12 13:03:56,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:03:56,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:03:56,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:03:56,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:56 INFO connection: Opened connection [connectionId{localValue:1, serverValue:18}] to mongodb:27017
2025-05-12 13:03:56,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4140349}
2025-05-12 13:03:56,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:56 INFO connection: Opened connection [connectionId{localValue:2, serverValue:19}] to mongodb:27017
2025-05-12 13:03:56,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:56 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:03:57,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:57 INFO CodeGenerator: Code generated in 289.937948 ms
2025-05-12 13:03:58,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:03:58,030 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:03:58,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO connection: Opened connection [connectionId{localValue:3, serverValue:20}] to mongodb:27017
2025-05-12 13:03:58,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1341469}
2025-05-12 13:03:58,043 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO connection: Opened connection [connectionId{localValue:4, serverValue:21}] to mongodb:27017
2025-05-12 13:03:58,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 13:03:58,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 13:03:58,153 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 13:03:58,153 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:03:58,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:03:58,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:03:58,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 13:03:58,222 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 13:03:58,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2542ef3b9551:36997 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:03:58,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:03:58,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:03:58,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 13:03:58,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:03:58,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:38107 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:03:59,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:03:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:38107 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:04:01,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3013 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:01,408 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 13:04:01,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 3.243 s
2025-05-12 13:04:01,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:04:01,425 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: running: Set()
2025-05-12 13:04:01,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: waiting: Set()
2025-05-12 13:04:01,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: failed: Set()
2025-05-12 13:04:01,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 13:04:01,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 0.004087 s
2025-05-12 13:04:01,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 13:04:01,605 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 13:04:01,607 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2542ef3b9551:36997 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:04:01,615 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO SparkContext: Created broadcast 2 from broadcast at MongoSpark.scala:530
2025-05-12 13:04:01,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:01,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO connection: Closed connection [connectionId{localValue:2, serverValue:19}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:01,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:01,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:01,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:01,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO connection: Opened connection [connectionId{localValue:5, serverValue:30}] to mongodb:27017
2025-05-12 13:04:01,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1405099}
2025-05-12 13:04:01,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO connection: Opened connection [connectionId{localValue:6, serverValue:31}] to mongodb:27017
2025-05-12 13:04:01,735 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:38107 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:01,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2542ef3b9551:36997 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:01,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:04:01,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO CodeGenerator: Code generated in 59.881267 ms
2025-05-12 13:04:01,938 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: Registering RDD 17 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 13:04:01,939 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 13:04:01,940 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (rdd at MongoSpark.scala:169)
2025-05-12 13:04:01,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:04:01,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:01,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[17] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:04:01,949 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 13:04:01,958 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 13:04:01,962 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2542ef3b9551:36997 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:01,964 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:01,966 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[17] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:01,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 2542ef3b9551:36997 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:04:01,968 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
2025-05-12 13:04:01,973 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:01,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:04:01,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO connection: Closed connection [connectionId{localValue:6, serverValue:31}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:01,976 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:01,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:01 INFO connection: Closed connection [connectionId{localValue:4, serverValue:21}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:02,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.8:38107 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:02,156 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 186 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:02,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
2025-05-12 13:04:02,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: ShuffleMapStage 1 (rdd at MongoSpark.scala:169) finished in 0.215 s
2025-05-12 13:04:02,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:04:02,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: running: Set()
2025-05-12 13:04:02,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: waiting: Set()
2025-05-12 13:04:02,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: failed: Set()
2025-05-12 13:04:02,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 13:04:02,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.000576 s
2025-05-12 13:04:02,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 13:04:02,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 13:04:02,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 2542ef3b9551:36997 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:04:02,240 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO SparkContext: Created broadcast 4 from broadcast at MongoSpark.scala:530
2025-05-12 13:04:02,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 2542ef3b9551:36997 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:02,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.23.0.8:38107 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:02,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:02,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:02,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:02,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Opened connection [connectionId{localValue:7, serverValue:32}] to mongodb:27017
2025-05-12 13:04:02,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1292551}
2025-05-12 13:04:02,329 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Opened connection [connectionId{localValue:8, serverValue:33}] to mongodb:27017
2025-05-12 13:04:02,332 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:02,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Closed connection [connectionId{localValue:8, serverValue:33}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:02,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:04:02,534 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:04:02,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO CodeGenerator: Code generated in 83.988427 ms
2025-05-12 13:04:02,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:02,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:02,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:02,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Opened connection [connectionId{localValue:9, serverValue:34}] to mongodb:27017
2025-05-12 13:04:02,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1286143}
2025-05-12 13:04:02,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Opened connection [connectionId{localValue:10, serverValue:35}] to mongodb:27017
2025-05-12 13:04:02,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:02,736 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Closed connection [connectionId{localValue:10, serverValue:35}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:02,739 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:02,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:02,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:02,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Opened connection [connectionId{localValue:11, serverValue:36}] to mongodb:27017
2025-05-12 13:04:02,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1751432}
2025-05-12 13:04:02,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Opened connection [connectionId{localValue:12, serverValue:37}] to mongodb:27017
2025-05-12 13:04:02,759 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:02,760 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO connection: Closed connection [connectionId{localValue:12, serverValue:37}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:02,762 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:02,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:02,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:02,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: Registering RDD 29 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 13:04:02,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 13:04:02,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (rdd at MongoSpark.scala:169)
2025-05-12 13:04:02,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:04:02,780 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:02,781 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[29] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:04:02,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 13:04:02,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 13:04:02,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 2542ef3b9551:36997 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:04:02,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 2542ef3b9551:36997 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:02,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:02,812 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[29] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:02,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 13:04:02,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:04:02,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:38107 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:03,141 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 324 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:03,142 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 13:04:03,143 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: ShuffleMapStage 2 (rdd at MongoSpark.scala:169) finished in 0.363 s
2025-05-12 13:04:03,144 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:04:03,145 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: running: Set()
2025-05-12 13:04:03,145 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: waiting: Set()
2025-05-12 13:04:03,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: failed: Set()
2025-05-12 13:04:03,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 13:04:03,179 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.000607 s
2025-05-12 13:04:03,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 13:04:03,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 13:04:03,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 2542ef3b9551:36997 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:04:03,244 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 13:04:03,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 2542ef3b9551:36997 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:03,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:38107 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:03,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:04:03,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:03,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:03,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:03,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO connection: Opened connection [connectionId{localValue:14, serverValue:39}] to mongodb:27017
2025-05-12 13:04:03,535 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1488003}
2025-05-12 13:04:03,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO connection: Opened connection [connectionId{localValue:15, serverValue:40}] to mongodb:27017
2025-05-12 13:04:03,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:03,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO connection: Closed connection [connectionId{localValue:15, serverValue:40}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:03,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:03,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:03,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:03,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO connection: Opened connection [connectionId{localValue:16, serverValue:41}] to mongodb:27017
2025-05-12 13:04:03,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1226414}
2025-05-12 13:04:03,556 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO connection: Opened connection [connectionId{localValue:17, serverValue:42}] to mongodb:27017
2025-05-12 13:04:03,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:03,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO connection: Closed connection [connectionId{localValue:17, serverValue:42}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:03,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:03,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:03,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:03,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Registering RDD 41 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 13:04:03,588 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:04:03,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:04:03,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:04:03,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:03,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[41] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:04:03,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 13:04:03,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 13:04:03,606 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 2542ef3b9551:36997 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:03,607 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:03,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 2542ef3b9551:36997 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:04:03,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[41] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:03,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 13:04:03,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:04:03,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:38107 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:03,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 144 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:03,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 13:04:03,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.183 s
2025-05-12 13:04:03,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:04:03,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: running: Set()
2025-05-12 13:04:03,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: waiting: Set()
2025-05-12 13:04:03,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: failed: Set()
2025-05-12 13:04:03,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:04:03,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO CodeGenerator: Code generated in 19.940843 ms
2025-05-12 13:04:03,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO CodeGenerator: Code generated in 15.485799 ms
2025-05-12 13:04:03,883 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:04:03,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO CodeGenerator: Code generated in 24.407848 ms
2025-05-12 13:04:03,986 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 13:04:03,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:04:03,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:04:03,993 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 13:04:03,994 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:03,997 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[46] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:04:04,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 13:04:04,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 13:04:04,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 2542ef3b9551:36997 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:04,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 2542ef3b9551:36997 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:04,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:38107 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:04,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:04,041 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[46] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:04,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 13:04:04,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (172.23.0.8, executor 0, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:04:04,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:38107 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:04,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:44470
2025-05-12 13:04:04,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 530 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:04,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 13:04:04,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.567 s
2025-05-12 13:04:04,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:04:04,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 13:04:04,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.597613 s
2025-05-12 13:04:04,683 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:04:04,790 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:04,791 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:04,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:04,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO connection: Opened connection [connectionId{localValue:19, serverValue:44}] to mongodb:27017
2025-05-12 13:04:04,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1637877}
2025-05-12 13:04:04,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO connection: Opened connection [connectionId{localValue:20, serverValue:45}] to mongodb:27017
2025-05-12 13:04:04,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:04,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO connection: Closed connection [connectionId{localValue:20, serverValue:45}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:04,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:04,806 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:04,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:04,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO connection: Opened connection [connectionId{localValue:21, serverValue:46}] to mongodb:27017
2025-05-12 13:04:04,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1319212}
2025-05-12 13:04:04,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO connection: Opened connection [connectionId{localValue:22, serverValue:48}] to mongodb:27017
2025-05-12 13:04:04,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:04,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO connection: Closed connection [connectionId{localValue:22, serverValue:48}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:04,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:04,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:04,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:04,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Registering RDD 51 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 13:04:04,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:04:04,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:04:04,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:04:04,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:04,839 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[51] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:04:04,845 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 47.8 KiB, free 434.3 MiB)
2025-05-12 13:04:04,854 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 13:04:04,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 2542ef3b9551:36997 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:04,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:04,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[51] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:04,865 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 13:04:04,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:04:04,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 2542ef3b9551:36997 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:04,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:38107 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:04,922 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:04 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.23.0.8:38107 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 144 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:05,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 13:04:05,014 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.176 s
2025-05-12 13:04:05,015 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:04:05,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: running: Set()
2025-05-12 13:04:05,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: waiting: Set()
2025-05-12 13:04:05,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: failed: Set()
2025-05-12 13:04:05,023 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:04:05,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:04:05,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 13:04:05,081 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:04:05,082 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:04:05,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 13:04:05,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:05,085 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[56] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:04:05,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 13:04:05,102 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 13:04:05,105 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 2542ef3b9551:36997 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,106 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:05,109 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[56] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:05,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 13:04:05,111 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (172.23.0.8, executor 0, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:04:05,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 2542ef3b9551:36997 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.23.0.8:38107 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:38107 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,186 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:44470
2025-05-12 13:04:05,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 142 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:05,254 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 13:04:05,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.167 s
2025-05-12 13:04:05,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:04:05,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 13:04:05,260 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.176006 s
2025-05-12 13:04:05,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:04:05,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:04:05,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:05,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:05,476 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:05,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO connection: Opened connection [connectionId{localValue:24, serverValue:50}] to mongodb:27017
2025-05-12 13:04:05,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1495616}
2025-05-12 13:04:05,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO connection: Opened connection [connectionId{localValue:25, serverValue:51}] to mongodb:27017
2025-05-12 13:04:05,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:05,486 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO connection: Closed connection [connectionId{localValue:25, serverValue:51}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:05,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:05,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:05,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:05,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO connection: Opened connection [connectionId{localValue:26, serverValue:52}] to mongodb:27017
2025-05-12 13:04:05,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1784807}
2025-05-12 13:04:05,496 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO connection: Opened connection [connectionId{localValue:27, serverValue:53}] to mongodb:27017
2025-05-12 13:04:05,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:05,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO connection: Closed connection [connectionId{localValue:27, serverValue:53}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:05,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:05,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:05,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:05,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Registering RDD 61 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 13:04:05,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:04:05,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:04:05,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:04:05,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:05,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[61] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:04:05,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 13:04:05,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 13:04:05,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 2542ef3b9551:36997 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,535 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:05,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[61] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:05,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 13:04:05,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 2542ef3b9551:36997 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:04:05,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:38107 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:38107 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 96 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:05,641 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 13:04:05,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.122 s
2025-05-12 13:04:05,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:04:05,644 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: running: Set()
2025-05-12 13:04:05,644 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: waiting: Set()
2025-05-12 13:04:05,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: failed: Set()
2025-05-12 13:04:05,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:04:05,680 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:04:05,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO CodeGenerator: Code generated in 25.974664 ms
2025-05-12 13:04:05,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 13:04:05,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:04:05,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:04:05,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 13:04:05,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:05,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[66] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:04:05,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 58.7 KiB, free 434.3 MiB)
2025-05-12 13:04:05,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.2 MiB)
2025-05-12 13:04:05,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 2542ef3b9551:36997 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:05,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 2542ef3b9551:36997 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,777 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[66] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:05,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 13:04:05,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (172.23.0.8, executor 0, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:04:05,780 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:38107 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,816 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:38107 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,850 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:44470
2025-05-12 13:04:05,917 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 143 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:05,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 13:04:05,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.167 s
2025-05-12 13:04:05,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:04:05,922 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 13:04:05,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.177315 s
2025-05-12 13:04:05,931 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 0, 'weekly': 0, 'hourly': 0}
2025-05-12 13:04:05,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 13:04:05,954 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 2542ef3b9551:36997 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,961 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:38107 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 13:04:05,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 13:04:05,983 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 2542ef3b9551:36997 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:04:05,986 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:05 INFO SparkContext: Created broadcast 13 from broadcast at MongoSpark.scala:530
2025-05-12 13:04:06,001 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:06,015 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:06,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:04:06,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO connection: Opened connection [connectionId{localValue:29, serverValue:55}] to mongodb:27017
2025-05-12 13:04:06,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1853036}
2025-05-12 13:04:06,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO connection: Opened connection [connectionId{localValue:30, serverValue:56}] to mongodb:27017
2025-05-12 13:04:06,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:06,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO connection: Closed connection [connectionId{localValue:30, serverValue:56}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:04:06,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:04:06,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:04:06,054 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:04:06,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 13:04:06,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 13:04:06,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: Final stage: ResultStage 12 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 13:04:06,081 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:04:06,082 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:04:06,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[71] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 13:04:06,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 13:04:06,103 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 13:04:06,111 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 2542ef3b9551:36997 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:04:06,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:04:06,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[71] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:04:06,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 13:04:06,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 13:04:06,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:38107 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:04:06,207 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:38107 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:04:06,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 111 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:04:06,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 13:04:06,231 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: ResultStage 12 (treeAggregate at MongoInferSchema.scala:88) finished in 0.148 s
2025-05-12 13:04:06,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:04:06,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
2025-05-12 13:04:06,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.154874 s
2025-05-12 13:04:06,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO SparkUI: Stopped Spark web UI at http://2542ef3b9551:4040
2025-05-12 13:04:06,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 13:04:06,447 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 13:04:06,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 13:04:06,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO MemoryStore: MemoryStore cleared
2025-05-12 13:04:06,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO BlockManager: BlockManager stopped
2025-05-12 13:04:06,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 13:04:06,574 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 13:04:06,619 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:06 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 13:04:06,997 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 13:04:06,998 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 13:04:07,001 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 13:04:07,002 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 196, in main
2025-05-12 13:04:07,005 - SparkScheduler - INFO - [trend_analysis] activity_results = analyze_user_activity(spark)
2025-05-12 13:04:07,006 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 116, in analyze_user_activity
2025-05-12 13:04:07,010 - SparkScheduler - INFO - [trend_analysis] .withColumn("date", to_date("created_at"))
2025-05-12 13:04:07,011 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 3036, in withColumn
2025-05-12 13:04:07,013 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 13:04:07,014 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 13:04:07,024 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Column 'created_at' does not exist. Did you mean one of the following? [];
2025-05-12 13:04:07,025 - SparkScheduler - INFO - [trend_analysis] 'Project [to_date('created_at, None, Some(Etc/UTC)) AS date#119]
2025-05-12 13:04:07,025 - SparkScheduler - INFO - [trend_analysis] +- Relation [] MongoRelation(MongoRDD[67] at RDD at MongoRDD.scala:51,Some(StructType()))
2025-05-12 13:04:07,026 - SparkScheduler - INFO - [trend_analysis] 
2025-05-12 13:04:07,076 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:07 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 13:04:07,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-338b93eb-c1af-4955-9da4-8122a0bbdef9
2025-05-12 13:04:07,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ed70ea9-15cd-4183-9ee7-6763df515795
2025-05-12 13:04:07,140 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:04:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-338b93eb-c1af-4955-9da4-8122a0bbdef9/pyspark-47f92992-26e6-431e-9487-c0a389f15f19
2025-05-12 13:04:07,243 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 13:04:07,244 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 13:04:07,245 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 13:04:07,246 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-70b4325b-8a4c-44e9-a77b-786e8ccb561f;1.0
2025-05-12 13:04:07,247 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 13:04:07,248 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 13:04:07,249 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 13:04:07,250 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 13:04:07,251 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 13:04:07,251 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...
2025-05-12 13:04:07,252 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (1231ms)
2025-05-12 13:04:07,252 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...
2025-05-12 13:04:07,253 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (341ms)
2025-05-12 13:04:07,254 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...
2025-05-12 13:04:07,255 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (588ms)
2025-05-12 13:04:07,256 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...
2025-05-12 13:04:07,258 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (1196ms)
2025-05-12 13:04:07,258 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 3669ms :: artifacts dl 3377ms
2025-05-12 13:04:07,259 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 13:04:07,260 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 13:04:07,261 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 13:04:07,261 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 13:04:07,263 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 13:04:07,263 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 13:04:07,264 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 13:04:07,265 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 13:04:07,266 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 13:04:07,266 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   4   |   4   |   0   ||   4   |   4   |
2025-05-12 13:04:07,267 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 13:04:07,268 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-70b4325b-8a4c-44e9-a77b-786e8ccb561f
2025-05-12 13:04:07,269 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 13:04:07,271 - SparkScheduler - ERROR - [trend_analysis] 4 artifacts copied, 0 already retrieved (2728kB/51ms)
2025-05-12 13:04:07,272 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 13:04:07,272 - SparkScheduler - INFO - Job trend_analysis duration: 42.69 seconds
2025-05-12 13:04:07,275 - SparkScheduler - INFO - Starting job: user_recommender - Generate user recommendations
2025-05-12 13:04:07,275 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/user_recommender.py
2025-05-12 13:04:11,070 - SparkScheduler - INFO - [user_recommender] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 13:04:11,862 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:04:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 13:04:13,698 - SparkScheduler - INFO - [user_recommender] Traceback (most recent call last):
2025-05-12 13:04:13,699 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 3, in <module>
2025-05-12 13:04:13,714 - SparkScheduler - INFO - [user_recommender] from pyspark.ml.feature import CountVectorizer
2025-05-12 13:04:13,715 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py", line 22, in <module>
2025-05-12 13:04:13,716 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py", line 40, in <module>
2025-05-12 13:04:13,717 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py", line 32, in <module>
2025-05-12 13:04:13,718 - SparkScheduler - INFO - [user_recommender] ModuleNotFoundError: No module named 'numpy'
2025-05-12 13:04:13,263 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:04:13 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 13:04:13,267 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:04:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-25f1ca17-12e0-4372-bc47-194536ee3c02
2025-05-12 13:04:13,361 - SparkScheduler - ERROR - [user_recommender] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 13:04:13,362 - SparkScheduler - ERROR - [user_recommender] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 13:04:13,363 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 13:04:13,364 - SparkScheduler - ERROR - [user_recommender] :: resolving dependencies :: org.apache.spark#spark-submit-parent-47537dcd-500a-4440-a7d2-10a87a96ca0c;1.0
2025-05-12 13:04:13,365 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 13:04:13,366 - SparkScheduler - ERROR - [user_recommender] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 13:04:13,367 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 13:04:13,368 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#bson;4.0.5 in central
2025-05-12 13:04:13,369 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 13:04:13,372 - SparkScheduler - ERROR - [user_recommender] :: resolution report :: resolve 313ms :: artifacts dl 15ms
2025-05-12 13:04:13,373 - SparkScheduler - ERROR - [user_recommender] :: modules in use:
2025-05-12 13:04:13,374 - SparkScheduler - ERROR - [user_recommender] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 13:04:13,378 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 13:04:13,379 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 13:04:13,380 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 13:04:13,382 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 13:04:13,383 - SparkScheduler - ERROR - [user_recommender] |                  |            modules            ||   artifacts   |
2025-05-12 13:04:13,384 - SparkScheduler - ERROR - [user_recommender] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 13:04:13,385 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 13:04:13,388 - SparkScheduler - ERROR - [user_recommender] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 13:04:13,389 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 13:04:13,390 - SparkScheduler - ERROR - [user_recommender] :: retrieving :: org.apache.spark#spark-submit-parent-47537dcd-500a-4440-a7d2-10a87a96ca0c
2025-05-12 13:04:13,391 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 13:04:13,392 - SparkScheduler - ERROR - [user_recommender] 0 artifacts copied, 4 already retrieved (0kB/8ms)
2025-05-12 13:04:13,393 - SparkScheduler - ERROR - Job user_recommender failed with exit code 1
2025-05-12 13:04:13,394 - SparkScheduler - INFO - Job user_recommender duration: 6.12 seconds
2025-05-12 13:04:13,395 - SparkScheduler - INFO - Starting job: content_analyzer - Analyze tweet content and topics
2025-05-12 13:04:13,395 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/content_analyzer.py
2025-05-12 13:04:19,855 - SparkScheduler - INFO - [content_analyzer] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 13:04:20,605 - SparkScheduler - INFO - [content_analyzer] 25/05/12 13:04:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 13:04:21,702 - SparkScheduler - INFO - [content_analyzer] Traceback (most recent call last):
2025-05-12 13:04:21,703 - SparkScheduler - INFO - [content_analyzer] File "/opt/spark-jobs/content_analyzer.py", line 4, in <module>
2025-05-12 13:04:21,708 - SparkScheduler - INFO - [content_analyzer] from pyspark.ml.feature import StopWordsRemover, Tokenizer
2025-05-12 13:04:21,709 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py", line 22, in <module>
2025-05-12 13:04:21,710 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py", line 40, in <module>
2025-05-12 13:04:21,711 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py", line 32, in <module>
2025-05-12 13:04:21,712 - SparkScheduler - INFO - [content_analyzer] ModuleNotFoundError: No module named 'numpy'
2025-05-12 13:04:21,744 - SparkScheduler - INFO - [content_analyzer] 25/05/12 13:04:21 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 13:04:21,745 - SparkScheduler - INFO - [content_analyzer] 25/05/12 13:04:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-d780085c-1f21-44db-8f4e-e4bbe775ae0e
2025-05-12 13:04:21,795 - SparkScheduler - ERROR - [content_analyzer] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 13:04:21,796 - SparkScheduler - ERROR - [content_analyzer] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 13:04:21,796 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 13:04:21,797 - SparkScheduler - ERROR - [content_analyzer] :: resolving dependencies :: org.apache.spark#spark-submit-parent-5f5ac9cc-02c4-45eb-9287-9e9a337f2dcb;1.0
2025-05-12 13:04:21,797 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 13:04:21,798 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 13:04:21,798 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 13:04:21,799 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#bson;4.0.5 in central
2025-05-12 13:04:21,799 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 13:04:21,800 - SparkScheduler - ERROR - [content_analyzer] :: resolution report :: resolve 316ms :: artifacts dl 17ms
2025-05-12 13:04:21,800 - SparkScheduler - ERROR - [content_analyzer] :: modules in use:
2025-05-12 13:04:21,801 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 13:04:21,802 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 13:04:21,802 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 13:04:21,803 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 13:04:21,803 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 13:04:21,804 - SparkScheduler - ERROR - [content_analyzer] |                  |            modules            ||   artifacts   |
2025-05-12 13:04:21,804 - SparkScheduler - ERROR - [content_analyzer] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 13:04:21,805 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 13:04:21,806 - SparkScheduler - ERROR - [content_analyzer] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 13:04:21,806 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 13:04:21,807 - SparkScheduler - ERROR - [content_analyzer] :: retrieving :: org.apache.spark#spark-submit-parent-5f5ac9cc-02c4-45eb-9287-9e9a337f2dcb
2025-05-12 13:04:21,807 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 13:04:21,808 - SparkScheduler - ERROR - [content_analyzer] 0 artifacts copied, 4 already retrieved (0kB/11ms)
2025-05-12 13:04:21,809 - SparkScheduler - ERROR - Job content_analyzer failed with exit code 1
2025-05-12 13:04:21,810 - SparkScheduler - INFO - Job content_analyzer duration: 8.41 seconds
2025-05-12 13:04:21,810 - SparkScheduler - INFO - Scheduler running. Press Ctrl+C to exit.
2025-05-12 13:21:15,073 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 13:21:15,074 - SparkScheduler - INFO - Waiting for services to be ready...
2025-05-12 13:21:48,456 - SparkScheduler - INFO - Setting up job schedules
2025-05-12 13:21:48,460 - SparkScheduler - INFO - Scheduled trend_analysis to run every 30 minutes
2025-05-12 13:21:48,476 - SparkScheduler - INFO - Scheduled user_recommender to run every 3 hours
2025-05-12 13:21:48,523 - SparkScheduler - INFO - Scheduled content_analyzer to run daily at 02:00
2025-05-12 13:21:48,575 - SparkScheduler - INFO - All jobs scheduled
2025-05-12 13:21:48,579 - SparkScheduler - INFO - Running initial job executions...
2025-05-12 13:21:48,582 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 13:21:48,588 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 13:21:51,894 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 13:21:58,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:21:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 13:22:04,070 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 13:22:04,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO ResourceUtils: ==============================================================
2025-05-12 13:22:04,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 13:22:04,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO ResourceUtils: ==============================================================
2025-05-12 13:22:04,122 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 13:22:04,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 13:22:04,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 13:22:04,181 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 13:22:04,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SecurityManager: Changing view acls to: spark
2025-05-12 13:22:04,307 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 13:22:04,309 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SecurityManager: Changing view acls groups to:
2025-05-12 13:22:04,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 13:22:04,314 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 13:22:04,787 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO Utils: Successfully started service 'sparkDriver' on port 46217.
2025-05-12 13:22:04,862 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 13:22:04,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 13:22:04,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 13:22:04,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 13:22:04,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 13:22:05,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd2296ac-67a0-4413-9019-eafb09cab4c1
2025-05-12 13:22:05,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 13:22:05,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 13:22:05,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 13:22:05,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://dbe8b5eed567:46217/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747056124053
2025-05-12 13:22:05,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://dbe8b5eed567:46217/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747056124053
2025-05-12 13:22:05,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://dbe8b5eed567:46217/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747056124053
2025-05-12 13:22:05,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://dbe8b5eed567:46217/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747056124053
2025-05-12 13:22:05,738 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://dbe8b5eed567:46217/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747056124053
2025-05-12 13:22:05,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-f58903e2-8310-4d70-8072-09f8fb143771/userFiles-de7869e7-a885-44af-b2a7-f26be6284cb8/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 13:22:05,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://dbe8b5eed567:46217/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747056124053
2025-05-12 13:22:05,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-f58903e2-8310-4d70-8072-09f8fb143771/userFiles-de7869e7-a885-44af-b2a7-f26be6284cb8/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 13:22:05,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://dbe8b5eed567:46217/files/org.mongodb_bson-4.0.5.jar with timestamp 1747056124053
2025-05-12 13:22:05,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-f58903e2-8310-4d70-8072-09f8fb143771/userFiles-de7869e7-a885-44af-b2a7-f26be6284cb8/org.mongodb_bson-4.0.5.jar
2025-05-12 13:22:05,812 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://dbe8b5eed567:46217/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747056124053
2025-05-12 13:22:05,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-f58903e2-8310-4d70-8072-09f8fb143771/userFiles-de7869e7-a885-44af-b2a7-f26be6284cb8/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 13:22:05,973 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:05 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 13:22:06,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 41 ms (0 ms spent in bootstraps)
2025-05-12 13:22:06,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512132206-0000
2025-05-12 13:22:06,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34785.
2025-05-12 13:22:06,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO NettyBlockTransferService: Server created on dbe8b5eed567:34785
2025-05-12 13:22:06,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 13:22:06,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dbe8b5eed567, 34785, None)
2025-05-12 13:22:06,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO BlockManagerMasterEndpoint: Registering block manager dbe8b5eed567:34785 with 434.4 MiB RAM, BlockManagerId(driver, dbe8b5eed567, 34785, None)
2025-05-12 13:22:06,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dbe8b5eed567, 34785, None)
2025-05-12 13:22:06,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, dbe8b5eed567, 34785, None)
2025-05-12 13:22:06,459 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512132206-0000/0 on worker-20250512132122-172.23.0.8-44075 (172.23.0.8:44075) with 2 core(s)
2025-05-12 13:22:06,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512132206-0000/0 on hostPort 172.23.0.8:44075 with 2 core(s), 1024.0 MiB RAM
2025-05-12 13:22:06,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512132206-0000/0 is now RUNNING
2025-05-12 13:22:07,024 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:07 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 13:22:07,506 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 13:22:07,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 13:22:07,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:07 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 13:22:10,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 13:22:10,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 13:22:10,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on dbe8b5eed567:34785 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:10,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:10 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:13,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:37926) with ID 0,  ResourceProfileId 0
2025-05-12 13:22:14,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:33883 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 33883, None)
2025-05-12 13:22:16,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:16 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:16,467 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:16 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:16,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:16 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:16,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:16 INFO connection: Opened connection [connectionId{localValue:1, serverValue:10776}] to mongodb:27017
2025-05-12 13:22:16,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:16 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5233987}
2025-05-12 13:22:16,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:16 INFO connection: Opened connection [connectionId{localValue:2, serverValue:10777}] to mongodb:27017
2025-05-12 13:22:17,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:17 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:22:18,206 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO CodeGenerator: Code generated in 387.514552 ms
2025-05-12 13:22:18,335 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:18,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:18,341 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO connection: Opened connection [connectionId{localValue:3, serverValue:10778}] to mongodb:27017
2025-05-12 13:22:18,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1115125}
2025-05-12 13:22:18,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO connection: Opened connection [connectionId{localValue:4, serverValue:10779}] to mongodb:27017
2025-05-12 13:22:18,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 13:22:18,537 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 13:22:18,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 13:22:18,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:18,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:18,577 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:18,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 13:22:18,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 13:22:18,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on dbe8b5eed567:34785 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:18,815 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:18,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:18,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 13:22:19,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:19,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:33883 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:20,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:33883 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:21,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:21 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:21,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:21 INFO connection: Closed connection [connectionId{localValue:2, serverValue:10777}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:23,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:23,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO connection: Closed connection [connectionId{localValue:4, serverValue:10779}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:23,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4779 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:23,815 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 13:22:23,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 5.122 s
2025-05-12 13:22:23,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:23,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO DAGScheduler: running: Set()
2025-05-12 13:22:23,840 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:23,842 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:23 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:24,015 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:24,084 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO CodeGenerator: Code generated in 34.362432 ms
2025-05-12 13:22:24,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:24,163 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO CodeGenerator: Code generated in 51.494609 ms
2025-05-12 13:22:24,309 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 13:22:24,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 13:22:24,317 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 13:22:24,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 13:22:24,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:24,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:24,361 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 13:22:24,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 13:22:24,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on dbe8b5eed567:34785 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 13:22:24,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:24,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:24,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 13:22:24,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:24,459 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on dbe8b5eed567:34785 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:24,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:33883 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:24,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:33883 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 13:22:24,924 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:37926
2025-05-12 13:22:25,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 890 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:25,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 13:22:25,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 0.948 s
2025-05-12 13:22:25,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:25,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 13:22:25,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 1.000630 s
2025-05-12 13:22:25,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 13:22:25,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 13:22:25,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on dbe8b5eed567:34785 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:25,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:25,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:25,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:25,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:25,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO connection: Opened connection [connectionId{localValue:5, serverValue:10785}] to mongodb:27017
2025-05-12 13:22:25,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1708370}
2025-05-12 13:22:25,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO connection: Opened connection [connectionId{localValue:6, serverValue:10786}] to mongodb:27017
2025-05-12 13:22:25,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:22:25,790 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO CodeGenerator: Code generated in 107.723354 ms
2025-05-12 13:22:25,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:25,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:25,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:25,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO connection: Opened connection [connectionId{localValue:7, serverValue:10787}] to mongodb:27017
2025-05-12 13:22:25,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2257405}
2025-05-12 13:22:25,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO connection: Opened connection [connectionId{localValue:8, serverValue:10788}] to mongodb:27017
2025-05-12 13:22:25,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 13:22:25,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 13:22:25,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 13:22:25,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:25,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:25,891 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:25,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.3 MiB)
2025-05-12 13:22:25,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 13:22:25,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on dbe8b5eed567:34785 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:25,935 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:25,944 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO connection: Closed connection [connectionId{localValue:8, serverValue:10788}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:25,945 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:25,947 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO connection: Closed connection [connectionId{localValue:6, serverValue:10786}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:25,949 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:25,968 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:25,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 13:22:25,986 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO BlockManagerInfo: Removed broadcast_3_piece0 on dbe8b5eed567:34785 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:25,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:25,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO BlockManagerInfo: Removed broadcast_2_piece0 on dbe8b5eed567:34785 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 13:22:25,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:25 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:33883 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 13:22:26,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:33883 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:26,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 334 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:26,297 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 13:22:26,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.410 s
2025-05-12 13:22:26,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:26,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: running: Set()
2025-05-12 13:22:26,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:26,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:26,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:26,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:26,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 13:22:26,441 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 13:22:26,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 13:22:26,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 13:22:26,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:26,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:26,459 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 13:22:26,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 13:22:26,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on dbe8b5eed567:34785 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 13:22:26,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:26,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:26,476 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 13:22:26,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:26,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:33883 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 13:22:26,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:37926
2025-05-12 13:22:26,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 161 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:26,647 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 13:22:26,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.197 s
2025-05-12 13:22:26,654 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:26,656 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 13:22:26,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.218829 s
2025-05-12 13:22:26,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 13:22:26,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 13:22:26,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on dbe8b5eed567:34785 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:26,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO BlockManagerInfo: Removed broadcast_4_piece0 on dbe8b5eed567:34785 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:26,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:26,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:33883 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:26,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on dbe8b5eed567:34785 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 13:22:26,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:33883 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 13:22:26,842 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:26,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:26,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:26,850 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO connection: Opened connection [connectionId{localValue:9, serverValue:10789}] to mongodb:27017
2025-05-12 13:22:26,854 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1955598}
2025-05-12 13:22:26,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO connection: Opened connection [connectionId{localValue:10, serverValue:10790}] to mongodb:27017
2025-05-12 13:22:26,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:26,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:26 INFO connection: Closed connection [connectionId{localValue:10, serverValue:10790}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:27,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:22:27,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:27,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO CodeGenerator: Code generated in 69.075499 ms
2025-05-12 13:22:27,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:27,440 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:27,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:27,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO connection: Opened connection [connectionId{localValue:11, serverValue:10791}] to mongodb:27017
2025-05-12 13:22:27,447 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3278736}
2025-05-12 13:22:27,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO connection: Opened connection [connectionId{localValue:12, serverValue:10792}] to mongodb:27017
2025-05-12 13:22:27,452 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:27,454 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO connection: Closed connection [connectionId{localValue:12, serverValue:10792}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:27,459 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:27,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:27,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:27,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO connection: Opened connection [connectionId{localValue:13, serverValue:10793}] to mongodb:27017
2025-05-12 13:22:27,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1451958}
2025-05-12 13:22:27,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO connection: Opened connection [connectionId{localValue:14, serverValue:10794}] to mongodb:27017
2025-05-12 13:22:27,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:27,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO connection: Closed connection [connectionId{localValue:14, serverValue:10794}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:27,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:27,499 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:27,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:27,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO connection: Opened connection [connectionId{localValue:15, serverValue:10795}] to mongodb:27017
2025-05-12 13:22:27,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 13:22:27,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 13:22:27,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 13:22:27,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:27,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:27,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:27,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 13:22:27,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 13:22:27,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on dbe8b5eed567:34785 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:27,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:27,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:27,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 13:22:27,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:27,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:27 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:33883 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:28,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 652 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:28,194 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 13:22:28,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.673 s
2025-05-12 13:22:28,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:28,203 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: running: Set()
2025-05-12 13:22:28,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:28,208 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:28,220 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:28,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO CodeGenerator: Code generated in 26.81292 ms
2025-05-12 13:22:28,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:28,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO CodeGenerator: Code generated in 76.095054 ms
2025-05-12 13:22:28,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 13:22:28,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 13:22:28,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 13:22:28,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 13:22:28,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:28,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:28,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.3 MiB)
2025-05-12 13:22:28,562 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)
2025-05-12 13:22:28,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on dbe8b5eed567:34785 (size: 27.2 KiB, free: 434.3 MiB)
2025-05-12 13:22:28,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:28,602 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:28,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 13:22:28,613 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO BlockManagerInfo: Removed broadcast_6_piece0 on dbe8b5eed567:34785 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:28,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:28,646 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO BlockManagerInfo: Removed broadcast_7_piece0 on dbe8b5eed567:34785 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:28,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:33883 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:28,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:33883 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 13:22:28,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:37926
2025-05-12 13:22:29,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 427 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:29,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 13:22:29,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.530 s
2025-05-12 13:22:29,044 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:29,045 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 13:22:29,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.563254 s
2025-05-12 13:22:29,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 13:22:29,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 13:22:29,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on dbe8b5eed567:34785 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:29,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:29,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:22:29,374 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:29,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:29,379 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:29,380 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO connection: Opened connection [connectionId{localValue:16, serverValue:10796}] to mongodb:27017
2025-05-12 13:22:29,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1575980}
2025-05-12 13:22:29,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO connection: Opened connection [connectionId{localValue:17, serverValue:10797}] to mongodb:27017
2025-05-12 13:22:29,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:29,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO connection: Closed connection [connectionId{localValue:17, serverValue:10797}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:29,395 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:29,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:29,399 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:29,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO connection: Opened connection [connectionId{localValue:18, serverValue:10798}] to mongodb:27017
2025-05-12 13:22:29,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3391060}
2025-05-12 13:22:29,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO connection: Opened connection [connectionId{localValue:19, serverValue:10799}] to mongodb:27017
2025-05-12 13:22:29,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:29,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO connection: Closed connection [connectionId{localValue:19, serverValue:10799}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:29,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:29,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:29,438 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:29,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 13:22:29,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:29,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:29,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:29,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:29,455 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:29,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.3 MiB)
2025-05-12 13:22:29,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 13:22:29,486 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on dbe8b5eed567:34785 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:29,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:29,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:29,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 13:22:29,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:29,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO BlockManagerInfo: Removed broadcast_9_piece0 on dbe8b5eed567:34785 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:29,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO BlockManagerInfo: Removed broadcast_8_piece0 on dbe8b5eed567:34785 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 13:22:29,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:33883 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 13:22:29,562 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:33883 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:29,667 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 169 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:29,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 13:22:29,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.210 s
2025-05-12 13:22:29,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:29,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: running: Set()
2025-05-12 13:22:29,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:29,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:29,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:29,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO CodeGenerator: Code generated in 31.773326 ms
2025-05-12 13:22:29,812 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO CodeGenerator: Code generated in 22.055648 ms
2025-05-12 13:22:29,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:29,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO CodeGenerator: Code generated in 64.934752 ms
2025-05-12 13:22:29,966 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 13:22:29,970 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:29,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:29,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 13:22:29,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:29,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:29,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:29 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 13:22:30,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 13:22:30,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on dbe8b5eed567:34785 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:30,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:30,014 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:30,015 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 13:22:30,021 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:30,085 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:33883 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:30,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:37926
2025-05-12 13:22:30,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 240 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:30,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 13:22:30,265 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.273 s
2025-05-12 13:22:30,269 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:30,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 13:22:30,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.296714 s
2025-05-12 13:22:30,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:22:30,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:30,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:30,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:30,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO connection: Opened connection [connectionId{localValue:21, serverValue:10801}] to mongodb:27017
2025-05-12 13:22:30,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3096582}
2025-05-12 13:22:30,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO connection: Opened connection [connectionId{localValue:22, serverValue:10802}] to mongodb:27017
2025-05-12 13:22:30,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:30,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO connection: Closed connection [connectionId{localValue:22, serverValue:10802}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:30,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:30,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:30,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:30,556 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO connection: Opened connection [connectionId{localValue:23, serverValue:10803}] to mongodb:27017
2025-05-12 13:22:30,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1503978}
2025-05-12 13:22:30,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO connection: Opened connection [connectionId{localValue:24, serverValue:10804}] to mongodb:27017
2025-05-12 13:22:30,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:30,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO connection: Closed connection [connectionId{localValue:24, serverValue:10804}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:30,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:30,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:30,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:30,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 13:22:30,603 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:30,606 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:30,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:30,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:30,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:30,626 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.2 MiB)
2025-05-12 13:22:30,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 13:22:30,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on dbe8b5eed567:34785 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 13:22:30,665 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO BlockManagerInfo: Removed broadcast_11_piece0 on dbe8b5eed567:34785 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:30,667 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:30,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:30,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 13:22:30,679 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:30,680 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:33883 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:30,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO BlockManagerInfo: Removed broadcast_10_piece0 on dbe8b5eed567:34785 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:30,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:33883 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:30,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:33883 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:30,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 226 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:30,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 13:22:30,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.282 s
2025-05-12 13:22:30,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:30,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: running: Set()
2025-05-12 13:22:30,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:30,909 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:30,916 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:30 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:31,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:31,082 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 13:22:31,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:31,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:31,088 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 13:22:31,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:31,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:31,109 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 13:22:31,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 13:22:31,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on dbe8b5eed567:34785 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:31,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:31,156 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:31,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 13:22:31,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:31,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:33883 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:31,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:37926
2025-05-12 13:22:31,297 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 179 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:31,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 13:22:31,299 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.206 s
2025-05-12 13:22:31,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:31,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 13:22:31,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.218360 s
2025-05-12 13:22:31,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 13:22:31,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:31,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:31,535 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:31,536 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:31,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO connection: Opened connection [connectionId{localValue:26, serverValue:10806}] to mongodb:27017
2025-05-12 13:22:31,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1183625}
2025-05-12 13:22:31,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO connection: Opened connection [connectionId{localValue:27, serverValue:10807}] to mongodb:27017
2025-05-12 13:22:31,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:31,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO connection: Closed connection [connectionId{localValue:27, serverValue:10807}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:31,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:31,550 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:31,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:31,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO connection: Opened connection [connectionId{localValue:28, serverValue:10808}] to mongodb:27017
2025-05-12 13:22:31,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=12532686}
2025-05-12 13:22:31,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO connection: Opened connection [connectionId{localValue:29, serverValue:10809}] to mongodb:27017
2025-05-12 13:22:31,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:31,593 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO connection: Closed connection [connectionId{localValue:29, serverValue:10809}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:31,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:31,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:31,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO connection: Opened connection [connectionId{localValue:30, serverValue:10810}] to mongodb:27017
2025-05-12 13:22:31,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:31,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 13:22:31,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:31,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:31,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:31,615 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:31,616 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:31,626 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.2 MiB)
2025-05-12 13:22:31,641 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 13:22:31,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on dbe8b5eed567:34785 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 13:22:31,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:31,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO BlockManagerInfo: Removed broadcast_13_piece0 on dbe8b5eed567:34785 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:31,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:31,661 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 13:22:31,662 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:31,670 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:33883 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:31,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO BlockManagerInfo: Removed broadcast_12_piece0 on dbe8b5eed567:34785 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:31,699 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:33883 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 13:22:31,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:33883 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:31,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 156 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:31,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 13:22:31,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.199 s
2025-05-12 13:22:31,816 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:31,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: running: Set()
2025-05-12 13:22:31,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:31,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:31,845 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:31,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:31,956 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO CodeGenerator: Code generated in 27.57404 ms
2025-05-12 13:22:31,987 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 13:22:31,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:31,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:31,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 13:22:31,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:31,997 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:31 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:32,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.3 MiB)
2025-05-12 13:22:32,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.2 MiB)
2025-05-12 13:22:32,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on dbe8b5eed567:34785 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 13:22:32,014 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:32,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:32,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 13:22:32,020 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:32,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:33883 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 13:22:32,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:37926
2025-05-12 13:22:32,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO BlockManagerInfo: Removed broadcast_14_piece0 on dbe8b5eed567:34785 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:32,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:33883 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:32,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 132 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:32,153 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 13:22:32,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.153 s
2025-05-12 13:22:32,157 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:32,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 13:22:32,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.245903 s
2025-05-12 13:22:32,162 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 4, 'weekly': 4, 'hourly': 8}
2025-05-12 13:22:32,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 13:22:32,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 13:22:32,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on dbe8b5eed567:34785 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:32,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:32,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:32,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:32,237 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:32,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO connection: Opened connection [connectionId{localValue:31, serverValue:10811}] to mongodb:27017
2025-05-12 13:22:32,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1623280}
2025-05-12 13:22:32,244 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO connection: Opened connection [connectionId{localValue:32, serverValue:10812}] to mongodb:27017
2025-05-12 13:22:32,246 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:32,249 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:32 INFO connection: Closed connection [connectionId{localValue:32, serverValue:10812}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:35,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:35,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:35,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:35,775 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 13:22:35,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 13:22:35,777 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 13:22:35,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:35,783 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:35,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 13:22:35,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 13:22:35,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.3 MiB)
2025-05-12 13:22:35,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on dbe8b5eed567:34785 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:35,822 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:35,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:35,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 13:22:35,839 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:33883 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 13:22:35,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 13:22:35,854 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO BlockManagerInfo: Removed broadcast_15_piece0 on dbe8b5eed567:34785 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 13:22:35,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:35 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:33883 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:36,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:33883 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:36,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 467 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:36,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 13:22:36,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.530 s
2025-05-12 13:22:36,317 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:36,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 13:22:36,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.563631 s
2025-05-12 13:22:36,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:36,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:36,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:36,812 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO connection: Opened connection [connectionId{localValue:33, serverValue:10820}] to mongodb:27017
2025-05-12 13:22:36,815 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2611050}
2025-05-12 13:22:36,822 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO connection: Opened connection [connectionId{localValue:34, serverValue:10821}] to mongodb:27017
2025-05-12 13:22:36,830 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:36,832 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO connection: Closed connection [connectionId{localValue:34, serverValue:10821}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:36,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:36 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 13:22:37,084 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO CodeGenerator: Code generated in 135.659586 ms
2025-05-12 13:22:37,096 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:37,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:37,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:37,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO connection: Opened connection [connectionId{localValue:35, serverValue:10825}] to mongodb:27017
2025-05-12 13:22:37,134 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4596874}
2025-05-12 13:22:37,138 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO connection: Opened connection [connectionId{localValue:36, serverValue:10826}] to mongodb:27017
2025-05-12 13:22:37,143 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:37,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO connection: Closed connection [connectionId{localValue:36, serverValue:10826}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:37,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:37,156 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO connection: Opened connection [connectionId{localValue:37, serverValue:10827}] to mongodb:27017
2025-05-12 13:22:37,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:37,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:37,163 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1781995}
2025-05-12 13:22:37,168 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO connection: Opened connection [connectionId{localValue:38, serverValue:10828}] to mongodb:27017
2025-05-12 13:22:37,187 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:37,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO connection: Closed connection [connectionId{localValue:38, serverValue:10828}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:37,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:37,195 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:37,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:37,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 13:22:37,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 13:22:37,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 13:22:37,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:37,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:37,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:37,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.4 KiB, free 434.3 MiB)
2025-05-12 13:22:37,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 434.3 MiB)
2025-05-12 13:22:37,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on dbe8b5eed567:34785 (size: 19.9 KiB, free: 434.4 MiB)
2025-05-12 13:22:37,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:37,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:37,244 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 13:22:37,246 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:37,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:33883 (size: 19.9 KiB, free: 434.4 MiB)
2025-05-12 13:22:37,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 373 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:37,613 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 13:22:37,615 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.398 s
2025-05-12 13:22:37,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:37,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: running: Set()
2025-05-12 13:22:37,636 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:37,638 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:37,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:37,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:37,787 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO CodeGenerator: Code generated in 92.559583 ms
2025-05-12 13:22:37,853 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 13:22:37,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Got job 14 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 13:22:37,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Final stage: ResultStage 21 (foreachPartition at MongoSpark.scala:120)
2025-05-12 13:22:37,867 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 13:22:37,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:37,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:37,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 13:22:37,958 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 22.9 KiB, free 434.3 MiB)
2025-05-12 13:22:37,959 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO BlockManagerInfo: Removed broadcast_17_piece0 on dbe8b5eed567:34785 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:37,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on dbe8b5eed567:34785 (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 13:22:37,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:37,983 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:37,987 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 13:22:37,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:33883 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:37,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:37 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:38,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO BlockManagerInfo: Removed broadcast_18_piece0 on dbe8b5eed567:34785 in memory (size: 19.9 KiB, free: 434.4 MiB)
2025-05-12 13:22:38,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:33883 in memory (size: 19.9 KiB, free: 434.4 MiB)
2025-05-12 13:22:38,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:33883 (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 13:22:38,111 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:37926
2025-05-12 13:22:38,307 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 313 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:38,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 13:22:38,313 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO DAGScheduler: ResultStage 21 (foreachPartition at MongoSpark.scala:120) finished in 0.423 s
2025-05-12 13:22:38,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:38,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 13:22:38,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO DAGScheduler: Job 14 finished: foreachPartition at MongoSpark.scala:120, took 0.468797 s
2025-05-12 13:22:38,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 13:22:38,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 13:22:38,408 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on dbe8b5eed567:34785 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:38,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO SparkContext: Created broadcast 20 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:38,441 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO BlockManagerInfo: Removed broadcast_19_piece0 on dbe8b5eed567:34785 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 13:22:38,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:33883 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 13:22:38,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:38,626 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:38,627 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:38,633 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Opened connection [connectionId{localValue:40, serverValue:10858}] to mongodb:27017
2025-05-12 13:22:38,636 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2527691}
2025-05-12 13:22:38,644 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Opened connection [connectionId{localValue:41, serverValue:10859}] to mongodb:27017
2025-05-12 13:22:38,649 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:38,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Closed connection [connectionId{localValue:41, serverValue:10859}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:38,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 13:22:38,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO CodeGenerator: Code generated in 75.497323 ms
2025-05-12 13:22:38,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:38,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:38,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:38,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Opened connection [connectionId{localValue:42, serverValue:10865}] to mongodb:27017
2025-05-12 13:22:38,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5179405}
2025-05-12 13:22:38,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Opened connection [connectionId{localValue:43, serverValue:10868}] to mongodb:27017
2025-05-12 13:22:38,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:38,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Closed connection [connectionId{localValue:43, serverValue:10868}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:38,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:38,914 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:38,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:38,917 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Opened connection [connectionId{localValue:44, serverValue:10869}] to mongodb:27017
2025-05-12 13:22:38,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4818208}
2025-05-12 13:22:38,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Opened connection [connectionId{localValue:45, serverValue:10871}] to mongodb:27017
2025-05-12 13:22:38,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:38,970 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO connection: Closed connection [connectionId{localValue:45, serverValue:10871}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:38,972 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:38,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:38,983 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:38,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO DAGScheduler: Registering RDD 91 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 13:22:38,994 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 13:22:39,000 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (rdd at MongoSpark.scala:169)
2025-05-12 13:22:39,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:39,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:39,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:39,020 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 39.5 KiB, free 434.4 MiB)
2025-05-12 13:22:39,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 434.3 MiB)
2025-05-12 13:22:39,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on dbe8b5eed567:34785 (size: 18.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:39,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:39,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:39,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
2025-05-12 13:22:39,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 15) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:39,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:33883 (size: 18.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:39,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 15) in 286 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:39,339 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
2025-05-12 13:22:39,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: ShuffleMapStage 22 (rdd at MongoSpark.scala:169) finished in 0.330 s
2025-05-12 13:22:39,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:39,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: running: Set()
2025-05-12 13:22:39,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:39,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:39,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:39,373 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:39,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 13:22:39,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 13:22:39,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Final stage: ResultStage 24 (foreachPartition at MongoSpark.scala:120)
2025-05-12 13:22:39,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
2025-05-12 13:22:39,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:39,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 13:22:39,452 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 48.1 KiB, free 434.3 MiB)
2025-05-12 13:22:39,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 434.3 MiB)
2025-05-12 13:22:39,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on dbe8b5eed567:34785 (size: 21.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:39,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:39,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:39,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
2025-05-12 13:22:39,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO BlockManagerInfo: Removed broadcast_21_piece0 on dbe8b5eed567:34785 in memory (size: 18.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:39,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:33883 in memory (size: 18.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:39,536 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:39,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO BlockManagerInfo: Removed broadcast_20_piece0 on dbe8b5eed567:34785 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:39,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.23.0.8:33883 (size: 21.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:39,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:37926
2025-05-12 13:22:39,841 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 16) in 321 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:39,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
2025-05-12 13:22:39,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: ResultStage 24 (foreachPartition at MongoSpark.scala:120) finished in 0.394 s
2025-05-12 13:22:39,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:39,865 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
2025-05-12 13:22:39,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.418755 s
2025-05-12 13:22:39,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 13:22:39,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 13:22:39,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on dbe8b5eed567:34785 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:39,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO SparkContext: Created broadcast 23 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:39,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:39 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 13:22:40,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:40,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO CodeGenerator: Code generated in 66.564278 ms
2025-05-12 13:22:40,267 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:40,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:40,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:40,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO connection: Opened connection [connectionId{localValue:47, serverValue:10899}] to mongodb:27017
2025-05-12 13:22:40,314 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1489937}
2025-05-12 13:22:40,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO connection: Opened connection [connectionId{localValue:48, serverValue:10900}] to mongodb:27017
2025-05-12 13:22:40,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:40,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO connection: Closed connection [connectionId{localValue:48, serverValue:10900}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:40,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:40,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:40,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:40,339 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO connection: Opened connection [connectionId{localValue:49, serverValue:10901}] to mongodb:27017
2025-05-12 13:22:40,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=9041018}
2025-05-12 13:22:40,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO connection: Opened connection [connectionId{localValue:50, serverValue:10902}] to mongodb:27017
2025-05-12 13:22:40,374 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:40,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO connection: Closed connection [connectionId{localValue:50, serverValue:10902}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:40,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:40,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:40,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:40,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Registering RDD 103 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 8
2025-05-12 13:22:40,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Got map stage job 17 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:40,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:40,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:40,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:40,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:40,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 36.2 KiB, free 434.3 MiB)
2025-05-12 13:22:40,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 434.3 MiB)
2025-05-12 13:22:40,440 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on dbe8b5eed567:34785 (size: 16.8 KiB, free: 434.4 MiB)
2025-05-12 13:22:40,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:40,457 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:40,461 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
2025-05-12 13:22:40,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:40,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:33883 (size: 16.8 KiB, free: 434.4 MiB)
2025-05-12 13:22:40,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 327 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:40,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
2025-05-12 13:22:40,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.362 s
2025-05-12 13:22:40,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:40,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: running: Set()
2025-05-12 13:22:40,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:40,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:40,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:40,832 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:40,919 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO CodeGenerator: Code generated in 61.10311 ms
2025-05-12 13:22:40,946 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Registering RDD 106 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 13:22:40,947 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Got map stage job 18 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:40,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:40,953 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
2025-05-12 13:22:40,955 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:40,963 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:40,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 44.8 KiB, free 434.2 MiB)
2025-05-12 13:22:40,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 434.2 MiB)
2025-05-12 13:22:40,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on dbe8b5eed567:34785 (size: 20.3 KiB, free: 434.3 MiB)
2025-05-12 13:22:41,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:40 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:41,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:41,015 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 13:22:41,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 13:22:41,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.23.0.8:33883 (size: 20.3 KiB, free: 434.3 MiB)
2025-05-12 13:22:41,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:37926
2025-05-12 13:22:41,254 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 18) in 238 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:41,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 13:22:41,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0) finished in 0.299 s
2025-05-12 13:22:41,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:41,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: running: Set()
2025-05-12 13:22:41,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:41,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:41,329 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO CodeGenerator: Code generated in 19.039612 ms
2025-05-12 13:22:41,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 13:22:41,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Got job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:41,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Final stage: ResultStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:41,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
2025-05-12 13:22:41,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:41,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:41,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.2 MiB)
2025-05-12 13:22:41,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.2 MiB)
2025-05-12 13:22:41,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on dbe8b5eed567:34785 (size: 5.5 KiB, free: 434.3 MiB)
2025-05-12 13:22:41,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:41,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:41,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 13:22:41,441 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:41,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:33883 (size: 5.5 KiB, free: 434.3 MiB)
2025-05-12 13:22:41,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:37926
2025-05-12 13:22:41,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 150 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:41,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 13:22:41,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: ResultStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.192 s
2025-05-12 13:22:41,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:41,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
2025-05-12 13:22:41,605 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO DAGScheduler: Job 19 finished: count at NativeMethodAccessorImpl.java:0, took 0.214123 s
2025-05-12 13:22:41,736 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 13:22:41,867 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:41,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:41,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:41,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO connection: Opened connection [connectionId{localValue:52, serverValue:10942}] to mongodb:27017
2025-05-12 13:22:41,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2144216}
2025-05-12 13:22:41,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO connection: Opened connection [connectionId{localValue:53, serverValue:10944}] to mongodb:27017
2025-05-12 13:22:41,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:41,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO connection: Closed connection [connectionId{localValue:53, serverValue:10944}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:41,938 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:41,946 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:41,948 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:41,951 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO connection: Opened connection [connectionId{localValue:54, serverValue:10946}] to mongodb:27017
2025-05-12 13:22:41,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1527477}
2025-05-12 13:22:41,960 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO connection: Opened connection [connectionId{localValue:55, serverValue:10949}] to mongodb:27017
2025-05-12 13:22:41,979 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:41,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO connection: Closed connection [connectionId{localValue:55, serverValue:10949}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:41,988 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:41,994 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:41,997 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:42,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Registering RDD 114 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 13:22:42,008 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:42,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:42,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:42,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:42,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:42,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 39.5 KiB, free 434.2 MiB)
2025-05-12 13:22:42,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.1 MiB)
2025-05-12 13:22:42,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on dbe8b5eed567:34785 (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 13:22:42,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:42,062 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:42,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
2025-05-12 13:22:42,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 20) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 13:22:42,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:33883 (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 13:22:42,222 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 20) in 153 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:42,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
2025-05-12 13:22:42,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0) finished in 0.190 s
2025-05-12 13:22:42,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 13:22:42,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: running: Set()
2025-05-12 13:22:42,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: waiting: Set()
2025-05-12 13:22:42,228 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: failed: Set()
2025-05-12 13:22:42,246 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 13:22:42,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 13:22:42,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 13:22:42,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 13:22:42,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Final stage: ResultStage 33 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 13:22:42,395 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
2025-05-12 13:22:42,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:42,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 13:22:42,412 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 46.1 KiB, free 434.1 MiB)
2025-05-12 13:22:42,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.1 MiB)
2025-05-12 13:22:42,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on dbe8b5eed567:34785 (size: 21.1 KiB, free: 434.3 MiB)
2025-05-12 13:22:42,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:42,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:42,457 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
2025-05-12 13:22:42,460 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_27_piece0 on dbe8b5eed567:34785 in memory (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 13:22:42,461 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.23.0.8:33883 in memory (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 13:22:42,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 13:22:42,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_25_piece0 on dbe8b5eed567:34785 in memory (size: 20.3 KiB, free: 434.3 MiB)
2025-05-12 13:22:42,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.23.0.8:33883 in memory (size: 20.3 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,537 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:33883 (size: 21.1 KiB, free: 434.3 MiB)
2025-05-12 13:22:42,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.23.0.8:33883 in memory (size: 21.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_22_piece0 on dbe8b5eed567:34785 in memory (size: 21.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_23_piece0 on dbe8b5eed567:34785 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 13:22:42,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:37926
2025-05-12 13:22:42,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_24_piece0 on dbe8b5eed567:34785 in memory (size: 16.8 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:33883 in memory (size: 16.8 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_26_piece0 on dbe8b5eed567:34785 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,639 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.23.0.8:33883 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 21) in 194 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:42,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
2025-05-12 13:22:42,654 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: ResultStage 33 (count at NativeMethodAccessorImpl.java:0) finished in 0.265 s
2025-05-12 13:22:42,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:42,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
2025-05-12 13:22:42,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.277030 s
2025-05-12 13:22:42,671 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 1, 'active_users': 1}
2025-05-12 13:22:42,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 13:22:42,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 13:22:42,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on dbe8b5eed567:34785 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:42,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO SparkContext: Created broadcast 29 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:42,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:42,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:42,736 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:42,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO connection: Opened connection [connectionId{localValue:57, serverValue:10951}] to mongodb:27017
2025-05-12 13:22:42,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2685106}
2025-05-12 13:22:42,758 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO connection: Opened connection [connectionId{localValue:58, serverValue:10952}] to mongodb:27017
2025-05-12 13:22:42,759 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:42,761 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO connection: Closed connection [connectionId{localValue:58, serverValue:10952}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:42,775 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:42,784 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:42,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:42,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 13:22:42,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Got job 22 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 13:22:42,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Final stage: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 13:22:42,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:42,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:42,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 13:22:42,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 13:22:42,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.3 MiB)
2025-05-12 13:22:42,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on dbe8b5eed567:34785 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,874 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:42,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:42,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
2025-05-12 13:22:42,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 13:22:42,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:33883 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:42,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:33883 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:42,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 22) in 122 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:43,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:42 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
2025-05-12 13:22:43,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88) finished in 0.148 s
2025-05-12 13:22:43,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:43,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
2025-05-12 13:22:43,014 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Job 22 finished: treeAggregate at MongoInferSchema.scala:88, took 0.158410 s
2025-05-12 13:22:43,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 13:22:43,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 13:22:43,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on dbe8b5eed567:34785 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:43,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:43,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:43,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:43,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:43,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO connection: Opened connection [connectionId{localValue:60, serverValue:10954}] to mongodb:27017
2025-05-12 13:22:43,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4852534}
2025-05-12 13:22:43,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO connection: Opened connection [connectionId{localValue:61, serverValue:10955}] to mongodb:27017
2025-05-12 13:22:43,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:43,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO connection: Closed connection [connectionId{localValue:61, serverValue:10955}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:22:43,101 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:43,103 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:43,104 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:22:43,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 13:22:43,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Got job 23 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 13:22:43,131 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Final stage: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 13:22:43,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:43,142 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:43,144 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 13:22:43,145 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 13:22:43,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.3 MiB)
2025-05-12 13:22:43,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on dbe8b5eed567:34785 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:43,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:43,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:43,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 13:22:43,157 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 23) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 13:22:43,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:33883 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:43,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:33883 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:43,250 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 23) in 93 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:22:43,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 13:22:43,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88) finished in 0.121 s
2025-05-12 13:22:43,254 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:22:43,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 13:22:43,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO DAGScheduler: Job 23 finished: treeAggregate at MongoInferSchema.scala:88, took 0.130474 s
2025-05-12 13:22:43,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO SparkUI: Stopped Spark web UI at http://dbe8b5eed567:4040
2025-05-12 13:22:43,460 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 13:22:43,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 13:22:43,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 13:22:43,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO MemoryStore: MemoryStore cleared
2025-05-12 13:22:43,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO BlockManager: BlockManager stopped
2025-05-12 13:22:43,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 13:22:43,787 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 13:22:43,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:43 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 13:22:43,987 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 13:22:43,988 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 13:22:43,994 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 13:22:43,996 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 13:22:44,002 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 13:22:44,003 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 13:22:44,007 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 13:22:44,008 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 13:22:44,011 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 13:22:44,011 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 13:22:44,028 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 13:22:44,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:44 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 13:22:44,133 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-f58903e2-8310-4d70-8072-09f8fb143771/pyspark-16d077d4-2d03-4667-b199-f2d02b867d45
2025-05-12 13:22:44,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-3405a226-14d4-4801-8904-50c7454ec128
2025-05-12 13:22:44,172 - SparkScheduler - INFO - [trend_analysis] 25/05/12 13:22:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-f58903e2-8310-4d70-8072-09f8fb143771
2025-05-12 13:22:44,286 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 13:22:44,287 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 13:22:44,289 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 13:22:44,290 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-f9019c78-c9ee-4050-ad50-5da7fac5f01e;1.0
2025-05-12 13:22:44,292 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 13:22:44,293 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 13:22:44,296 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 13:22:44,298 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 13:22:44,300 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 13:22:44,301 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...
2025-05-12 13:22:44,302 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (851ms)
2025-05-12 13:22:44,304 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...
2025-05-12 13:22:44,306 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (347ms)
2025-05-12 13:22:44,308 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...
2025-05-12 13:22:44,309 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (383ms)
2025-05-12 13:22:44,310 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...
2025-05-12 13:22:44,312 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (597ms)
2025-05-12 13:22:44,313 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 3091ms :: artifacts dl 2214ms
2025-05-12 13:22:44,315 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 13:22:44,316 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 13:22:44,318 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 13:22:44,319 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 13:22:44,320 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 13:22:44,321 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 13:22:44,322 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 13:22:44,323 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 13:22:44,325 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 13:22:44,327 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   4   |   4   |   0   ||   4   |   4   |
2025-05-12 13:22:44,328 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 13:22:44,329 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-f9019c78-c9ee-4050-ad50-5da7fac5f01e
2025-05-12 13:22:44,331 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 13:22:44,331 - SparkScheduler - ERROR - [trend_analysis] 4 artifacts copied, 0 already retrieved (2728kB/71ms)
2025-05-12 13:22:44,333 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 13:22:44,334 - SparkScheduler - INFO - Job trend_analysis duration: 55.75 seconds
2025-05-12 13:22:44,338 - SparkScheduler - INFO - Starting job: user_recommender - Generate user recommendations
2025-05-12 13:22:44,340 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/user_recommender.py
2025-05-12 13:22:47,786 - SparkScheduler - INFO - [user_recommender] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 13:22:48,523 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 13:22:51,366 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 13:22:51,401 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO ResourceUtils: ==============================================================
2025-05-12 13:22:51,402 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 13:22:51,404 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO ResourceUtils: ==============================================================
2025-05-12 13:22:51,405 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO SparkContext: Submitted application: MiniTwitterUserRecommender
2025-05-12 13:22:51,446 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 13:22:51,462 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 13:22:51,465 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 13:22:51,530 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO SecurityManager: Changing view acls to: spark
2025-05-12 13:22:51,531 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 13:22:51,533 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO SecurityManager: Changing view acls groups to:
2025-05-12 13:22:51,534 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 13:22:51,535 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 13:22:52,071 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO Utils: Successfully started service 'sparkDriver' on port 44017.
2025-05-12 13:22:52,136 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 13:22:52,202 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 13:22:52,252 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 13:22:52,255 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 13:22:52,267 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 13:22:52,406 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-081e779b-11cb-47f8-b905-b193215a5563
2025-05-12 13:22:52,469 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 13:22:52,523 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 13:22:52,855 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 13:22:52,931 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://dbe8b5eed567:44017/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747056171358
2025-05-12 13:22:52,932 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://dbe8b5eed567:44017/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747056171358
2025-05-12 13:22:52,932 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://dbe8b5eed567:44017/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747056171358
2025-05-12 13:22:52,933 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://dbe8b5eed567:44017/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747056171358
2025-05-12 13:22:52,950 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://dbe8b5eed567:44017/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747056171358
2025-05-12 13:22:52,953 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-6d9328e2-90ad-48ad-98f2-28437229d7f3/userFiles-207745cb-d7bd-4ebe-a2f9-0a5e6e1ba2d8/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 13:22:52,995 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://dbe8b5eed567:44017/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747056171358
2025-05-12 13:22:52,996 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:52 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-6d9328e2-90ad-48ad-98f2-28437229d7f3/userFiles-207745cb-d7bd-4ebe-a2f9-0a5e6e1ba2d8/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 13:22:53,018 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://dbe8b5eed567:44017/files/org.mongodb_bson-4.0.5.jar with timestamp 1747056171358
2025-05-12 13:22:53,020 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-6d9328e2-90ad-48ad-98f2-28437229d7f3/userFiles-207745cb-d7bd-4ebe-a2f9-0a5e6e1ba2d8/org.mongodb_bson-4.0.5.jar
2025-05-12 13:22:53,043 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://dbe8b5eed567:44017/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747056171358
2025-05-12 13:22:53,043 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-6d9328e2-90ad-48ad-98f2-28437229d7f3/userFiles-207745cb-d7bd-4ebe-a2f9-0a5e6e1ba2d8/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 13:22:53,220 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 13:22:53,291 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 37 ms (0 ms spent in bootstraps)
2025-05-12 13:22:53,461 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512132253-0001
2025-05-12 13:22:53,469 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512132253-0001/0 on worker-20250512132122-172.23.0.8-44075 (172.23.0.8:44075) with 2 core(s)
2025-05-12 13:22:53,478 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512132253-0001/0 on hostPort 172.23.0.8:44075 with 2 core(s), 1024.0 MiB RAM
2025-05-12 13:22:53,495 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44617.
2025-05-12 13:22:53,500 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO NettyBlockTransferService: Server created on dbe8b5eed567:44617
2025-05-12 13:22:53,517 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 13:22:53,601 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dbe8b5eed567, 44617, None)
2025-05-12 13:22:53,658 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO BlockManagerMasterEndpoint: Registering block manager dbe8b5eed567:44617 with 434.4 MiB RAM, BlockManagerId(driver, dbe8b5eed567, 44617, None)
2025-05-12 13:22:53,689 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dbe8b5eed567, 44617, None)
2025-05-12 13:22:53,750 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, dbe8b5eed567, 44617, None)
2025-05-12 13:22:53,838 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512132253-0001/0 is now RUNNING
2025-05-12 13:22:54,435 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:54 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 13:22:55,031 - SparkScheduler - INFO - [user_recommender] Starting Mini Twitter User Recommender...
2025-05-12 13:22:55,047 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 13:22:55,065 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:55 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 13:22:56,725 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 13:22:56,826 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 13:22:56,838 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on dbe8b5eed567:44617 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:22:56,864 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:56 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 13:22:57,072 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 13:22:57,192 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 13:22:57,225 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 13:22:57,250 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO connection: Opened connection [connectionId{localValue:1, serverValue:10957}] to mongodb:27017
2025-05-12 13:22:57,260 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6891526}
2025-05-12 13:22:57,302 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO connection: Opened connection [connectionId{localValue:2, serverValue:10959}] to mongodb:27017
2025-05-12 13:22:57,878 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 13:22:57,913 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO DAGScheduler: Got job 0 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 13:22:57,917 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 13:22:57,920 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 13:22:57,922 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO DAGScheduler: Missing parents: List()
2025-05-12 13:22:57,945 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 13:22:58,043 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 13:22:58,059 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 13:22:58,061 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on dbe8b5eed567:44617 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:22:58,066 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 13:22:58,152 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 13:22:58,160 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 13:22:58,885 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:52070) with ID 0,  ResourceProfileId 0
2025-05-12 13:22:59,033 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:35959 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 35959, None)
2025-05-12 13:22:59,451 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 13:22:59,891 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:22:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:35959 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:23:00,216 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:35959 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 13:23:00,702 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1270 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 13:23:00,705 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 13:23:00,714 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:00 INFO DAGScheduler: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88) finished in 2.699 s
2025-05-12 13:23:00,721 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 13:23:00,722 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
2025-05-12 13:23:00,777 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:00 INFO DAGScheduler: Job 0 finished: treeAggregate at MongoInferSchema.scala:88, took 2.898318 s
2025-05-12 13:23:01,994 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on dbe8b5eed567:44617 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:23:02,006 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:35959 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 13:23:02,901 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:02 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 13:23:02,907 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:02 INFO connection: Closed connection [connectionId{localValue:2, serverValue:10959}] to mongodb:27017 because the pool has been closed.
2025-05-12 13:23:03,480 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO SparkUI: Stopped Spark web UI at http://dbe8b5eed567:4040
2025-05-12 13:23:03,484 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 13:23:03,485 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 13:23:03,515 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 13:23:03,548 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO MemoryStore: MemoryStore cleared
2025-05-12 13:23:03,549 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO BlockManager: BlockManager stopped
2025-05-12 13:23:03,552 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 13:23:03,556 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 13:23:03,605 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:03 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 13:23:04,044 - SparkScheduler - INFO - [user_recommender] Traceback (most recent call last):
2025-05-12 13:23:04,045 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 170, in <module>
2025-05-12 13:23:04,050 - SparkScheduler - INFO - [user_recommender] main()
2025-05-12 13:23:04,050 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 159, in main
2025-05-12 13:23:04,055 - SparkScheduler - INFO - [user_recommender] recommendation_results = generate_user_recommendations(spark)
2025-05-12 13:23:04,056 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 35, in generate_user_recommendations
2025-05-12 13:23:04,060 - SparkScheduler - INFO - [user_recommender] .load())
2025-05-12 13:23:04,061 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 184, in load
2025-05-12 13:23:04,061 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 13:23:04,063 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
2025-05-12 13:23:04,064 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
2025-05-12 13:23:04,067 - SparkScheduler - INFO - [user_recommender] py4j.protocol.Py4JJavaError: An error occurred while calling o45.load.
2025-05-12 13:23:04,068 - SparkScheduler - INFO - [user_recommender] : java.sql.SQLException: No suitable driver
2025-05-12 13:23:04,069 - SparkScheduler - INFO - [user_recommender] at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)
2025-05-12 13:23:04,070 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)
2025-05-12 13:23:04,070 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 13:23:04,071 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)
2025-05-12 13:23:04,072 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)
2025-05-12 13:23:04,072 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
2025-05-12 13:23:04,073 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
2025-05-12 13:23:04,074 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
2025-05-12 13:23:04,075 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
2025-05-12 13:23:04,075 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 13:23:04,076 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
2025-05-12 13:23:04,077 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
2025-05-12 13:23:04,078 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2025-05-12 13:23:04,079 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
2025-05-12 13:23:04,080 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2025-05-12 13:23:04,081 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.reflect.Method.invoke(Method.java:568)
2025-05-12 13:23:04,082 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2025-05-12 13:23:04,083 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2025-05-12 13:23:04,084 - SparkScheduler - INFO - [user_recommender] at py4j.Gateway.invoke(Gateway.java:282)
2025-05-12 13:23:04,084 - SparkScheduler - INFO - [user_recommender] at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2025-05-12 13:23:04,085 - SparkScheduler - INFO - [user_recommender] at py4j.commands.CallCommand.execute(CallCommand.java:79)
2025-05-12 13:23:04,086 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-05-12 13:23:04,086 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-05-12 13:23:04,087 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.Thread.run(Thread.java:840)
2025-05-12 13:23:04,088 - SparkScheduler - INFO - [user_recommender] 
2025-05-12 13:23:04,207 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:04 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 13:23:04,208 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d9328e2-90ad-48ad-98f2-28437229d7f3/pyspark-6ccf0bdc-36a1-4ca3-8c9e-366fad00831b
2025-05-12 13:23:04,224 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-15394354-3315-4d14-a22a-a838e1af4d86
2025-05-12 13:23:04,241 - SparkScheduler - INFO - [user_recommender] 25/05/12 13:23:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d9328e2-90ad-48ad-98f2-28437229d7f3
2025-05-12 13:23:04,325 - SparkScheduler - ERROR - [user_recommender] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 13:23:04,326 - SparkScheduler - ERROR - [user_recommender] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 13:23:04,327 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 13:23:04,327 - SparkScheduler - ERROR - [user_recommender] :: resolving dependencies :: org.apache.spark#spark-submit-parent-7812cf57-b0ad-4d5d-b567-c13ebf62420e;1.0
2025-05-12 13:23:04,328 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 13:23:04,328 - SparkScheduler - ERROR - [user_recommender] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 13:23:04,329 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 13:23:04,329 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#bson;4.0.5 in central
2025-05-12 13:23:04,330 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 13:23:04,330 - SparkScheduler - ERROR - [user_recommender] :: resolution report :: resolve 283ms :: artifacts dl 16ms
2025-05-12 13:23:04,331 - SparkScheduler - ERROR - [user_recommender] :: modules in use:
2025-05-12 13:23:04,331 - SparkScheduler - ERROR - [user_recommender] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 13:23:04,332 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 13:23:04,333 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 13:23:04,333 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 13:23:04,334 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 13:23:04,334 - SparkScheduler - ERROR - [user_recommender] |                  |            modules            ||   artifacts   |
2025-05-12 13:23:04,335 - SparkScheduler - ERROR - [user_recommender] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 13:23:04,336 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 13:23:04,336 - SparkScheduler - ERROR - [user_recommender] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 13:23:04,337 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 13:23:04,337 - SparkScheduler - ERROR - [user_recommender] :: retrieving :: org.apache.spark#spark-submit-parent-7812cf57-b0ad-4d5d-b567-c13ebf62420e
2025-05-12 13:23:04,338 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 13:23:04,338 - SparkScheduler - ERROR - [user_recommender] 0 artifacts copied, 4 already retrieved (0kB/9ms)
2025-05-12 13:23:04,339 - SparkScheduler - ERROR - Job user_recommender failed with exit code 1
2025-05-12 13:23:04,340 - SparkScheduler - INFO - Job user_recommender duration: 20.00 seconds
2025-05-12 13:23:04,341 - SparkScheduler - INFO - Starting job: content_analyzer - Analyze tweet content and topics
2025-05-12 13:23:04,341 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/content_analyzer.py
2025-05-12 13:23:10,818 - SparkScheduler - INFO - [content_analyzer] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 13:23:11,725 - SparkScheduler - INFO - [content_analyzer] 25/05/12 13:23:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 13:23:17,587 - SparkScheduler - INFO - [content_analyzer] [nltk_data] Downloading package punkt to /nltk_data...
2025-05-12 13:23:17,588 - SparkScheduler - INFO - [content_analyzer] Traceback (most recent call last):
2025-05-12 13:23:17,589 - SparkScheduler - INFO - [content_analyzer] File "/opt/spark-jobs/content_analyzer.py", line 11, in <module>
2025-05-12 13:23:17,593 - SparkScheduler - INFO - [content_analyzer] nltk.download('punkt')
2025-05-12 13:23:17,594 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 777, in download
2025-05-12 13:23:17,596 - SparkScheduler - INFO - [content_analyzer] for msg in self.incr_download(info_or_id, download_dir, force):
2025-05-12 13:23:17,597 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 642, in incr_download
2025-05-12 13:23:17,598 - SparkScheduler - INFO - [content_analyzer] yield from self._download_package(info, download_dir, force)
2025-05-12 13:23:17,599 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 699, in _download_package
2025-05-12 13:23:17,600 - SparkScheduler - INFO - [content_analyzer] os.makedirs(download_dir)
2025-05-12 13:23:17,601 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/os.py", line 225, in makedirs
2025-05-12 13:23:17,602 - SparkScheduler - INFO - [content_analyzer] mkdir(name, mode)
2025-05-12 13:23:17,603 - SparkScheduler - INFO - [content_analyzer] PermissionError: [Errno 13] Permission denied: '/nltk_data'
2025-05-12 13:23:17,841 - SparkScheduler - INFO - [content_analyzer] 25/05/12 13:23:17 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 13:23:17,843 - SparkScheduler - INFO - [content_analyzer] 25/05/12 13:23:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-6fd540c6-0b78-4181-a946-d7d7281e51e0
2025-05-12 13:23:17,901 - SparkScheduler - ERROR - [content_analyzer] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 13:23:17,902 - SparkScheduler - ERROR - [content_analyzer] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 13:23:17,903 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 13:23:17,904 - SparkScheduler - ERROR - [content_analyzer] :: resolving dependencies :: org.apache.spark#spark-submit-parent-5dcc5fb2-f6bd-4c14-846d-187facbdb53a;1.0
2025-05-12 13:23:17,905 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 13:23:17,906 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 13:23:17,906 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 13:23:17,907 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#bson;4.0.5 in central
2025-05-12 13:23:17,908 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 13:23:17,909 - SparkScheduler - ERROR - [content_analyzer] :: resolution report :: resolve 383ms :: artifacts dl 13ms
2025-05-12 13:23:17,910 - SparkScheduler - ERROR - [content_analyzer] :: modules in use:
2025-05-12 13:23:17,911 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 13:23:17,912 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 13:23:17,913 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 13:23:17,913 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 13:23:17,914 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 13:23:17,915 - SparkScheduler - ERROR - [content_analyzer] |                  |            modules            ||   artifacts   |
2025-05-12 13:23:17,915 - SparkScheduler - ERROR - [content_analyzer] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 13:23:17,916 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 13:23:17,918 - SparkScheduler - ERROR - [content_analyzer] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 13:23:17,918 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 13:23:17,919 - SparkScheduler - ERROR - [content_analyzer] :: retrieving :: org.apache.spark#spark-submit-parent-5dcc5fb2-f6bd-4c14-846d-187facbdb53a
2025-05-12 13:23:17,920 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 13:23:17,921 - SparkScheduler - ERROR - [content_analyzer] 0 artifacts copied, 4 already retrieved (0kB/12ms)
2025-05-12 13:23:17,922 - SparkScheduler - ERROR - Job content_analyzer failed with exit code 1
2025-05-12 13:23:17,923 - SparkScheduler - INFO - Job content_analyzer duration: 13.58 seconds
2025-05-12 13:23:17,924 - SparkScheduler - INFO - Scheduler running. Press Ctrl+C to exit.
2025-05-12 15:51:26,307 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 15:51:26,311 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 15:51:30,354 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 15:51:31,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 15:51:33,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:33 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 15:51:33,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:33 INFO ResourceUtils: ==============================================================
2025-05-12 15:51:33,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:33 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 15:51:33,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:33 INFO ResourceUtils: ==============================================================
2025-05-12 15:51:33,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:33 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 15:51:33,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 15:51:33,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:33 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 15:51:33,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 15:51:34,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO SecurityManager: Changing view acls to: spark
2025-05-12 15:51:34,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 15:51:34,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO SecurityManager: Changing view acls groups to:
2025-05-12 15:51:34,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 15:51:34,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 15:51:34,751 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO Utils: Successfully started service 'sparkDriver' on port 39317.
2025-05-12 15:51:34,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 15:51:34,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 15:51:34,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 15:51:34,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 15:51:34,909 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 15:51:34,954 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-657f0eb2-c4fb-4553-8d81-9084690407a0
2025-05-12 15:51:34,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 15:51:35,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 15:51:35,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 15:51:35,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://dbe8b5eed567:39317/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747065093694
2025-05-12 15:51:35,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://dbe8b5eed567:39317/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747065093694
2025-05-12 15:51:35,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://dbe8b5eed567:39317/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747065093694
2025-05-12 15:51:35,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://dbe8b5eed567:39317/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747065093694
2025-05-12 15:51:35,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://dbe8b5eed567:39317/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747065093694
2025-05-12 15:51:35,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-fe956c86-af34-44bd-8ee2-900e6117805d/userFiles-5ceb5fff-a70d-4276-9d54-3183b0aa8065/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 15:51:35,470 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://dbe8b5eed567:39317/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747065093694
2025-05-12 15:51:35,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-fe956c86-af34-44bd-8ee2-900e6117805d/userFiles-5ceb5fff-a70d-4276-9d54-3183b0aa8065/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 15:51:35,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://dbe8b5eed567:39317/files/org.mongodb_bson-4.0.5.jar with timestamp 1747065093694
2025-05-12 15:51:35,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-fe956c86-af34-44bd-8ee2-900e6117805d/userFiles-5ceb5fff-a70d-4276-9d54-3183b0aa8065/org.mongodb_bson-4.0.5.jar
2025-05-12 15:51:35,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://dbe8b5eed567:39317/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747065093694
2025-05-12 15:51:35,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-fe956c86-af34-44bd-8ee2-900e6117805d/userFiles-5ceb5fff-a70d-4276-9d54-3183b0aa8065/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 15:51:35,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:35 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 15:51:36,107 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 110 ms (0 ms spent in bootstraps)
2025-05-12 15:51:36,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512155136-0002
2025-05-12 15:51:36,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512155136-0002/0 on worker-20250512132122-172.23.0.8-44075 (172.23.0.8:44075) with 2 core(s)
2025-05-12 15:51:36,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512155136-0002/0 on hostPort 172.23.0.8:44075 with 2 core(s), 1024.0 MiB RAM
2025-05-12 15:51:36,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36039.
2025-05-12 15:51:36,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO NettyBlockTransferService: Server created on dbe8b5eed567:36039
2025-05-12 15:51:36,307 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 15:51:36,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dbe8b5eed567, 36039, None)
2025-05-12 15:51:36,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO BlockManagerMasterEndpoint: Registering block manager dbe8b5eed567:36039 with 434.4 MiB RAM, BlockManagerId(driver, dbe8b5eed567, 36039, None)
2025-05-12 15:51:36,339 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dbe8b5eed567, 36039, None)
2025-05-12 15:51:36,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, dbe8b5eed567, 36039, None)
2025-05-12 15:51:36,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512155136-0002/0 is now RUNNING
2025-05-12 15:51:37,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:37 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 15:51:37,455 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 15:51:37,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 15:51:37,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:37 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 15:51:39,715 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 15:51:39,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 15:51:39,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on dbe8b5eed567:36039 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 15:51:39,806 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:39 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:41,610 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:41 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:59464) with ID 0,  ResourceProfileId 0
2025-05-12 15:51:41,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:41 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:34943 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 34943, None)
2025-05-12 15:51:44,267 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:44 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:44,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:44 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:44,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:44 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:44,339 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:44 INFO connection: Opened connection [connectionId{localValue:1, serverValue:20183}] to mongodb:27017
2025-05-12 15:51:44,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:44 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3382759}
2025-05-12 15:51:44,355 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:44 INFO connection: Opened connection [connectionId{localValue:2, serverValue:20186}] to mongodb:27017
2025-05-12 15:51:44,606 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:44 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 15:51:45,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO CodeGenerator: Code generated in 229.413151 ms
2025-05-12 15:51:45,670 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:45,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:45,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO connection: Opened connection [connectionId{localValue:3, serverValue:20223}] to mongodb:27017
2025-05-12 15:51:45,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1358927}
2025-05-12 15:51:45,682 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO connection: Opened connection [connectionId{localValue:4, serverValue:20224}] to mongodb:27017
2025-05-12 15:51:45,752 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 15:51:45,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 15:51:45,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 15:51:45,758 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:45,759 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:45,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:45,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 15:51:45,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 15:51:45,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on dbe8b5eed567:36039 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:45,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:45,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:45,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 15:51:45,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:43,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:34943 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:48,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:34943 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 15:51:50,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:50 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:50,663 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:50 INFO connection: Closed connection [connectionId{localValue:2, serverValue:20186}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:51,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5186 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:51,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 15:51:51,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 5.283 s
2025-05-12 15:51:51,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:51,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: running: Set()
2025-05-12 15:51:51,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:51,070 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:51,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:51,173 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO CodeGenerator: Code generated in 23.516879 ms
2025-05-12 15:51:51,176 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:51,220 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO CodeGenerator: Code generated in 25.741318 ms
2025-05-12 15:51:51,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 15:51:51,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 15:51:51,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 15:51:51,329 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 15:51:51,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:51,335 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:51,355 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 15:51:51,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 15:51:51,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on dbe8b5eed567:36039 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:51,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:51,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:51,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 15:51:51,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:51,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on dbe8b5eed567:36039 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:51,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:34943 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:51,458 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:34943 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:51,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:59464
2025-05-12 15:51:51,959 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:51,960 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:51 INFO connection: Closed connection [connectionId{localValue:4, serverValue:20224}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:52,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 761 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:52,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 15:51:52,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 0.805 s
2025-05-12 15:51:52,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:52,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 15:51:52,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 0.835489 s
2025-05-12 15:51:52,170 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 15:51:52,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on dbe8b5eed567:36039 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:34943 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 15:51:52,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on dbe8b5eed567:36039 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 15:51:52,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:52,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:52,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:52,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:52,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO connection: Opened connection [connectionId{localValue:5, serverValue:20369}] to mongodb:27017
2025-05-12 15:51:52,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=939980}
2025-05-12 15:51:52,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO connection: Opened connection [connectionId{localValue:6, serverValue:20370}] to mongodb:27017
2025-05-12 15:51:52,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 15:51:52,456 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO CodeGenerator: Code generated in 51.378766 ms
2025-05-12 15:51:52,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:52,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:52,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:52,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO connection: Opened connection [connectionId{localValue:7, serverValue:20374}] to mongodb:27017
2025-05-12 15:51:52,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1157144}
2025-05-12 15:51:52,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO connection: Opened connection [connectionId{localValue:8, serverValue:20375}] to mongodb:27017
2025-05-12 15:51:52,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 15:51:52,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 15:51:52,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 15:51:52,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:52,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:52,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:52,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 15:51:52,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 15:51:52,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on dbe8b5eed567:36039 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:52,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:52,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 15:51:52,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:52,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:52,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO connection: Closed connection [connectionId{localValue:8, serverValue:20375}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:52,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:52,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO connection: Closed connection [connectionId{localValue:6, serverValue:20370}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:52,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Removed broadcast_3_piece0 on dbe8b5eed567:36039 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 15:51:52,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:34943 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 180 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:52,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 15:51:52,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.203 s
2025-05-12 15:51:52,699 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:52,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: running: Set()
2025-05-12 15:51:52,701 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:52,701 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:52,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:52,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:52,761 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 15:51:52,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 15:51:52,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 15:51:52,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 15:51:52,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:52,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:52,783 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 15:51:52,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 15:51:52,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on dbe8b5eed567:36039 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Removed broadcast_4_piece0 on dbe8b5eed567:36039 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:52,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:52,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 15:51:52,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:34943 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:52,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:34943 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:59464
2025-05-12 15:51:52,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 149 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:52,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 15:51:52,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.192 s
2025-05-12 15:51:52,970 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:52,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 15:51:52,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.208362 s
2025-05-12 15:51:52,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 15:51:52,985 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 15:51:52,988 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on dbe8b5eed567:36039 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 15:51:52,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:52,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Removed broadcast_5_piece0 on dbe8b5eed567:36039 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:52,994 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:52 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:34943 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:53,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:53,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:53,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:53,041 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Opened connection [connectionId{localValue:9, serverValue:20390}] to mongodb:27017
2025-05-12 15:51:53,043 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=980733}
2025-05-12 15:51:53,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Opened connection [connectionId{localValue:10, serverValue:20392}] to mongodb:27017
2025-05-12 15:51:53,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:53,049 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Closed connection [connectionId{localValue:10, serverValue:20392}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:53,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 15:51:53,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:53,371 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO CodeGenerator: Code generated in 82.20115 ms
2025-05-12 15:51:53,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:53,392 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:53,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:53,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Opened connection [connectionId{localValue:11, serverValue:20402}] to mongodb:27017
2025-05-12 15:51:53,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1091456}
2025-05-12 15:51:53,399 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Opened connection [connectionId{localValue:12, serverValue:20403}] to mongodb:27017
2025-05-12 15:51:53,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:53,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Closed connection [connectionId{localValue:12, serverValue:20403}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:53,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:53,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:53,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:53,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Opened connection [connectionId{localValue:13, serverValue:20404}] to mongodb:27017
2025-05-12 15:51:53,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=960507}
2025-05-12 15:51:53,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Opened connection [connectionId{localValue:14, serverValue:20405}] to mongodb:27017
2025-05-12 15:51:53,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:53,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO connection: Closed connection [connectionId{localValue:14, serverValue:20405}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:53,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:53,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:53,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:53,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 15:51:53,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 15:51:53,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 15:51:53,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:53,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:53,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:53,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 15:51:53,447 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 15:51:53,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on dbe8b5eed567:36039 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:53,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:53,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:53,451 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 15:51:53,453 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:53,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:34943 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:53,690 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 237 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:53,691 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 15:51:53,693 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.261 s
2025-05-12 15:51:53,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:53,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: running: Set()
2025-05-12 15:51:53,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:53,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:53,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:53,735 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO CodeGenerator: Code generated in 15.329978 ms
2025-05-12 15:51:53,739 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:53,783 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO CodeGenerator: Code generated in 30.320771 ms
2025-05-12 15:51:53,831 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 15:51:53,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 15:51:53,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 15:51:53,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 15:51:53,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:53,839 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:53,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.3 MiB)
2025-05-12 15:51:53,862 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)
2025-05-12 15:51:53,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on dbe8b5eed567:36039 (size: 27.2 KiB, free: 434.3 MiB)
2025-05-12 15:51:53,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:53,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:53,867 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 15:51:53,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:53,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:34943 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 15:51:53,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:59464
2025-05-12 15:51:54,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 224 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:54,095 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 15:51:54,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.249 s
2025-05-12 15:51:54,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:54,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 15:51:54,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.267429 s
2025-05-12 15:51:54,106 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.2 MiB)
2025-05-12 15:51:54,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.2 MiB)
2025-05-12 15:51:54,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on dbe8b5eed567:36039 (size: 410.0 B, free: 434.3 MiB)
2025-05-12 15:51:54,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:54,207 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 15:51:54,353 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:54,354 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:54,355 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:54,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Opened connection [connectionId{localValue:16, serverValue:20429}] to mongodb:27017
2025-05-12 15:51:54,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1029347}
2025-05-12 15:51:54,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Opened connection [connectionId{localValue:17, serverValue:20430}] to mongodb:27017
2025-05-12 15:51:54,362 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:54,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Closed connection [connectionId{localValue:17, serverValue:20430}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:54,364 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:54,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:54,366 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:54,367 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Opened connection [connectionId{localValue:18, serverValue:20431}] to mongodb:27017
2025-05-12 15:51:54,369 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=947859}
2025-05-12 15:51:54,371 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Opened connection [connectionId{localValue:19, serverValue:20432}] to mongodb:27017
2025-05-12 15:51:54,380 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:54,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Closed connection [connectionId{localValue:19, serverValue:20432}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:54,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:54,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:54,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:54,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 15:51:54,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:54,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:54,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:54,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:54,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:54,392 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.2 MiB)
2025-05-12 15:51:54,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 15:51:54,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on dbe8b5eed567:36039 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 15:51:54,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:54,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Removed broadcast_8_piece0 on dbe8b5eed567:36039 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 15:51:54,408 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:54,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 15:51:54,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:34943 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 15:51:54,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:54,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Removed broadcast_6_piece0 on dbe8b5eed567:36039 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 15:51:54,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:34943 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:54,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Removed broadcast_9_piece0 on dbe8b5eed567:36039 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 15:51:54,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Removed broadcast_7_piece0 on dbe8b5eed567:36039 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:54,470 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:34943 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:54,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 107 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:54,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 15:51:54,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.129 s
2025-05-12 15:51:54,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:54,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: running: Set()
2025-05-12 15:51:54,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:54,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:54,534 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:54,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO CodeGenerator: Code generated in 12.885837 ms
2025-05-12 15:51:54,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO CodeGenerator: Code generated in 8.127622 ms
2025-05-12 15:51:54,602 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:54,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO CodeGenerator: Code generated in 11.061954 ms
2025-05-12 15:51:54,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 15:51:54,631 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:54,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:54,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 15:51:54,633 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:54,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:54,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 15:51:54,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 15:51:54,646 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on dbe8b5eed567:36039 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:54,647 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:54,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:54,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 15:51:54,650 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:54,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:34943 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:54,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:59464
2025-05-12 15:51:54,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 157 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:54,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 15:51:54,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.171 s
2025-05-12 15:51:54,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:54,812 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 15:51:54,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.181170 s
2025-05-12 15:51:54,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 15:51:54,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:54,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:54,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:54,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Opened connection [connectionId{localValue:21, serverValue:20449}] to mongodb:27017
2025-05-12 15:51:54,986 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1317222}
2025-05-12 15:51:54,988 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Opened connection [connectionId{localValue:22, serverValue:20450}] to mongodb:27017
2025-05-12 15:51:54,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:54,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Closed connection [connectionId{localValue:22, serverValue:20450}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:54,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:54,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:54,993 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:54,994 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Opened connection [connectionId{localValue:23, serverValue:20451}] to mongodb:27017
2025-05-12 15:51:54,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1069617}
2025-05-12 15:51:54,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:54 INFO connection: Opened connection [connectionId{localValue:24, serverValue:20452}] to mongodb:27017
2025-05-12 15:51:55,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:55,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO connection: Closed connection [connectionId{localValue:24, serverValue:20452}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:55,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:55,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:55,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:55,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 15:51:55,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:55,018 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:55,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:55,020 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:55,021 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:55,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.2 MiB)
2025-05-12 15:51:55,026 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 15:51:55,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on dbe8b5eed567:36039 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:55,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:55,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 15:51:55,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:55,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:34943 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 90 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:55,122 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 15:51:55,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.102 s
2025-05-12 15:51:55,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:55,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: running: Set()
2025-05-12 15:51:55,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:55,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:55,133 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:55,163 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:55,194 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 15:51:55,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:55,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:55,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 15:51:55,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:55,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:55,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.1 MiB)
2025-05-12 15:51:55,207 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.1 MiB)
2025-05-12 15:51:55,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on dbe8b5eed567:36039 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:55,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:55,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 15:51:55,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:55,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:34943 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:59464
2025-05-12 15:51:55,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 102 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:55,321 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 15:51:55,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.117 s
2025-05-12 15:51:55,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:55,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 15:51:55,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.129648 s
2025-05-12 15:51:55,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 15:51:55,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:55,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:55,646 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:55,647 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:55,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO connection: Opened connection [connectionId{localValue:26, serverValue:20466}] to mongodb:27017
2025-05-12 15:51:55,649 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=862696}
2025-05-12 15:51:55,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO connection: Opened connection [connectionId{localValue:27, serverValue:20467}] to mongodb:27017
2025-05-12 15:51:55,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:55,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO connection: Closed connection [connectionId{localValue:27, serverValue:20467}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:55,654 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:55,655 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:55,655 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:55,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO connection: Opened connection [connectionId{localValue:28, serverValue:20468}] to mongodb:27017
2025-05-12 15:51:55,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=820746}
2025-05-12 15:51:55,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO connection: Opened connection [connectionId{localValue:29, serverValue:20469}] to mongodb:27017
2025-05-12 15:51:55,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:55,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO connection: Closed connection [connectionId{localValue:29, serverValue:20469}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:55,669 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:55,670 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:55,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:55,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 15:51:55,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:55,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:55,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:55,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:55,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:55,679 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.1 MiB)
2025-05-12 15:51:55,687 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.0 MiB)
2025-05-12 15:51:55,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on dbe8b5eed567:36039 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:55,693 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Removed broadcast_11_piece0 on dbe8b5eed567:36039 in memory (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:55,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 15:51:55,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:55,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:34943 in memory (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Removed broadcast_10_piece0 on dbe8b5eed567:36039 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:34943 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:55,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:34943 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 15:51:55,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Removed broadcast_13_piece0 on dbe8b5eed567:36039 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:55,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:34943 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:55,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Removed broadcast_12_piece0 on dbe8b5eed567:36039 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:55,755 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:34943 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:55,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 103 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:55,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 15:51:55,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.125 s
2025-05-12 15:51:55,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:55,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: running: Set()
2025-05-12 15:51:55,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:55,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:55,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:55,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:55,845 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO CodeGenerator: Code generated in 15.759936 ms
2025-05-12 15:51:55,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 15:51:55,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:55,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:55,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 15:51:55,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:55,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:55,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.3 MiB)
2025-05-12 15:51:55,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.2 MiB)
2025-05-12 15:51:55,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on dbe8b5eed567:36039 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 15:51:55,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:55,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:55,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 15:51:55,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:55,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:34943 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 15:51:55,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:59464
2025-05-12 15:51:55,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 85 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:55,968 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 15:51:55,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.097 s
2025-05-12 15:51:55,970 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:55,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 15:51:55,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.104068 s
2025-05-12 15:51:55,974 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 4, 'weekly': 4, 'hourly': 8}
2025-05-12 15:51:55,985 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.2 MiB)
2025-05-12 15:51:55,988 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.2 MiB)
2025-05-12 15:51:55,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on dbe8b5eed567:36039 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 15:51:55,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:55 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:56,002 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:56,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:56,005 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:56,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Opened connection [connectionId{localValue:31, serverValue:20480}] to mongodb:27017
2025-05-12 15:51:56,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1358585}
2025-05-12 15:51:56,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Opened connection [connectionId{localValue:32, serverValue:20481}] to mongodb:27017
2025-05-12 15:51:56,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:56,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Closed connection [connectionId{localValue:32, serverValue:20481}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:56,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:56,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:56,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:56,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 15:51:56,054 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 15:51:56,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 15:51:56,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:56,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:56,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 15:51:56,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.2 MiB)
2025-05-12 15:51:56,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.2 MiB)
2025-05-12 15:51:56,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on dbe8b5eed567:36039 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 15:51:56,070 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:56,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:56,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 15:51:56,074 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 15:51:56,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:34943 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 15:51:56,137 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:34943 (size: 401.0 B, free: 434.3 MiB)
2025-05-12 15:51:56,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 128 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:56,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 15:51:56,203 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.144 s
2025-05-12 15:51:56,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:56,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 15:51:56,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.156678 s
2025-05-12 15:51:56,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:56,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:56,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:56,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Opened connection [connectionId{localValue:34, serverValue:20492}] to mongodb:27017
2025-05-12 15:51:56,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1221030}
2025-05-12 15:51:56,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Opened connection [connectionId{localValue:35, serverValue:20493}] to mongodb:27017
2025-05-12 15:51:56,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:56,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Closed connection [connectionId{localValue:35, serverValue:20493}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:56,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 15:51:56,457 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO CodeGenerator: Code generated in 38.785671 ms
2025-05-12 15:51:56,461 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:56,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:56,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:56,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Opened connection [connectionId{localValue:36, serverValue:20494}] to mongodb:27017
2025-05-12 15:51:56,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=980274}
2025-05-12 15:51:56,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Opened connection [connectionId{localValue:37, serverValue:20495}] to mongodb:27017
2025-05-12 15:51:56,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:56,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Closed connection [connectionId{localValue:37, serverValue:20495}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:56,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:56,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:56,473 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:56,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Opened connection [connectionId{localValue:38, serverValue:20497}] to mongodb:27017
2025-05-12 15:51:56,477 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=888448}
2025-05-12 15:51:56,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Opened connection [connectionId{localValue:39, serverValue:20500}] to mongodb:27017
2025-05-12 15:51:56,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:56,488 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO connection: Closed connection [connectionId{localValue:39, serverValue:20500}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:56,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:56,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:56,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:56,495 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 15:51:56,495 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 15:51:56,496 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 15:51:56,496 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:56,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:56,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:56,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.5 KiB, free 434.2 MiB)
2025-05-12 15:51:56,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.2 MiB)
2025-05-12 15:51:56,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on dbe8b5eed567:36039 (size: 20.0 KiB, free: 434.3 MiB)
2025-05-12 15:51:56,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:56,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:56,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 15:51:56,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:56,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Removed broadcast_14_piece0 on dbe8b5eed567:36039 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:34943 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Removed broadcast_17_piece0 on dbe8b5eed567:36039 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:34943 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:34943 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Removed broadcast_15_piece0 on dbe8b5eed567:36039 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:34943 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 285 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:56,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 15:51:56,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.306 s
2025-05-12 15:51:56,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:56,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: running: Set()
2025-05-12 15:51:56,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:56,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:56,815 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:56,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:56,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO CodeGenerator: Code generated in 24.786572 ms
2025-05-12 15:51:56,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 15:51:56,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Got job 14 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 15:51:56,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Final stage: ResultStage 21 (foreachPartition at MongoSpark.scala:120)
2025-05-12 15:51:56,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 15:51:56,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:56,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:56,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 52.5 KiB, free 434.3 MiB)
2025-05-12 15:51:56,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 434.3 MiB)
2025-05-12 15:51:56,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on dbe8b5eed567:36039 (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:56,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:56,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 15:51:56,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:56,926 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:34943 (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:56,947 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:59464
2025-05-12 15:51:57,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 146 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:57,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 15:51:57,049 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: ResultStage 21 (foreachPartition at MongoSpark.scala:120) finished in 0.160 s
2025-05-12 15:51:57,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:57,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 15:51:57,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Job 14 finished: foreachPartition at MongoSpark.scala:120, took 0.170144 s
2025-05-12 15:51:57,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 15:51:57,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 15:51:57,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on dbe8b5eed567:36039 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 15:51:57,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO SparkContext: Created broadcast 20 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:57,104 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:57,106 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:57,108 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:57,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:41, serverValue:20514}] to mongodb:27017
2025-05-12 15:51:57,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1209660}
2025-05-12 15:51:57,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:42, serverValue:20515}] to mongodb:27017
2025-05-12 15:51:57,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:57,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Closed connection [connectionId{localValue:42, serverValue:20515}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:57,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 15:51:57,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO CodeGenerator: Code generated in 51.716188 ms
2025-05-12 15:51:57,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:57,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:57,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:57,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:43, serverValue:20517}] to mongodb:27017
2025-05-12 15:51:57,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2415902}
2025-05-12 15:51:57,259 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:44, serverValue:20520}] to mongodb:27017
2025-05-12 15:51:57,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:57,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Closed connection [connectionId{localValue:44, serverValue:20520}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:57,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:57,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:57,274 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:57,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:45, serverValue:20521}] to mongodb:27017
2025-05-12 15:51:57,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1647951}
2025-05-12 15:51:57,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:46, serverValue:20522}] to mongodb:27017
2025-05-12 15:51:57,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:57,297 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Closed connection [connectionId{localValue:46, serverValue:20522}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:57,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:57,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:57,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:47, serverValue:20523}] to mongodb:27017
2025-05-12 15:51:57,309 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:57,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Registering RDD 91 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 15:51:57,317 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 15:51:57,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (rdd at MongoSpark.scala:169)
2025-05-12 15:51:57,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:57,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:57,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:57,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 39.6 KiB, free 434.2 MiB)
2025-05-12 15:51:57,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.2 MiB)
2025-05-12 15:51:57,330 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on dbe8b5eed567:36039 (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 15:51:57,332 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:57,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:57,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
2025-05-12 15:51:57,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 15) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:57,361 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:34943 (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 15:51:57,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 15) in 185 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:57,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
2025-05-12 15:51:57,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: ShuffleMapStage 22 (rdd at MongoSpark.scala:169) finished in 0.201 s
2025-05-12 15:51:57,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:57,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: running: Set()
2025-05-12 15:51:57,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:57,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:57,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:57,537 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:57,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 15:51:57,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 15:51:57,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Final stage: ResultStage 24 (foreachPartition at MongoSpark.scala:120)
2025-05-12 15:51:57,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
2025-05-12 15:51:57,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:57,569 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 15:51:57,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 48.4 KiB, free 434.2 MiB)
2025-05-12 15:51:57,593 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.1 MiB)
2025-05-12 15:51:57,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on dbe8b5eed567:36039 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 15:51:57,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:57,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:57,600 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
2025-05-12 15:51:57,602 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Removed broadcast_20_piece0 on dbe8b5eed567:36039 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 15:51:57,602 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:57,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:34943 in memory (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:57,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Removed broadcast_19_piece0 on dbe8b5eed567:36039 in memory (size: 23.0 KiB, free: 434.3 MiB)
2025-05-12 15:51:57,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.23.0.8:34943 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 15:51:57,627 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Removed broadcast_18_piece0 on dbe8b5eed567:36039 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:57,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:34943 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:57,644 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:59464
2025-05-12 15:51:57,669 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Removed broadcast_21_piece0 on dbe8b5eed567:36039 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:57,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:34943 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:57,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 16) in 96 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:57,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
2025-05-12 15:51:57,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: ResultStage 24 (foreachPartition at MongoSpark.scala:120) finished in 0.127 s
2025-05-12 15:51:57,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:57,699 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
2025-05-12 15:51:57,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.134226 s
2025-05-12 15:51:57,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 15:51:57,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 15:51:57,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on dbe8b5eed567:36039 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 15:51:57,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO SparkContext: Created broadcast 23 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:57,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 15:51:57,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:57,787 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO CodeGenerator: Code generated in 17.616617 ms
2025-05-12 15:51:57,790 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:57,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:57,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:57,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:48, serverValue:20533}] to mongodb:27017
2025-05-12 15:51:57,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1238419}
2025-05-12 15:51:57,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:49, serverValue:20534}] to mongodb:27017
2025-05-12 15:51:57,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:57,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Closed connection [connectionId{localValue:49, serverValue:20534}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:57,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:57,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:57,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:57,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:50, serverValue:20535}] to mongodb:27017
2025-05-12 15:51:57,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1026200}
2025-05-12 15:51:57,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Opened connection [connectionId{localValue:51, serverValue:20536}] to mongodb:27017
2025-05-12 15:51:57,817 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:57,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO connection: Closed connection [connectionId{localValue:51, serverValue:20536}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:57,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:57,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:57,820 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:57,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Registering RDD 103 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 8
2025-05-12 15:51:57,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Got map stage job 17 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:57,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:57,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:57,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:57,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:57,831 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 36.4 KiB, free 434.3 MiB)
2025-05-12 15:51:57,832 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 434.3 MiB)
2025-05-12 15:51:57,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on dbe8b5eed567:36039 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 15:51:57,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:57,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:57,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
2025-05-12 15:51:57,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:57,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:34943 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 15:51:57,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 80 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:57,919 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
2025-05-12 15:51:57,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.092 s
2025-05-12 15:51:57,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:57,924 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: running: Set()
2025-05-12 15:51:57,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:57,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:57,944 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:57,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:57 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:58,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO CodeGenerator: Code generated in 27.872348 ms
2025-05-12 15:51:58,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Registering RDD 106 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 15:51:58,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Got map stage job 18 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:58,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:58,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
2025-05-12 15:51:58,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:58,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:58,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 45.0 KiB, free 434.2 MiB)
2025-05-12 15:51:58,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.2 MiB)
2025-05-12 15:51:58,065 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on dbe8b5eed567:36039 (size: 20.4 KiB, free: 434.3 MiB)
2025-05-12 15:51:58,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:58,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:58,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 15:51:58,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 15:51:58,105 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.23.0.8:34943 (size: 20.4 KiB, free: 434.3 MiB)
2025-05-12 15:51:58,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:59464
2025-05-12 15:51:58,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 18) in 115 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:58,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 15:51:58,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0) finished in 0.141 s
2025-05-12 15:51:58,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:58,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: running: Set()
2025-05-12 15:51:58,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:58,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:58,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO CodeGenerator: Code generated in 12.502003 ms
2025-05-12 15:51:58,254 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 15:51:58,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Got job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:58,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Final stage: ResultStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:58,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
2025-05-12 15:51:58,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:58,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:58,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.2 MiB)
2025-05-12 15:51:58,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.2 MiB)
2025-05-12 15:51:58,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on dbe8b5eed567:36039 (size: 5.5 KiB, free: 434.3 MiB)
2025-05-12 15:51:58,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_23_piece0 on dbe8b5eed567:36039 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 15:51:58,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:58,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:58,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 15:51:58,312 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_25_piece0 on dbe8b5eed567:36039 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:58,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.23.0.8:34943 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_24_piece0 on dbe8b5eed567:36039 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,361 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:34943 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,364 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:34943 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,379 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_22_piece0 on dbe8b5eed567:36039 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:59464
2025-05-12 15:51:58,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.23.0.8:34943 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 95 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:58,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 15:51:58,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: ResultStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.160 s
2025-05-12 15:51:58,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:58,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
2025-05-12 15:51:58,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Job 19 finished: count at NativeMethodAccessorImpl.java:0, took 0.166928 s
2025-05-12 15:51:58,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 15:51:58,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:58,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:58,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:58,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Opened connection [connectionId{localValue:53, serverValue:20550}] to mongodb:27017
2025-05-12 15:51:58,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1022322}
2025-05-12 15:51:58,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Opened connection [connectionId{localValue:54, serverValue:20551}] to mongodb:27017
2025-05-12 15:51:58,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:58,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Closed connection [connectionId{localValue:54, serverValue:20551}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:58,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:58,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:58,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:58,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Opened connection [connectionId{localValue:55, serverValue:20552}] to mongodb:27017
2025-05-12 15:51:58,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1016742}
2025-05-12 15:51:58,535 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Opened connection [connectionId{localValue:56, serverValue:20554}] to mongodb:27017
2025-05-12 15:51:58,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:58,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Closed connection [connectionId{localValue:56, serverValue:20554}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:58,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:58,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:58,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:58,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Registering RDD 114 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 15:51:58,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:58,556 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:58,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:58,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:58,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:58,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 39.7 KiB, free 434.3 MiB)
2025-05-12 15:51:58,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 15:51:58,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on dbe8b5eed567:36039 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:58,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:58,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
2025-05-12 15:51:58,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 20) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 15:51:58,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:34943 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,688 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 20) in 119 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:58,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
2025-05-12 15:51:58,690 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0) finished in 0.130 s
2025-05-12 15:51:58,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 15:51:58,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: running: Set()
2025-05-12 15:51:58,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: waiting: Set()
2025-05-12 15:51:58,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: failed: Set()
2025-05-12 15:51:58,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 15:51:58,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 15:51:58,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 15:51:58,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 15:51:58,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Final stage: ResultStage 33 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 15:51:58,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
2025-05-12 15:51:58,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:58,775 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 15:51:58,780 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 46.3 KiB, free 434.3 MiB)
2025-05-12 15:51:58,784 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.3 MiB)
2025-05-12 15:51:58,785 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on dbe8b5eed567:36039 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:58,787 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:58,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
2025-05-12 15:51:58,790 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 15:51:58,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:34943 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:59464
2025-05-12 15:51:58,845 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 21) in 55 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:58,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
2025-05-12 15:51:58,850 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: ResultStage 33 (count at NativeMethodAccessorImpl.java:0) finished in 0.072 s
2025-05-12 15:51:58,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:58,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
2025-05-12 15:51:58,855 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.080709 s
2025-05-12 15:51:58,856 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 1, 'active_users': 1}
2025-05-12 15:51:58,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 15:51:58,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 15:51:58,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on dbe8b5eed567:36039 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 15:51:58,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Created broadcast 29 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:58,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:58,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:58,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:58,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Opened connection [connectionId{localValue:58, serverValue:20564}] to mongodb:27017
2025-05-12 15:51:58,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1053734}
2025-05-12 15:51:58,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Opened connection [connectionId{localValue:59, serverValue:20565}] to mongodb:27017
2025-05-12 15:51:58,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:58,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO connection: Closed connection [connectionId{localValue:59, serverValue:20565}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:58,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:58,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:58,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:58,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 15:51:58,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Got job 22 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 15:51:58,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Final stage: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 15:51:58,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:58,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:58,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 15:51:58,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 15:51:58,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.2 MiB)
2025-05-12 15:51:58,931 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on dbe8b5eed567:36039 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,933 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:58,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:58,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
2025-05-12 15:51:58,936 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 15:51:58,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_28_piece0 on dbe8b5eed567:36039 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,938 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.23.0.8:34943 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,953 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_26_piece0 on dbe8b5eed567:36039 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,958 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.23.0.8:34943 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,960 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:34943 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_27_piece0 on dbe8b5eed567:36039 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.23.0.8:34943 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 15:51:58,987 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:58 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:34943 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 15:51:59,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 22) in 87 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:59,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
2025-05-12 15:51:59,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88) finished in 0.107 s
2025-05-12 15:51:59,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:59,014 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
2025-05-12 15:51:59,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Job 22 finished: treeAggregate at MongoInferSchema.scala:88, took 0.113392 s
2025-05-12 15:51:59,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 15:51:59,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 15:51:59,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on dbe8b5eed567:36039 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 15:51:59,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 15:51:59,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:59,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:59,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 15:51:59,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO connection: Opened connection [connectionId{localValue:60, serverValue:20569}] to mongodb:27017
2025-05-12 15:51:59,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=905318}
2025-05-12 15:51:59,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO connection: Opened connection [connectionId{localValue:61, serverValue:20570}] to mongodb:27017
2025-05-12 15:51:59,065 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:59,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO connection: Closed connection [connectionId{localValue:61, serverValue:20570}] to mongodb:27017 because the pool has been closed.
2025-05-12 15:51:59,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 15:51:59,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 15:51:59,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 15:51:59,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 15:51:59,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Got job 23 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 15:51:59,088 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Final stage: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 15:51:59,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 15:51:59,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Missing parents: List()
2025-05-12 15:51:59,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 15:51:59,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 15:51:59,096 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 15:51:59,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on dbe8b5eed567:36039 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:59,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 15:51:59,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 15:51:59,101 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 15:51:59,103 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 23) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 15:51:59,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:34943 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 15:51:59,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:34943 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 15:51:59,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 23) in 46 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 15:51:59,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 15:51:59,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88) finished in 0.060 s
2025-05-12 15:51:59,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 15:51:59,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 15:51:59,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO DAGScheduler: Job 23 finished: treeAggregate at MongoInferSchema.scala:88, took 0.064660 s
2025-05-12 15:51:59,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO SparkUI: Stopped Spark web UI at http://dbe8b5eed567:4040
2025-05-12 15:51:59,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 15:51:59,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 15:51:59,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 15:51:59,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO MemoryStore: MemoryStore cleared
2025-05-12 15:51:59,290 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO BlockManager: BlockManager stopped
2025-05-12 15:51:59,297 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 15:51:59,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 15:51:59,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 15:51:59,412 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 15:51:59,413 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 15:51:59,416 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 15:51:59,417 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 15:51:59,420 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 15:51:59,421 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 15:51:59,424 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 15:51:59,425 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 15:51:59,425 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 15:51:59,426 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 15:51:59,435 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 15:51:59,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 15:51:59,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-c5b602a1-8ac6-4990-a914-89a97c9b392d
2025-05-12 15:51:59,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe956c86-af34-44bd-8ee2-900e6117805d
2025-05-12 15:51:59,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 15:51:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe956c86-af34-44bd-8ee2-900e6117805d/pyspark-c03a4d9a-ae88-4603-9ae0-dac3822bf2c9
2025-05-12 15:51:59,599 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 15:51:59,600 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 15:51:59,600 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 15:51:59,601 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-e6adb08f-03df-4aa3-928f-8d8aa85f2173;1.0
2025-05-12 15:51:59,601 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 15:51:59,601 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 15:51:59,602 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 15:51:59,602 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 15:51:59,602 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 15:51:59,603 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 479ms :: artifacts dl 26ms
2025-05-12 15:51:59,603 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 15:51:59,603 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 15:51:59,604 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 15:51:59,604 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 15:51:59,604 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 15:51:59,604 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 15:51:59,605 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 15:51:59,605 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 15:51:59,605 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 15:51:59,606 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 15:51:59,606 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 15:51:59,606 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-e6adb08f-03df-4aa3-928f-8d8aa85f2173
2025-05-12 15:51:59,606 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 15:51:59,607 - SparkScheduler - ERROR - [trend_analysis] 0 artifacts copied, 4 already retrieved (0kB/15ms)
2025-05-12 15:51:59,609 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 15:51:59,610 - SparkScheduler - INFO - Job trend_analysis duration: 33.30 seconds
2025-05-12 16:04:07,389 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 16:04:07,390 - SparkScheduler - INFO - Waiting for services to be ready...
2025-05-12 16:04:40,746 - SparkScheduler - INFO - Setting up job schedules
2025-05-12 16:04:40,747 - SparkScheduler - INFO - Scheduled trend_analysis to run every 30 minutes
2025-05-12 16:04:40,748 - SparkScheduler - INFO - Scheduled user_recommender to run every 3 hours
2025-05-12 16:04:40,749 - SparkScheduler - INFO - Scheduled content_analyzer to run daily at 02:00
2025-05-12 16:04:40,749 - SparkScheduler - INFO - All jobs scheduled
2025-05-12 16:04:40,750 - SparkScheduler - INFO - Running initial job executions...
2025-05-12 16:04:40,750 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 16:04:40,751 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 16:04:46,429 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 16:06:19,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 16:06:21,456 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 16:06:21,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO ResourceUtils: ==============================================================
2025-05-12 16:06:21,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 16:06:21,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO ResourceUtils: ==============================================================
2025-05-12 16:06:21,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 16:06:21,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 16:06:21,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 16:06:21,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 16:06:21,656 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO SecurityManager: Changing view acls to: spark
2025-05-12 16:06:21,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 16:06:21,659 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO SecurityManager: Changing view acls groups to:
2025-05-12 16:06:21,669 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 16:06:21,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 16:06:22,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO Utils: Successfully started service 'sparkDriver' on port 35707.
2025-05-12 16:06:22,107 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 16:06:22,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 16:06:22,170 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 16:06:22,171 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 16:06:22,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 16:06:22,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a2b7d3ac-9204-4c6a-8137-84ae0024f732
2025-05-12 16:06:22,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 16:06:22,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 16:06:22,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 16:06:22,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://fadb49c66c7d:35707/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747065981443
2025-05-12 16:06:22,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://fadb49c66c7d:35707/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747065981443
2025-05-12 16:06:22,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://fadb49c66c7d:35707/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747065981443
2025-05-12 16:06:22,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://fadb49c66c7d:35707/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747065981443
2025-05-12 16:06:22,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://fadb49c66c7d:35707/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747065981443
2025-05-12 16:06:22,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-7048b9e5-e4aa-404f-8f1f-2241a41b0880/userFiles-b4f11fe8-e2b5-41b5-8f02-9c6ac545e9fe/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 16:06:22,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://fadb49c66c7d:35707/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747065981443
2025-05-12 16:06:22,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-7048b9e5-e4aa-404f-8f1f-2241a41b0880/userFiles-b4f11fe8-e2b5-41b5-8f02-9c6ac545e9fe/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 16:06:22,550 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://fadb49c66c7d:35707/files/org.mongodb_bson-4.0.5.jar with timestamp 1747065981443
2025-05-12 16:06:22,550 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-7048b9e5-e4aa-404f-8f1f-2241a41b0880/userFiles-b4f11fe8-e2b5-41b5-8f02-9c6ac545e9fe/org.mongodb_bson-4.0.5.jar
2025-05-12 16:06:22,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://fadb49c66c7d:35707/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747065981443
2025-05-12 16:06:22,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-7048b9e5-e4aa-404f-8f1f-2241a41b0880/userFiles-b4f11fe8-e2b5-41b5-8f02-9c6ac545e9fe/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 16:06:22,667 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 16:06:22,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 26 ms (0 ms spent in bootstraps)
2025-05-12 16:06:22,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512160622-0000
2025-05-12 16:06:22,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37545.
2025-05-12 16:06:22,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO NettyBlockTransferService: Server created on fadb49c66c7d:37545
2025-05-12 16:06:22,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 16:06:22,874 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fadb49c66c7d, 37545, None)
2025-05-12 16:06:22,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO BlockManagerMasterEndpoint: Registering block manager fadb49c66c7d:37545 with 434.4 MiB RAM, BlockManagerId(driver, fadb49c66c7d, 37545, None)
2025-05-12 16:06:22,883 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fadb49c66c7d, 37545, None)
2025-05-12 16:06:22,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fadb49c66c7d, 37545, None)
2025-05-12 16:06:22,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512160622-0000/0 on worker-20250512160413-172.23.0.8-36379 (172.23.0.8:36379) with 2 core(s)
2025-05-12 16:06:22,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512160622-0000/0 on hostPort 172.23.0.8:36379 with 2 core(s), 1024.0 MiB RAM
2025-05-12 16:06:23,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512160622-0000/0 is now RUNNING
2025-05-12 16:06:23,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:23 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 16:06:23,628 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 16:06:23,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 16:06:23,655 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:23 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 16:06:25,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 16:06:25,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 16:06:25,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fadb49c66c7d:37545 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:06:25,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:25 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:27,656 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:59378) with ID 0,  ResourceProfileId 0
2025-05-12 16:06:27,816 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:44011 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 44011, None)
2025-05-12 16:06:29,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:29 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:29,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:29 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:29,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:29 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:29,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:29 INFO connection: Opened connection [connectionId{localValue:1, serverValue:11}] to mongodb:27017
2025-05-12 16:06:29,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:29 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4010566}
2025-05-12 16:06:29,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:29 INFO connection: Opened connection [connectionId{localValue:2, serverValue:12}] to mongodb:27017
2025-05-12 16:06:29,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:29 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:06:30,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO CodeGenerator: Code generated in 162.973162 ms
2025-05-12 16:06:30,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:30,274 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:30,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO connection: Opened connection [connectionId{localValue:3, serverValue:13}] to mongodb:27017
2025-05-12 16:06:30,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1191463}
2025-05-12 16:06:30,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO connection: Opened connection [connectionId{localValue:4, serverValue:14}] to mongodb:27017
2025-05-12 16:06:30,370 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 16:06:30,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:06:30,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 16:06:30,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:30,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:30,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:30,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 16:06:30,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:06:30,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fadb49c66c7d:37545 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:30,452 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:30,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:30,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 16:06:30,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:31,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:44011 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:31,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:44011 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:06:33,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3444 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:33,986 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 16:06:33,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:33 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 3.599 s
2025-05-12 16:06:33,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:33 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:34,000 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: running: Set()
2025-05-12 16:06:34,001 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:34,002 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:34,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:34,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO CodeGenerator: Code generated in 17.767037 ms
2025-05-12 16:06:34,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:34,122 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO CodeGenerator: Code generated in 17.984158 ms
2025-05-12 16:06:34,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:06:34,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:06:34,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:06:34,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 16:06:34,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:34,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:34,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:06:34,245 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 16:06:34,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fadb49c66c7d:37545 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:34,249 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:34,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:34,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 16:06:34,259 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:34,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fadb49c66c7d:37545 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:34,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:44011 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:34,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:44011 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:34,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:34,454 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO connection: Closed connection [connectionId{localValue:2, serverValue:12}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:34,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:59378
2025-05-12 16:06:35,143 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 886 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:35,144 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 16:06:35,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 0.918 s
2025-05-12 16:06:35,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:35,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 16:06:35,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 0.945761 s
2025-05-12 16:06:35,163 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:06:35,172 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:06:35,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fadb49c66c7d:37545 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:35,195 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:44011 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:35,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fadb49c66c7d:37545 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:06:35,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:35,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:35,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:35,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:35,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO connection: Opened connection [connectionId{localValue:5, serverValue:20}] to mongodb:27017
2025-05-12 16:06:35,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1251148}
2025-05-12 16:06:35,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO connection: Opened connection [connectionId{localValue:6, serverValue:21}] to mongodb:27017
2025-05-12 16:06:35,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:35,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO connection: Closed connection [connectionId{localValue:4, serverValue:14}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:35,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:06:35,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO CodeGenerator: Code generated in 53.378039 ms
2025-05-12 16:06:35,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:35,496 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:35,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:35,499 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO connection: Opened connection [connectionId{localValue:7, serverValue:22}] to mongodb:27017
2025-05-12 16:06:35,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1004088}
2025-05-12 16:06:35,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO connection: Opened connection [connectionId{localValue:8, serverValue:23}] to mongodb:27017
2025-05-12 16:06:35,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 16:06:35,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:06:35,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 16:06:35,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:35,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:35,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:35,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 16:06:35,534 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:06:35,537 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fadb49c66c7d:37545 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:35,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:35,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:35,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 16:06:35,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:35,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO connection: Closed connection [connectionId{localValue:6, serverValue:21}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:35,544 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:35,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO connection: Closed connection [connectionId{localValue:8, serverValue:23}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:35,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:35,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fadb49c66c7d:37545 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:06:35,595 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:44011 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:35,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 226 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:35,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 16:06:35,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.249 s
2025-05-12 16:06:35,775 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:35,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: running: Set()
2025-05-12 16:06:35,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:35,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:35,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:35,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:35,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:06:35,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:06:35,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:06:35,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 16:06:35,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:35,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:35,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:06:35,936 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Removed broadcast_4_piece0 on fadb49c66c7d:37545 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:35,944 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:44011 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:35,957 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 16:06:35,963 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fadb49c66c7d:37545 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:35,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:35,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:35,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 16:06:35,976 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:35 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:36,030 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:44011 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:36,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:59378
2025-05-12 16:06:36,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 142 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:36,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 16:06:36,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.221 s
2025-05-12 16:06:36,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:36,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 16:06:36,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.243791 s
2025-05-12 16:06:36,143 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:06:36,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:06:36,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fadb49c66c7d:37545 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:06:36,166 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:36,166 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on fadb49c66c7d:37545 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:36,181 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:44011 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:36,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:36,291 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:36,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:36,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Opened connection [connectionId{localValue:9, serverValue:24}] to mongodb:27017
2025-05-12 16:06:36,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1501011}
2025-05-12 16:06:36,299 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Opened connection [connectionId{localValue:10, serverValue:25}] to mongodb:27017
2025-05-12 16:06:36,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:36,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Closed connection [connectionId{localValue:10, serverValue:25}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:36,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:06:36,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:36,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO CodeGenerator: Code generated in 100.294835 ms
2025-05-12 16:06:36,845 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:36,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:36,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:36,862 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Opened connection [connectionId{localValue:11, serverValue:26}] to mongodb:27017
2025-05-12 16:06:36,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1824857}
2025-05-12 16:06:36,865 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Opened connection [connectionId{localValue:12, serverValue:27}] to mongodb:27017
2025-05-12 16:06:36,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:36,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Closed connection [connectionId{localValue:12, serverValue:27}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:36,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:36,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:36,914 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:36,931 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Opened connection [connectionId{localValue:13, serverValue:28}] to mongodb:27017
2025-05-12 16:06:36,936 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1760341}
2025-05-12 16:06:36,940 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Opened connection [connectionId{localValue:14, serverValue:29}] to mongodb:27017
2025-05-12 16:06:36,960 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:36,961 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO connection: Closed connection [connectionId{localValue:14, serverValue:29}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:36,963 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:36,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:36,968 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:36,972 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 16:06:36,973 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:06:36,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 16:06:36,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:36,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:36,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:36,986 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:36 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:06:37,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 16:06:37,045 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO BlockManagerInfo: Removed broadcast_6_piece0 on fadb49c66c7d:37545 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:06:37,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fadb49c66c7d:37545 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:37,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:37,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:37,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 16:06:37,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:37,164 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:44011 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:37,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 384 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:37,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 16:06:37,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.502 s
2025-05-12 16:06:37,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:37,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: running: Set()
2025-05-12 16:06:37,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:37,486 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:37,501 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:37,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO CodeGenerator: Code generated in 14.741294 ms
2025-05-12 16:06:37,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:37,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO CodeGenerator: Code generated in 46.70024 ms
2025-05-12 16:06:37,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:06:37,637 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:06:37,639 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:06:37,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 16:06:37,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:37,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:37,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.3 MiB)
2025-05-12 16:06:37,693 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)
2025-05-12 16:06:37,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fadb49c66c7d:37545 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:06:37,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:37,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:37,699 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 16:06:37,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:37,738 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:44011 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:06:37,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:59378
2025-05-12 16:06:37,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 249 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:37,965 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 16:06:37,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.283 s
2025-05-12 16:06:37,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:37,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 16:06:37,983 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.321846 s
2025-05-12 16:06:37,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.2 MiB)
2025-05-12 16:06:37,988 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.2 MiB)
2025-05-12 16:06:37,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on fadb49c66c7d:37545 (size: 410.0 B, free: 434.3 MiB)
2025-05-12 16:06:37,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:37 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:38,194 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:06:38,372 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:38,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:38,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:38,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO connection: Opened connection [connectionId{localValue:16, serverValue:31}] to mongodb:27017
2025-05-12 16:06:38,379 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1320694}
2025-05-12 16:06:38,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO connection: Opened connection [connectionId{localValue:17, serverValue:32}] to mongodb:27017
2025-05-12 16:06:38,386 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:38,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO connection: Closed connection [connectionId{localValue:17, serverValue:32}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:38,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:38,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:38,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:38,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO connection: Opened connection [connectionId{localValue:18, serverValue:33}] to mongodb:27017
2025-05-12 16:06:38,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1557278}
2025-05-12 16:06:38,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO connection: Opened connection [connectionId{localValue:19, serverValue:34}] to mongodb:27017
2025-05-12 16:06:38,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:38,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO connection: Closed connection [connectionId{localValue:19, serverValue:34}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:38,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:38,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:38,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:38,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 16:06:38,431 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:38,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:38,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:38,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:38,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:38,440 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.2 MiB)
2025-05-12 16:06:38,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 16:06:38,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on fadb49c66c7d:37545 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:38,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:38,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:38,451 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 16:06:38,455 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:38,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:44011 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:38,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 122 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:38,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 16:06:38,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.142 s
2025-05-12 16:06:38,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:38,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: running: Set()
2025-05-12 16:06:38,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:38,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:38,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:38,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO CodeGenerator: Code generated in 15.500223 ms
2025-05-12 16:06:38,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO CodeGenerator: Code generated in 10.137373 ms
2025-05-12 16:06:38,691 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:38,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO CodeGenerator: Code generated in 13.839772 ms
2025-05-12 16:06:38,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:06:38,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:38,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:38,735 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 16:06:38,735 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:38,736 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:38,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.1 MiB)
2025-05-12 16:06:38,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.1 MiB)
2025-05-12 16:06:38,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on fadb49c66c7d:37545 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 16:06:38,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:38,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:38,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 16:06:38,750 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:38,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:44011 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 16:06:38,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:59378
2025-05-12 16:06:38,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 110 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:38,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 16:06:38,862 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.123 s
2025-05-12 16:06:38,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:38,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 16:06:38,865 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.131688 s
2025-05-12 16:06:38,959 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:06:38,962 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on fadb49c66c7d:37545 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:38,970 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:44011 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:38,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:38 INFO BlockManagerInfo: Removed broadcast_7_piece0 on fadb49c66c7d:37545 in memory (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 16:06:39,015 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:44011 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,040 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on fadb49c66c7d:37545 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:44011 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,065 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Removed broadcast_9_piece0 on fadb49c66c7d:37545 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:06:39,081 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Removed broadcast_8_piece0 on fadb49c66c7d:37545 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:44011 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:39,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:39,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:39,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:21, serverValue:36}] to mongodb:27017
2025-05-12 16:06:39,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1582341}
2025-05-12 16:06:39,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:22, serverValue:37}] to mongodb:27017
2025-05-12 16:06:39,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:39,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Closed connection [connectionId{localValue:22, serverValue:37}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:39,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:39,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:39,127 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:39,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:23, serverValue:38}] to mongodb:27017
2025-05-12 16:06:39,130 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1083420}
2025-05-12 16:06:39,133 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:24, serverValue:39}] to mongodb:27017
2025-05-12 16:06:39,143 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:39,144 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Closed connection [connectionId{localValue:24, serverValue:39}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:39,145 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:39,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:39,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:39,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:25, serverValue:40}] to mongodb:27017
2025-05-12 16:06:39,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 16:06:39,153 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:39,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:39,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:39,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:39,157 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:39,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.4 MiB)
2025-05-12 16:06:39,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:06:39,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on fadb49c66c7d:37545 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,171 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:39,173 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:39,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 16:06:39,176 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:39,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:44011 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 107 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:39,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 16:06:39,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.128 s
2025-05-12 16:06:39,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:39,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: running: Set()
2025-05-12 16:06:39,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:39,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:39,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:39,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:39,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:06:39,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:39,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:39,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 16:06:39,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:39,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:39,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 16:06:39,362 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 16:06:39,364 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on fadb49c66c7d:37545 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,367 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:39,367 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:39,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 16:06:39,369 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:39,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:44011 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:39,440 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:59378
2025-05-12 16:06:39,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 104 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:39,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 16:06:39,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.119 s
2025-05-12 16:06:39,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:39,476 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 16:06:39,477 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.128443 s
2025-05-12 16:06:39,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:06:39,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:39,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:39,662 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:39,663 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:39,665 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:26, serverValue:41}] to mongodb:27017
2025-05-12 16:06:39,666 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1257750}
2025-05-12 16:06:39,669 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:27, serverValue:42}] to mongodb:27017
2025-05-12 16:06:39,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:39,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Closed connection [connectionId{localValue:27, serverValue:42}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:39,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:39,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:39,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:39,678 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:28, serverValue:43}] to mongodb:27017
2025-05-12 16:06:39,680 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1176343}
2025-05-12 16:06:39,682 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Opened connection [connectionId{localValue:29, serverValue:44}] to mongodb:27017
2025-05-12 16:06:39,691 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:39,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO connection: Closed connection [connectionId{localValue:29, serverValue:44}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:39,693 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:39,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:39,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:39,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 16:06:39,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:39,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:39,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:39,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:39,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:39,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.2 MiB)
2025-05-12 16:06:39,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 16:06:39,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on fadb49c66c7d:37545 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 16:06:39,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:39,715 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:39,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 16:06:39,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:39,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:44011 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 16:06:39,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 74 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:39,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 16:06:39,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.090 s
2025-05-12 16:06:39,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:39,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: running: Set()
2025-05-12 16:06:39,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:39,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:39,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:39,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:39,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO CodeGenerator: Code generated in 20.217743 ms
2025-05-12 16:06:39,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:06:39,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:39,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:39,883 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 16:06:39,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:39,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:39,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.1 MiB)
2025-05-12 16:06:39,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.1 MiB)
2025-05-12 16:06:39,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on fadb49c66c7d:37545 (size: 25.7 KiB, free: 434.3 MiB)
2025-05-12 16:06:39,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:39,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:39,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 16:06:39,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:39,945 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:44011 (size: 25.7 KiB, free: 434.3 MiB)
2025-05-12 16:06:39,970 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:59378
2025-05-12 16:06:40,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 115 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:40,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 16:06:40,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.146 s
2025-05-12 16:06:40,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:40,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 16:06:40,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.155870 s
2025-05-12 16:06:40,037 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 4, 'weekly': 4, 'hourly': 8}
2025-05-12 16:06:40,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.1 MiB)
2025-05-12 16:06:40,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.1 MiB)
2025-05-12 16:06:40,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on fadb49c66c7d:37545 (size: 401.0 B, free: 434.3 MiB)
2025-05-12 16:06:40,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:40,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,070 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:40,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:31, serverValue:46}] to mongodb:27017
2025-05-12 16:06:40,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1050399}
2025-05-12 16:06:40,075 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:32, serverValue:47}] to mongodb:27017
2025-05-12 16:06:40,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Closed connection [connectionId{localValue:32, serverValue:47}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:40,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:06:40,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:06:40,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:06:40,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:40,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:40,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:06:40,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.1 MiB)
2025-05-12 16:06:40,130 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.1 MiB)
2025-05-12 16:06:40,131 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on fadb49c66c7d:37545 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 16:06:40,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:40,133 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:40,135 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 16:06:40,138 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:06:40,166 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:44011 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 16:06:40,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:44011 (size: 401.0 B, free: 434.3 MiB)
2025-05-12 16:06:40,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 144 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:40,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on fadb49c66c7d:37545 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:40,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 16:06:40,288 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.164 s
2025-05-12 16:06:40,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:40,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 16:06:40,290 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:44011 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:40,292 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.178930 s
2025-05-12 16:06:40,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:44011 in memory (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 16:06:40,314 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on fadb49c66c7d:37545 in memory (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 16:06:40,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on fadb49c66c7d:37545 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:44011 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,341 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_13_piece0 on fadb49c66c7d:37545 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:44011 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:40,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:34, serverValue:49}] to mongodb:27017
2025-05-12 16:06:40,425 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=717640}
2025-05-12 16:06:40,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:35, serverValue:50}] to mongodb:27017
2025-05-12 16:06:40,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Closed connection [connectionId{localValue:35, serverValue:50}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:40,455 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 16:06:40,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO CodeGenerator: Code generated in 27.333574 ms
2025-05-12 16:06:40,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:40,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:36, serverValue:51}] to mongodb:27017
2025-05-12 16:06:40,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=895065}
2025-05-12 16:06:40,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:37, serverValue:52}] to mongodb:27017
2025-05-12 16:06:40,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Closed connection [connectionId{localValue:37, serverValue:52}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:40,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:40,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:38, serverValue:53}] to mongodb:27017
2025-05-12 16:06:40,528 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1382189}
2025-05-12 16:06:40,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:39, serverValue:54}] to mongodb:27017
2025-05-12 16:06:40,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Closed connection [connectionId{localValue:39, serverValue:54}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:40,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 16:06:40,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:06:40,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 16:06:40,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:40,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:40,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:40,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.5 KiB, free 434.3 MiB)
2025-05-12 16:06:40,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.3 MiB)
2025-05-12 16:06:40,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on fadb49c66c7d:37545 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:40,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:40,562 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 16:06:40,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:40,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:44011 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,610 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_17_piece0 on fadb49c66c7d:37545 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:44011 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 155 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:40,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 16:06:40,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.170 s
2025-05-12 16:06:40,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:40,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: running: Set()
2025-05-12 16:06:40,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:40,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:40,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:40,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:40,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO CodeGenerator: Code generated in 14.80338 ms
2025-05-12 16:06:40,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:06:40,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Got job 14 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:06:40,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Final stage: ResultStage 21 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:06:40,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 16:06:40,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:40,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:40,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 52.5 KiB, free 434.3 MiB)
2025-05-12 16:06:40,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 434.3 MiB)
2025-05-12 16:06:40,780 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on fadb49c66c7d:37545 (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,781 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:40,783 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:40,784 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 16:06:40,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:40,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:44011 (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:40,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:59378
2025-05-12 16:06:40,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 95 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:40,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 16:06:40,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: ResultStage 21 (foreachPartition at MongoSpark.scala:120) finished in 0.109 s
2025-05-12 16:06:40,883 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:40,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 16:06:40,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Job 14 finished: foreachPartition at MongoSpark.scala:120, took 0.118262 s
2025-05-12 16:06:40,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:06:40,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:06:40,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on fadb49c66c7d:37545 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:06:40,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO SparkContext: Created broadcast 20 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:40,909 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,911 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:40,912 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:41, serverValue:56}] to mongodb:27017
2025-05-12 16:06:40,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=850275}
2025-05-12 16:06:40,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:42, serverValue:57}] to mongodb:27017
2025-05-12 16:06:40,917 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Closed connection [connectionId{localValue:42, serverValue:57}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:40,931 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 16:06:40,966 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO CodeGenerator: Code generated in 19.836611 ms
2025-05-12 16:06:40,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:40,972 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:43, serverValue:58}] to mongodb:27017
2025-05-12 16:06:40,973 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=825528}
2025-05-12 16:06:40,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:44, serverValue:59}] to mongodb:27017
2025-05-12 16:06:40,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Closed connection [connectionId{localValue:44, serverValue:59}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:40,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,979 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,979 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:40,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:45, serverValue:60}] to mongodb:27017
2025-05-12 16:06:40,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=679017}
2025-05-12 16:06:40,983 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Opened connection [connectionId{localValue:46, serverValue:61}] to mongodb:27017
2025-05-12 16:06:40,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO connection: Closed connection [connectionId{localValue:46, serverValue:61}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:40,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:40,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:40,993 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:40,997 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Registering RDD 91 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 16:06:40,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:06:40,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (rdd at MongoSpark.scala:169)
2025-05-12 16:06:40,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:41,002 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:41,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:40 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:41,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 39.6 KiB, free 434.2 MiB)
2025-05-12 16:06:41,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.2 MiB)
2025-05-12 16:06:41,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on fadb49c66c7d:37545 (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:41,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:41,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
2025-05-12 16:06:41,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 15) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:41,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:44011 (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,145 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 15) in 134 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:41,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
2025-05-12 16:06:41,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: ShuffleMapStage 22 (rdd at MongoSpark.scala:169) finished in 0.146 s
2025-05-12 16:06:41,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:41,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: running: Set()
2025-05-12 16:06:41,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:41,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:41,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:41,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:41,187 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:06:41,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:06:41,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Final stage: ResultStage 24 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:06:41,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
2025-05-12 16:06:41,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:41,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:06:41,194 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 48.4 KiB, free 434.2 MiB)
2025-05-12 16:06:41,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.1 MiB)
2025-05-12 16:06:41,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on fadb49c66c7d:37545 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:41,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:41,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
2025-05-12 16:06:41,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:41,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.23.0.8:44011 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:59378
2025-05-12 16:06:41,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 16) in 61 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:41,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
2025-05-12 16:06:41,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: ResultStage 24 (foreachPartition at MongoSpark.scala:120) finished in 0.073 s
2025-05-12 16:06:41,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:41,265 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
2025-05-12 16:06:41,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.077331 s
2025-05-12 16:06:41,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.0 B, free 434.1 MiB)
2025-05-12 16:06:41,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.1 MiB)
2025-05-12 16:06:41,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on fadb49c66c7d:37545 (size: 410.0 B, free: 434.3 MiB)
2025-05-12 16:06:41,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Created broadcast 23 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:41,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 16:06:41,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:41,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO CodeGenerator: Code generated in 11.773237 ms
2025-05-12 16:06:41,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:41,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:41,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:41,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Opened connection [connectionId{localValue:48, serverValue:64}] to mongodb:27017
2025-05-12 16:06:41,329 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=767362}
2025-05-12 16:06:41,330 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Opened connection [connectionId{localValue:49, serverValue:65}] to mongodb:27017
2025-05-12 16:06:41,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:41,332 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Closed connection [connectionId{localValue:49, serverValue:65}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:41,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:41,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:41,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:41,335 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Opened connection [connectionId{localValue:50, serverValue:66}] to mongodb:27017
2025-05-12 16:06:41,337 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1133591}
2025-05-12 16:06:41,339 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Opened connection [connectionId{localValue:51, serverValue:67}] to mongodb:27017
2025-05-12 16:06:41,345 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:41,346 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Closed connection [connectionId{localValue:51, serverValue:67}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:41,346 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:41,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:41,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:41,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Registering RDD 103 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 8
2025-05-12 16:06:41,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Got map stage job 17 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:41,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:41,353 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:41,353 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:41,354 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:41,355 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 36.4 KiB, free 434.1 MiB)
2025-05-12 16:06:41,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 434.1 MiB)
2025-05-12 16:06:41,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on fadb49c66c7d:37545 (size: 16.9 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:41,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:41,361 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
2025-05-12 16:06:41,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:41,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:44011 (size: 16.9 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 68 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:41,431 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
2025-05-12 16:06:41,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.078 s
2025-05-12 16:06:41,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:41,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: running: Set()
2025-05-12 16:06:41,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:41,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:41,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:41,453 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_20_piece0 on fadb49c66c7d:37545 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 16:06:41,458 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:41,467 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_21_piece0 on fadb49c66c7d:37545 in memory (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:44011 in memory (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_23_piece0 on fadb49c66c7d:37545 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 16:06:41,498 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_24_piece0 on fadb49c66c7d:37545 in memory (size: 16.9 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO CodeGenerator: Code generated in 34.087192 ms
2025-05-12 16:06:41,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:44011 in memory (size: 16.9 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_22_piece0 on fadb49c66c7d:37545 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Registering RDD 106 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 16:06:41,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Got map stage job 18 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:41,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:41,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
2025-05-12 16:06:41,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:41,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:41,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.23.0.8:44011 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 45.0 KiB, free 434.2 MiB)
2025-05-12 16:06:41,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.2 MiB)
2025-05-12 16:06:41,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on fadb49c66c7d:37545 (size: 20.4 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:41,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:41,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 16:06:41,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_19_piece0 on fadb49c66c7d:37545 in memory (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 16:06:41,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:44011 in memory (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_18_piece0 on fadb49c66c7d:37545 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.23.0.8:44011 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:44011 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:59378
2025-05-12 16:06:41,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 18) in 78 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:41,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 16:06:41,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0) finished in 0.101 s
2025-05-12 16:06:41,619 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:41,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: running: Set()
2025-05-12 16:06:41,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:41,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:41,639 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO CodeGenerator: Code generated in 7.853218 ms
2025-05-12 16:06:41,649 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:06:41,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Got job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:41,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Final stage: ResultStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:41,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
2025-05-12 16:06:41,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:41,654 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:41,656 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.3 MiB)
2025-05-12 16:06:41,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)
2025-05-12 16:06:41,659 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on fadb49c66c7d:37545 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:41,661 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:41,661 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 16:06:41,663 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:41,681 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:44011 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:59378
2025-05-12 16:06:41,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 46 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:41,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 16:06:41,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: ResultStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.056 s
2025-05-12 16:06:41,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:41,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
2025-05-12 16:06:41,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Job 19 finished: count at NativeMethodAccessorImpl.java:0, took 0.062148 s
2025-05-12 16:06:41,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 16:06:41,763 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:41,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:41,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:41,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Opened connection [connectionId{localValue:53, serverValue:69}] to mongodb:27017
2025-05-12 16:06:41,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=736905}
2025-05-12 16:06:41,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Opened connection [connectionId{localValue:54, serverValue:70}] to mongodb:27017
2025-05-12 16:06:41,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:41,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Closed connection [connectionId{localValue:54, serverValue:70}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:41,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:41,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:41,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:41,775 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Opened connection [connectionId{localValue:55, serverValue:71}] to mongodb:27017
2025-05-12 16:06:41,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=873974}
2025-05-12 16:06:41,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Opened connection [connectionId{localValue:56, serverValue:72}] to mongodb:27017
2025-05-12 16:06:41,785 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:41,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO connection: Closed connection [connectionId{localValue:56, serverValue:72}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:41,787 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:41,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:41,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:41,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Registering RDD 114 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 16:06:41,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:41,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:41,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:41,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:41,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:41,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 39.7 KiB, free 434.3 MiB)
2025-05-12 16:06:41,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 16:06:41,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on fadb49c66c7d:37545 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:41,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:41,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
2025-05-12 16:06:41,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 20) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:06:41,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:44011 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:06:41,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 20) in 69 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:41,874 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
2025-05-12 16:06:41,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0) finished in 0.079 s
2025-05-12 16:06:41,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:06:41,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: running: Set()
2025-05-12 16:06:41,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: waiting: Set()
2025-05-12 16:06:41,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: failed: Set()
2025-05-12 16:06:41,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:06:41,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:06:41,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:06:41,919 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:06:41,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Final stage: ResultStage 33 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:06:41,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
2025-05-12 16:06:41,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:41,924 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:06:41,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 46.3 KiB, free 434.2 MiB)
2025-05-12 16:06:41,927 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.2 MiB)
2025-05-12 16:06:41,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on fadb49c66c7d:37545 (size: 21.1 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:41,930 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:41,930 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
2025-05-12 16:06:41,933 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:06:41,951 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:44011 (size: 21.1 KiB, free: 434.3 MiB)
2025-05-12 16:06:41,962 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:59378
2025-05-12 16:06:41,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 21) in 48 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:41,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
2025-05-12 16:06:41,983 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: ResultStage 33 (count at NativeMethodAccessorImpl.java:0) finished in 0.060 s
2025-05-12 16:06:41,983 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:41,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
2025-05-12 16:06:41,985 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.065366 s
2025-05-12 16:06:41,986 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 1, 'active_users': 1}
2025-05-12 16:06:41,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 192.0 B, free 434.2 MiB)
2025-05-12 16:06:41,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.2 MiB)
2025-05-12 16:06:41,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:41 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on fadb49c66c7d:37545 (size: 401.0 B, free: 434.3 MiB)
2025-05-12 16:06:42,001 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO SparkContext: Created broadcast 29 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:42,002 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:42,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:42,005 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:42,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO connection: Opened connection [connectionId{localValue:58, serverValue:74}] to mongodb:27017
2025-05-12 16:06:42,008 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1421875}
2025-05-12 16:06:42,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO connection: Opened connection [connectionId{localValue:59, serverValue:75}] to mongodb:27017
2025-05-12 16:06:42,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:42,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO connection: Closed connection [connectionId{localValue:59, serverValue:75}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:42,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:42,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:42,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:42,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:06:42,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Got job 22 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:06:42,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Final stage: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:06:42,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:42,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:42,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:06:42,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.1 KiB, free 434.2 MiB)
2025-05-12 16:06:42,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.2 MiB)
2025-05-12 16:06:42,040 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on fadb49c66c7d:37545 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 16:06:42,041 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:42,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:42,043 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
2025-05-12 16:06:42,045 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:06:42,062 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:44011 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 16:06:42,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:44011 (size: 401.0 B, free: 434.3 MiB)
2025-05-12 16:06:42,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 22) in 50 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:42,095 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
2025-05-12 16:06:42,095 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88) finished in 0.061 s
2025-05-12 16:06:42,096 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:42,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
2025-05-12 16:06:42,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Job 22 finished: treeAggregate at MongoInferSchema.scala:88, took 0.065283 s
2025-05-12 16:06:42,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.2 MiB)
2025-05-12 16:06:42,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.2 MiB)
2025-05-12 16:06:42,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on fadb49c66c7d:37545 (size: 401.0 B, free: 434.3 MiB)
2025-05-12 16:06:42,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:42,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:42,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:42,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:42,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO connection: Opened connection [connectionId{localValue:61, serverValue:77}] to mongodb:27017
2025-05-12 16:06:42,122 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1312733}
2025-05-12 16:06:42,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO connection: Opened connection [connectionId{localValue:62, serverValue:78}] to mongodb:27017
2025-05-12 16:06:42,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:42,127 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO connection: Closed connection [connectionId{localValue:62, serverValue:78}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:06:42,131 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:42,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:42,133 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:06:42,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:06:42,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Got job 23 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:06:42,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Final stage: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:06:42,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:42,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:42,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:06:42,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.2 MiB)
2025-05-12 16:06:42,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.2 MiB)
2025-05-12 16:06:42,156 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on fadb49c66c7d:37545 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 16:06:42,157 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:42,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:42,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 16:06:42,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 23) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:06:42,181 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:44011 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 16:06:42,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:44011 (size: 401.0 B, free: 434.3 MiB)
2025-05-12 16:06:42,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 23) in 53 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:42,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 16:06:42,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88) finished in 0.064 s
2025-05-12 16:06:42,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:42,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 16:06:42,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO DAGScheduler: Job 23 finished: treeAggregate at MongoInferSchema.scala:88, took 0.069898 s
2025-05-12 16:06:42,265 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO SparkUI: Stopped Spark web UI at http://fadb49c66c7d:4040
2025-05-12 16:06:42,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 16:06:42,269 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 16:06:42,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 16:06:42,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO MemoryStore: MemoryStore cleared
2025-05-12 16:06:42,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManager: BlockManager stopped
2025-05-12 16:06:42,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 16:06:42,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 16:06:42,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 16:06:42,845 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 16:06:42,847 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 16:06:42,853 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 16:06:42,854 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 16:06:42,858 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 16:06:42,858 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 16:06:42,861 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 16:06:42,862 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 16:06:42,863 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 16:06:42,864 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 16:06:42,872 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 16:06:42,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 16:06:42,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-a083ef89-7d67-426f-a194-835a46d55a26
2025-05-12 16:06:42,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-7048b9e5-e4aa-404f-8f1f-2241a41b0880
2025-05-12 16:06:42,963 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:06:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-7048b9e5-e4aa-404f-8f1f-2241a41b0880/pyspark-e742a35c-3f9a-4b02-834a-63c7b23aa765
2025-05-12 16:06:43,079 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 16:06:43,092 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 16:06:43,093 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 16:06:43,094 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-401711d1-8e89-4f45-9f72-2d90aa2cb03c;1.0
2025-05-12 16:06:43,095 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 16:06:43,095 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 16:06:43,096 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 16:06:43,097 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 16:06:43,097 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 16:06:43,098 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...
2025-05-12 16:06:43,098 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (13784ms)
2025-05-12 16:06:43,099 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...
2025-05-12 16:06:43,099 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (2462ms)
2025-05-12 16:06:43,100 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...
2025-05-12 16:06:43,100 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (16966ms)
2025-05-12 16:06:43,100 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...
2025-05-12 16:06:43,101 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (51587ms)
2025-05-12 16:06:43,102 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 7705ms :: artifacts dl 84815ms
2025-05-12 16:06:43,102 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 16:06:43,103 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 16:06:43,104 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 16:06:43,105 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 16:06:43,105 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 16:06:43,106 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:06:43,106 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 16:06:43,108 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 16:06:43,111 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:06:43,114 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   4   |   4   |   0   ||   4   |   4   |
2025-05-12 16:06:43,115 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:06:43,115 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-401711d1-8e89-4f45-9f72-2d90aa2cb03c
2025-05-12 16:06:43,116 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 16:06:43,116 - SparkScheduler - ERROR - [trend_analysis] 4 artifacts copied, 0 already retrieved (2728kB/69ms)
2025-05-12 16:06:43,120 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 16:06:43,121 - SparkScheduler - INFO - Job trend_analysis duration: 122.37 seconds
2025-05-12 16:06:43,122 - SparkScheduler - INFO - Starting job: user_recommender - Generate user recommendations
2025-05-12 16:06:43,122 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/user_recommender.py
2025-05-12 16:06:47,902 - SparkScheduler - INFO - [user_recommender] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 16:06:48,930 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 16:06:51,043 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 16:06:51,072 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO ResourceUtils: ==============================================================
2025-05-12 16:06:51,072 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 16:06:51,073 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO ResourceUtils: ==============================================================
2025-05-12 16:06:51,074 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SparkContext: Submitted application: MiniTwitterUserRecommender
2025-05-12 16:06:51,097 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 16:06:51,119 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 16:06:51,121 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 16:06:51,207 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SecurityManager: Changing view acls to: spark
2025-05-12 16:06:51,208 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 16:06:51,210 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SecurityManager: Changing view acls groups to:
2025-05-12 16:06:51,210 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 16:06:51,212 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 16:06:51,507 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO Utils: Successfully started service 'sparkDriver' on port 40415.
2025-05-12 16:06:51,560 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 16:06:51,617 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 16:06:51,641 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 16:06:51,642 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 16:06:51,648 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 16:06:51,671 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ce68f89a-8d5b-464e-b28d-6933eeb64de6
2025-05-12 16:06:51,691 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 16:06:51,713 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 16:06:51,961 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 16:06:52,012 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://fadb49c66c7d:40415/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747066011036
2025-05-12 16:06:52,013 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://fadb49c66c7d:40415/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747066011036
2025-05-12 16:06:52,013 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://fadb49c66c7d:40415/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747066011036
2025-05-12 16:06:52,014 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://fadb49c66c7d:40415/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747066011036
2025-05-12 16:06:52,019 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://fadb49c66c7d:40415/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747066011036
2025-05-12 16:06:52,023 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-dc44d68f-8778-436a-93e0-63698cac83c4/userFiles-c575eb95-593d-4c15-b10b-31748d433bed/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 16:06:52,038 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://fadb49c66c7d:40415/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747066011036
2025-05-12 16:06:52,038 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-dc44d68f-8778-436a-93e0-63698cac83c4/userFiles-c575eb95-593d-4c15-b10b-31748d433bed/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 16:06:52,047 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://fadb49c66c7d:40415/files/org.mongodb_bson-4.0.5.jar with timestamp 1747066011036
2025-05-12 16:06:52,048 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-dc44d68f-8778-436a-93e0-63698cac83c4/userFiles-c575eb95-593d-4c15-b10b-31748d433bed/org.mongodb_bson-4.0.5.jar
2025-05-12 16:06:52,057 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://fadb49c66c7d:40415/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747066011036
2025-05-12 16:06:52,058 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-dc44d68f-8778-436a-93e0-63698cac83c4/userFiles-c575eb95-593d-4c15-b10b-31748d433bed/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 16:06:52,139 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 16:06:52,185 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 25 ms (0 ms spent in bootstraps)
2025-05-12 16:06:52,274 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512160652-0001
2025-05-12 16:06:52,276 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512160652-0001/0 on worker-20250512160413-172.23.0.8-36379 (172.23.0.8:36379) with 2 core(s)
2025-05-12 16:06:52,280 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512160652-0001/0 on hostPort 172.23.0.8:36379 with 2 core(s), 1024.0 MiB RAM
2025-05-12 16:06:52,284 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40881.
2025-05-12 16:06:52,284 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO NettyBlockTransferService: Server created on fadb49c66c7d:40881
2025-05-12 16:06:52,287 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 16:06:52,297 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fadb49c66c7d, 40881, None)
2025-05-12 16:06:52,304 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO BlockManagerMasterEndpoint: Registering block manager fadb49c66c7d:40881 with 434.4 MiB RAM, BlockManagerId(driver, fadb49c66c7d, 40881, None)
2025-05-12 16:06:52,308 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fadb49c66c7d, 40881, None)
2025-05-12 16:06:52,310 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fadb49c66c7d, 40881, None)
2025-05-12 16:06:52,332 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512160652-0001/0 is now RUNNING
2025-05-12 16:06:52,666 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 16:06:52,939 - SparkScheduler - INFO - [user_recommender] Starting Mini Twitter User Recommender...
2025-05-12 16:06:52,951 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 16:06:52,957 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:52 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 16:06:54,483 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 16:06:54,644 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 16:06:54,681 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fadb49c66c7d:40881 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:06:54,714 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:54 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 16:06:55,094 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:06:55,222 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:06:55,268 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:55 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:06:55,338 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:55 INFO connection: Opened connection [connectionId{localValue:1, serverValue:81}] to mongodb:27017
2025-05-12 16:06:55,362 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:55 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=18568688}
2025-05-12 16:06:55,415 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:55 INFO connection: Opened connection [connectionId{localValue:2, serverValue:82}] to mongodb:27017
2025-05-12 16:06:56,606 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:06:56,650 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO DAGScheduler: Got job 0 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:06:56,652 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:06:56,655 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:06:56,660 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:06:56,677 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:06:56,744 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 16:06:56,752 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 16:06:56,769 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fadb49c66c7d:40881 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:56,774 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:06:56,820 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:06:56,821 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 16:06:57,886 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:46626) with ID 0,  ResourceProfileId 0
2025-05-12 16:06:57,998 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:57 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:34719 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 34719, None)
2025-05-12 16:06:58,318 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:06:58,668 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:34719 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:06:58,959 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:34719 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:06:59,244 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 953 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:06:59,246 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 16:06:59,251 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:59 INFO DAGScheduler: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88) finished in 2.529 s
2025-05-12 16:06:59,254 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:06:59,255 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
2025-05-12 16:06:59,291 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:06:59 INFO DAGScheduler: Job 0 finished: treeAggregate at MongoInferSchema.scala:88, took 2.684327 s
2025-05-12 16:07:00,048 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fadb49c66c7d:40881 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:07:00,070 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:34719 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:07:01,126 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO SparkUI: Stopped Spark web UI at http://fadb49c66c7d:4040
2025-05-12 16:07:01,130 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 16:07:01,131 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 16:07:01,166 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 16:07:01,195 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:07:01,197 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO connection: Closed connection [connectionId{localValue:2, serverValue:82}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:07:01,200 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO MemoryStore: MemoryStore cleared
2025-05-12 16:07:01,200 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO BlockManager: BlockManager stopped
2025-05-12 16:07:01,205 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 16:07:01,211 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 16:07:01,231 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 16:07:01,745 - SparkScheduler - INFO - [user_recommender] Traceback (most recent call last):
2025-05-12 16:07:01,758 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 170, in <module>
2025-05-12 16:07:01,762 - SparkScheduler - INFO - [user_recommender] main()
2025-05-12 16:07:01,763 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 159, in main
2025-05-12 16:07:01,766 - SparkScheduler - INFO - [user_recommender] recommendation_results = generate_user_recommendations(spark)
2025-05-12 16:07:01,767 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 35, in generate_user_recommendations
2025-05-12 16:07:01,773 - SparkScheduler - INFO - [user_recommender] .load())
2025-05-12 16:07:01,774 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 184, in load
2025-05-12 16:07:01,775 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 16:07:01,777 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
2025-05-12 16:07:01,778 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
2025-05-12 16:07:01,783 - SparkScheduler - INFO - [user_recommender] py4j.protocol.Py4JJavaError: An error occurred while calling o45.load.
2025-05-12 16:07:01,792 - SparkScheduler - INFO - [user_recommender] : java.sql.SQLException: No suitable driver
2025-05-12 16:07:01,793 - SparkScheduler - INFO - [user_recommender] at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)
2025-05-12 16:07:01,794 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)
2025-05-12 16:07:01,796 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 16:07:01,797 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)
2025-05-12 16:07:01,797 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)
2025-05-12 16:07:01,798 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
2025-05-12 16:07:01,799 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
2025-05-12 16:07:01,800 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
2025-05-12 16:07:01,800 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
2025-05-12 16:07:01,801 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 16:07:01,802 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
2025-05-12 16:07:01,803 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
2025-05-12 16:07:01,804 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2025-05-12 16:07:01,805 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
2025-05-12 16:07:01,805 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2025-05-12 16:07:01,806 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.reflect.Method.invoke(Method.java:568)
2025-05-12 16:07:01,806 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2025-05-12 16:07:01,807 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2025-05-12 16:07:01,807 - SparkScheduler - INFO - [user_recommender] at py4j.Gateway.invoke(Gateway.java:282)
2025-05-12 16:07:01,808 - SparkScheduler - INFO - [user_recommender] at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2025-05-12 16:07:01,808 - SparkScheduler - INFO - [user_recommender] at py4j.commands.CallCommand.execute(CallCommand.java:79)
2025-05-12 16:07:01,809 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-05-12 16:07:01,809 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-05-12 16:07:01,809 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.Thread.run(Thread.java:840)
2025-05-12 16:07:01,810 - SparkScheduler - INFO - [user_recommender] 
2025-05-12 16:07:01,905 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 16:07:01,905 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-e539c8ff-621e-4bcf-84da-a0219aeb1e62
2025-05-12 16:07:01,915 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc44d68f-8778-436a-93e0-63698cac83c4
2025-05-12 16:07:01,923 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:07:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc44d68f-8778-436a-93e0-63698cac83c4/pyspark-96c1d49e-af72-42b3-95c7-f8f7ef597671
2025-05-12 16:07:01,976 - SparkScheduler - ERROR - [user_recommender] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 16:07:01,977 - SparkScheduler - ERROR - [user_recommender] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 16:07:01,977 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 16:07:01,978 - SparkScheduler - ERROR - [user_recommender] :: resolving dependencies :: org.apache.spark#spark-submit-parent-f8818b28-974d-49a2-8711-799750820ea9;1.0
2025-05-12 16:07:01,978 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 16:07:01,979 - SparkScheduler - ERROR - [user_recommender] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 16:07:01,979 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 16:07:01,980 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#bson;4.0.5 in central
2025-05-12 16:07:01,980 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 16:07:01,980 - SparkScheduler - ERROR - [user_recommender] :: resolution report :: resolve 283ms :: artifacts dl 12ms
2025-05-12 16:07:01,981 - SparkScheduler - ERROR - [user_recommender] :: modules in use:
2025-05-12 16:07:01,981 - SparkScheduler - ERROR - [user_recommender] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 16:07:01,982 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 16:07:01,982 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 16:07:01,983 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 16:07:01,983 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 16:07:01,983 - SparkScheduler - ERROR - [user_recommender] |                  |            modules            ||   artifacts   |
2025-05-12 16:07:01,983 - SparkScheduler - ERROR - [user_recommender] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 16:07:01,984 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 16:07:01,984 - SparkScheduler - ERROR - [user_recommender] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 16:07:01,984 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 16:07:01,985 - SparkScheduler - ERROR - [user_recommender] :: retrieving :: org.apache.spark#spark-submit-parent-f8818b28-974d-49a2-8711-799750820ea9
2025-05-12 16:07:01,985 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 16:07:01,985 - SparkScheduler - ERROR - [user_recommender] 0 artifacts copied, 4 already retrieved (0kB/9ms)
2025-05-12 16:07:01,986 - SparkScheduler - ERROR - Job user_recommender failed with exit code 1
2025-05-12 16:07:01,986 - SparkScheduler - INFO - Job user_recommender duration: 18.86 seconds
2025-05-12 16:07:01,986 - SparkScheduler - INFO - Starting job: content_analyzer - Analyze tweet content and topics
2025-05-12 16:07:01,987 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/content_analyzer.py
2025-05-12 16:07:03,823 - SparkScheduler - INFO - [content_analyzer] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 16:07:04,577 - SparkScheduler - INFO - [content_analyzer] 25/05/12 16:07:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 16:07:08,332 - SparkScheduler - INFO - [content_analyzer] [nltk_data] Downloading package punkt to /nltk_data...
2025-05-12 16:07:08,333 - SparkScheduler - INFO - [content_analyzer] Traceback (most recent call last):
2025-05-12 16:07:08,334 - SparkScheduler - INFO - [content_analyzer] File "/opt/spark-jobs/content_analyzer.py", line 11, in <module>
2025-05-12 16:07:08,336 - SparkScheduler - INFO - [content_analyzer] nltk.download('punkt')
2025-05-12 16:07:08,338 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 777, in download
2025-05-12 16:07:08,340 - SparkScheduler - INFO - [content_analyzer] for msg in self.incr_download(info_or_id, download_dir, force):
2025-05-12 16:07:08,340 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 642, in incr_download
2025-05-12 16:07:08,341 - SparkScheduler - INFO - [content_analyzer] yield from self._download_package(info, download_dir, force)
2025-05-12 16:07:08,342 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 699, in _download_package
2025-05-12 16:07:08,343 - SparkScheduler - INFO - [content_analyzer] os.makedirs(download_dir)
2025-05-12 16:07:08,344 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/os.py", line 225, in makedirs
2025-05-12 16:07:08,344 - SparkScheduler - INFO - [content_analyzer] mkdir(name, mode)
2025-05-12 16:07:08,345 - SparkScheduler - INFO - [content_analyzer] PermissionError: [Errno 13] Permission denied: '/nltk_data'
2025-05-12 16:07:08,643 - SparkScheduler - INFO - [content_analyzer] 25/05/12 16:07:08 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 16:07:08,644 - SparkScheduler - INFO - [content_analyzer] 25/05/12 16:07:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-c51673e5-6191-42c7-ac99-789600461e1f
2025-05-12 16:07:08,694 - SparkScheduler - ERROR - [content_analyzer] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 16:07:08,695 - SparkScheduler - ERROR - [content_analyzer] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 16:07:08,695 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 16:07:08,696 - SparkScheduler - ERROR - [content_analyzer] :: resolving dependencies :: org.apache.spark#spark-submit-parent-a21fb387-a0ca-411a-96ed-3af7000cf140;1.0
2025-05-12 16:07:08,697 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 16:07:08,698 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 16:07:08,699 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 16:07:08,699 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#bson;4.0.5 in central
2025-05-12 16:07:08,700 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 16:07:08,700 - SparkScheduler - ERROR - [content_analyzer] :: resolution report :: resolve 329ms :: artifacts dl 19ms
2025-05-12 16:07:08,700 - SparkScheduler - ERROR - [content_analyzer] :: modules in use:
2025-05-12 16:07:08,701 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 16:07:08,701 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 16:07:08,701 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 16:07:08,702 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 16:07:08,702 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 16:07:08,703 - SparkScheduler - ERROR - [content_analyzer] |                  |            modules            ||   artifacts   |
2025-05-12 16:07:08,703 - SparkScheduler - ERROR - [content_analyzer] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 16:07:08,703 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 16:07:08,704 - SparkScheduler - ERROR - [content_analyzer] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 16:07:08,704 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 16:07:08,704 - SparkScheduler - ERROR - [content_analyzer] :: retrieving :: org.apache.spark#spark-submit-parent-a21fb387-a0ca-411a-96ed-3af7000cf140
2025-05-12 16:07:08,705 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 16:07:08,705 - SparkScheduler - ERROR - [content_analyzer] 0 artifacts copied, 4 already retrieved (0kB/10ms)
2025-05-12 16:07:08,705 - SparkScheduler - ERROR - Job content_analyzer failed with exit code 1
2025-05-12 16:07:08,706 - SparkScheduler - INFO - Job content_analyzer duration: 6.72 seconds
2025-05-12 16:07:08,706 - SparkScheduler - INFO - Scheduler running. Press Ctrl+C to exit.
2025-05-12 16:11:10,367 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 16:11:10,368 - SparkScheduler - INFO - Waiting for services to be ready...
2025-05-12 16:11:40,026 - SparkScheduler - INFO - Setting up job schedules
2025-05-12 16:11:40,036 - SparkScheduler - INFO - Scheduled trend_analysis to run every 30 minutes
2025-05-12 16:11:40,037 - SparkScheduler - INFO - Scheduled user_recommender to run every 3 hours
2025-05-12 16:11:40,043 - SparkScheduler - INFO - Scheduled content_analyzer to run daily at 02:00
2025-05-12 16:11:40,044 - SparkScheduler - INFO - All jobs scheduled
2025-05-12 16:11:40,046 - SparkScheduler - INFO - Running initial job executions...
2025-05-12 16:11:40,046 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 16:11:40,047 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 16:11:51,059 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 16:12:35,678 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 16:12:38,045 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 16:12:38,111 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO ResourceUtils: ==============================================================
2025-05-12 16:12:38,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 16:12:38,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO ResourceUtils: ==============================================================
2025-05-12 16:12:38,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 16:12:38,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 16:12:38,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 16:12:38,164 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 16:12:38,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SecurityManager: Changing view acls to: spark
2025-05-12 16:12:38,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 16:12:38,265 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SecurityManager: Changing view acls groups to:
2025-05-12 16:12:38,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 16:12:38,267 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 16:12:38,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO Utils: Successfully started service 'sparkDriver' on port 40789.
2025-05-12 16:12:38,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 16:12:38,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 16:12:38,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 16:12:38,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 16:12:38,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 16:12:38,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-35f6958d-4548-4c40-bac0-bdfad859d2b9
2025-05-12 16:12:38,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 16:12:38,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 16:12:38,855 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 16:12:38,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://5e2bb930e569:40789/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747066358035
2025-05-12 16:12:38,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://5e2bb930e569:40789/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747066358035
2025-05-12 16:12:38,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://5e2bb930e569:40789/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747066358035
2025-05-12 16:12:38,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://5e2bb930e569:40789/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747066358035
2025-05-12 16:12:38,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://5e2bb930e569:40789/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747066358035
2025-05-12 16:12:38,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-6afcd5a9-a854-4e27-8244-9dd5aad087dd/userFiles-eff1c1bf-a77b-4f2d-a8ff-b9a5b2255783/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 16:12:38,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://5e2bb930e569:40789/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747066358035
2025-05-12 16:12:38,916 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-6afcd5a9-a854-4e27-8244-9dd5aad087dd/userFiles-eff1c1bf-a77b-4f2d-a8ff-b9a5b2255783/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 16:12:38,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://5e2bb930e569:40789/files/org.mongodb_bson-4.0.5.jar with timestamp 1747066358035
2025-05-12 16:12:38,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-6afcd5a9-a854-4e27-8244-9dd5aad087dd/userFiles-eff1c1bf-a77b-4f2d-a8ff-b9a5b2255783/org.mongodb_bson-4.0.5.jar
2025-05-12 16:12:38,939 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://5e2bb930e569:40789/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747066358035
2025-05-12 16:12:38,940 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:38 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-6afcd5a9-a854-4e27-8244-9dd5aad087dd/userFiles-eff1c1bf-a77b-4f2d-a8ff-b9a5b2255783/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 16:12:39,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 16:12:39,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 25 ms (0 ms spent in bootstraps)
2025-05-12 16:12:39,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512161239-0000
2025-05-12 16:12:39,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38185.
2025-05-12 16:12:39,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO NettyBlockTransferService: Server created on 5e2bb930e569:38185
2025-05-12 16:12:39,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 16:12:39,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e2bb930e569, 38185, None)
2025-05-12 16:12:39,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO BlockManagerMasterEndpoint: Registering block manager 5e2bb930e569:38185 with 434.4 MiB RAM, BlockManagerId(driver, 5e2bb930e569, 38185, None)
2025-05-12 16:12:39,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e2bb930e569, 38185, None)
2025-05-12 16:12:39,274 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e2bb930e569, 38185, None)
2025-05-12 16:12:39,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512161239-0000/0 on worker-20250512161115-172.23.0.8-43101 (172.23.0.8:43101) with 2 core(s)
2025-05-12 16:12:39,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512161239-0000/0 on hostPort 172.23.0.8:43101 with 2 core(s), 1024.0 MiB RAM
2025-05-12 16:12:39,486 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512161239-0000/0 is now RUNNING
2025-05-12 16:12:39,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:39 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 16:12:40,072 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 16:12:40,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 16:12:40,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:40 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 16:12:40,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 16:12:40,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 16:12:40,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5e2bb930e569:38185 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:12:40,738 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:40 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 16:12:43,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:58480) with ID 0,  ResourceProfileId 0
2025-05-12 16:12:43,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:43 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:37607 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 37607, None)
2025-05-12 16:12:44,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:44 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:44,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:44 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:44,883 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:44 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:44,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:44 INFO connection: Opened connection [connectionId{localValue:1, serverValue:8}] to mongodb:27017
2025-05-12 16:12:44,911 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:44 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5424271}
2025-05-12 16:12:44,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:44 INFO connection: Opened connection [connectionId{localValue:2, serverValue:9}] to mongodb:27017
2025-05-12 16:12:45,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:45 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:12:45,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:45 INFO CodeGenerator: Code generated in 187.323766 ms
2025-05-12 16:12:46,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:46,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:46,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO connection: Opened connection [connectionId{localValue:3, serverValue:10}] to mongodb:27017
2025-05-12 16:12:46,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1521182}
2025-05-12 16:12:46,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO connection: Opened connection [connectionId{localValue:4, serverValue:11}] to mongodb:27017
2025-05-12 16:12:46,165 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 16:12:46,172 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:12:46,173 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 16:12:46,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:12:46,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:46,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:12:46,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 16:12:46,237 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:12:46,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5e2bb930e569:38185 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:46,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:46,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:46,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 16:12:46,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:12:46,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:37607 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:47,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:37607 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:12:49,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:49 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:49,993 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:49 INFO connection: Closed connection [connectionId{localValue:2, serverValue:9}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:50,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4096 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:50,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 16:12:50,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 4.247 s
2025-05-12 16:12:50,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:12:50,447 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: running: Set()
2025-05-12 16:12:50,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: waiting: Set()
2025-05-12 16:12:50,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: failed: Set()
2025-05-12 16:12:50,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:12:50,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO CodeGenerator: Code generated in 23.144209 ms
2025-05-12 16:12:50,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:12:50,628 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO CodeGenerator: Code generated in 41.568269 ms
2025-05-12 16:12:50,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:12:50,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:12:50,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:12:50,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 16:12:50,782 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:50,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:12:50,817 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:12:50,822 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 16:12:50,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5e2bb930e569:38185 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:12:50,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:50,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:50,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 16:12:50,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:12:50,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:37607 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:12:51,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:51,185 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO connection: Closed connection [connectionId{localValue:4, serverValue:11}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:51,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:58480
2025-05-12 16:12:51,979 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 1145 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:51,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 16:12:51,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 1.178 s
2025-05-12 16:12:51,986 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:12:51,987 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 16:12:51,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:51 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 1.220819 s
2025-05-12 16:12:52,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:12:52,022 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:12:52,024 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5e2bb930e569:38185 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:12:52,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 16:12:52,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 5e2bb930e569:38185 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,085 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:37607 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:52,095 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:52,096 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:52,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO connection: Opened connection [connectionId{localValue:5, serverValue:16}] to mongodb:27017
2025-05-12 16:12:52,104 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2279751}
2025-05-12 16:12:52,109 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO connection: Opened connection [connectionId{localValue:6, serverValue:17}] to mongodb:27017
2025-05-12 16:12:52,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5e2bb930e569:38185 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,130 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:37607 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,194 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:12:52,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO CodeGenerator: Code generated in 86.268062 ms
2025-05-12 16:12:52,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:52,362 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:52,386 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:52,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO connection: Opened connection [connectionId{localValue:7, serverValue:18}] to mongodb:27017
2025-05-12 16:12:52,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1572533}
2025-05-12 16:12:52,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO connection: Opened connection [connectionId{localValue:8, serverValue:19}] to mongodb:27017
2025-05-12 16:12:52,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 16:12:52,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:12:52,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 16:12:52,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:12:52,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:52,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:12:52,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 16:12:52,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:12:52,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 5e2bb930e569:38185 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,431 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:52,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:52,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 16:12:52,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:12:52,459 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:37607 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,663 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 227 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:52,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 16:12:52,667 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.245 s
2025-05-12 16:12:52,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:12:52,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: running: Set()
2025-05-12 16:12:52,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: waiting: Set()
2025-05-12 16:12:52,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: failed: Set()
2025-05-12 16:12:52,686 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:12:52,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:12:52,782 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:12:52,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:12:52,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:12:52,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 16:12:52,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:52,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:12:52,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:12:52,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 16:12:52,831 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 5e2bb930e569:38185 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,832 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:52,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO connection: Closed connection [connectionId{localValue:8, serverValue:19}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:52,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:52,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:52,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO connection: Closed connection [connectionId{localValue:6, serverValue:17}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:52,839 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:52,840 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 16:12:52,841 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 5e2bb930e569:38185 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:12:52,842 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:12:52,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 5e2bb930e569:38185 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:37607 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:37607 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:12:52,917 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:58480
2025-05-12 16:12:52,985 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 144 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:52,986 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 16:12:52,988 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.186 s
2025-05-12 16:12:52,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:12:52,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 16:12:52,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.207285 s
2025-05-12 16:12:52,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:52 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:12:53,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:12:53,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 5e2bb930e569:38185 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:12:53,014 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 16:12:53,024 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 5e2bb930e569:38185 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:12:53,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:37607 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:12:53,127 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:53,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:53,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:53,138 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Opened connection [connectionId{localValue:9, serverValue:20}] to mongodb:27017
2025-05-12 16:12:53,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=8396050}
2025-05-12 16:12:53,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Opened connection [connectionId{localValue:10, serverValue:21}] to mongodb:27017
2025-05-12 16:12:53,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:53,165 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Closed connection [connectionId{localValue:10, serverValue:21}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:53,379 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:12:53,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:12:53,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO CodeGenerator: Code generated in 71.46895 ms
2025-05-12 16:12:53,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:53,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:53,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:53,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Opened connection [connectionId{localValue:11, serverValue:22}] to mongodb:27017
2025-05-12 16:12:53,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1452503}
2025-05-12 16:12:53,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Opened connection [connectionId{localValue:12, serverValue:23}] to mongodb:27017
2025-05-12 16:12:53,577 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:53,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Closed connection [connectionId{localValue:12, serverValue:23}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:53,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:53,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:53,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:53,588 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Opened connection [connectionId{localValue:13, serverValue:24}] to mongodb:27017
2025-05-12 16:12:53,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1397134}
2025-05-12 16:12:53,593 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Opened connection [connectionId{localValue:14, serverValue:25}] to mongodb:27017
2025-05-12 16:12:53,607 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:53,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO connection: Closed connection [connectionId{localValue:14, serverValue:25}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:53,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:53,610 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:53,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:53,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 16:12:53,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:12:53,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 16:12:53,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:12:53,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:53,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:12:53,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:12:53,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 16:12:53,636 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 5e2bb930e569:38185 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:12:53,637 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:53,638 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:53,639 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 16:12:53,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:12:53,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:37607 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:12:53,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 250 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:53,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 16:12:53,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.270 s
2025-05-12 16:12:53,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:12:53,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: running: Set()
2025-05-12 16:12:53,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: waiting: Set()
2025-05-12 16:12:53,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO DAGScheduler: failed: Set()
2025-05-12 16:12:53,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:12:53,926 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO CodeGenerator: Code generated in 11.591876 ms
2025-05-12 16:12:53,931 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:12:53,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:53 INFO CodeGenerator: Code generated in 35.401694 ms
2025-05-12 16:12:54,015 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:12:54,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:12:54,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:12:54,020 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 16:12:54,021 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:54,022 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:12:54,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.3 MiB)
2025-05-12 16:12:54,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)
2025-05-12 16:12:54,049 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 5e2bb930e569:38185 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:12:54,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 5e2bb930e569:38185 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:12:54,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:54,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:54,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 16:12:54,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:12:54,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 5e2bb930e569:38185 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:12:54,075 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:37607 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:12:54,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:37607 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:12:54,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:58480
2025-05-12 16:12:54,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 283 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:54,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 16:12:54,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.316 s
2025-05-12 16:12:54,346 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:12:54,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 16:12:54,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.330362 s
2025-05-12 16:12:54,353 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:12:54,362 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:12:54,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 5e2bb930e569:38185 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:12:54,367 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 16:12:54,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 5e2bb930e569:38185 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:12:54,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:37607 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:12:54,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:12:54,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:54,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:54,619 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:54,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO connection: Opened connection [connectionId{localValue:16, serverValue:27}] to mongodb:27017
2025-05-12 16:12:54,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1158651}
2025-05-12 16:12:54,626 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO connection: Opened connection [connectionId{localValue:17, serverValue:28}] to mongodb:27017
2025-05-12 16:12:54,627 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:54,628 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO connection: Closed connection [connectionId{localValue:17, serverValue:28}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:54,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:54,631 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:54,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:54,633 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO connection: Opened connection [connectionId{localValue:18, serverValue:29}] to mongodb:27017
2025-05-12 16:12:54,635 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1020765}
2025-05-12 16:12:54,637 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO connection: Opened connection [connectionId{localValue:19, serverValue:30}] to mongodb:27017
2025-05-12 16:12:54,649 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:54,650 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO connection: Closed connection [connectionId{localValue:19, serverValue:30}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:54,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:54,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:54,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:54,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 16:12:54,659 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:12:54,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:12:54,661 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:12:54,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:54,666 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:12:54,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.4 MiB)
2025-05-12 16:12:54,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:12:54,678 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 5e2bb930e569:38185 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:54,683 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:54,684 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:54,685 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 16:12:54,686 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:12:54,715 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:37607 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:54,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 151 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:54,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 16:12:54,840 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.173 s
2025-05-12 16:12:54,841 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:12:54,842 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: running: Set()
2025-05-12 16:12:54,843 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: waiting: Set()
2025-05-12 16:12:54,844 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO DAGScheduler: failed: Set()
2025-05-12 16:12:54,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:12:54,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO CodeGenerator: Code generated in 19.581084 ms
2025-05-12 16:12:54,956 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO CodeGenerator: Code generated in 9.375605 ms
2025-05-12 16:12:54,959 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:12:54,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:54 INFO CodeGenerator: Code generated in 23.787346 ms
2025-05-12 16:12:55,023 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:12:55,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:12:55,026 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:12:55,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 16:12:55,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:55,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:12:55,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 16:12:55,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 16:12:55,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 5e2bb930e569:38185 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:12:55,049 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 5e2bb930e569:38185 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:55,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:55,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:55,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 16:12:55,054 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:37607 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:55,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:12:55,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 5e2bb930e569:38185 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:12:55,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:37607 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:12:55,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:58480
2025-05-12 16:12:55,200 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 144 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:55,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 16:12:55,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.169 s
2025-05-12 16:12:55,203 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:12:55,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 16:12:55,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.180262 s
2025-05-12 16:12:55,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:12:55,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:55,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:55,341 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:55,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO connection: Opened connection [connectionId{localValue:21, serverValue:33}] to mongodb:27017
2025-05-12 16:12:55,345 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1430919}
2025-05-12 16:12:55,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO connection: Opened connection [connectionId{localValue:22, serverValue:34}] to mongodb:27017
2025-05-12 16:12:55,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:55,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO connection: Closed connection [connectionId{localValue:22, serverValue:34}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:55,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:55,353 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:55,354 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:12:55,356 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO connection: Opened connection [connectionId{localValue:23, serverValue:35}] to mongodb:27017
2025-05-12 16:12:55,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1285155}
2025-05-12 16:12:55,361 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO connection: Opened connection [connectionId{localValue:24, serverValue:36}] to mongodb:27017
2025-05-12 16:12:55,371 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:55,373 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO connection: Closed connection [connectionId{localValue:24, serverValue:36}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:12:55,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:12:55,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:12:55,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:12:55,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 16:12:55,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:12:55,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:12:55,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:12:55,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:55,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:12:55,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.3 MiB)
2025-05-12 16:12:55,399 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:12:55,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 5e2bb930e569:38185 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:55,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:12:55,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:12:55,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 16:12:55,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:37607 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:12:55,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:12:55,412 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 5e2bb930e569:38185 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:12:55,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:37607 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:12:55,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 108 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:12:55,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 16:12:55,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.134 s
2025-05-12 16:12:55,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:12:55,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: running: Set()
2025-05-12 16:12:55,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: waiting: Set()
2025-05-12 16:12:55,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: failed: Set()
2025-05-12 16:12:55,528 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:12:55,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:12:55,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:12:55,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:12:55,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:12:55,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 16:12:55,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:12:55,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:12:55 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:13:00,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 16:13:00,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 16:13:00,021 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 5e2bb930e569:38185 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,022 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:00,023 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:00,024 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 16:13:00,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:13:00,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:37607 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:58480
2025-05-12 16:13:00,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 5e2bb930e569:38185 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,084 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:37607 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,105 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 78 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:00,106 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 16:13:00,107 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.095 s
2025-05-12 16:13:00,108 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:00,109 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 16:13:00,109 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.104060 s
2025-05-12 16:13:00,185 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:13:00,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:13:00,260 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:00,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:00,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:00,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Opened connection [connectionId{localValue:26, serverValue:38}] to mongodb:27017
2025-05-12 16:13:00,265 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1210004}
2025-05-12 16:13:00,269 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Opened connection [connectionId{localValue:27, serverValue:39}] to mongodb:27017
2025-05-12 16:13:00,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:00,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Closed connection [connectionId{localValue:27, serverValue:39}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:00,274 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:00,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:00,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:00,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Opened connection [connectionId{localValue:28, serverValue:40}] to mongodb:27017
2025-05-12 16:13:00,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1218063}
2025-05-12 16:13:00,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Opened connection [connectionId{localValue:29, serverValue:41}] to mongodb:27017
2025-05-12 16:13:00,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:00,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Closed connection [connectionId{localValue:29, serverValue:41}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:00,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:00,297 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:00,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:00,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 16:13:00,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:13:00,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:13:00,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:00,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:00,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:13:00,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:13:00,313 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 16:13:00,314 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 5e2bb930e569:38185 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:00,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:00,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 16:13:00,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:13:00,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:37607 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 79 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:00,399 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 16:13:00,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.093 s
2025-05-12 16:13:00,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:13:00,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: running: Set()
2025-05-12 16:13:00,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: waiting: Set()
2025-05-12 16:13:00,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: failed: Set()
2025-05-12 16:13:00,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:13:00,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:13:00,461 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO CodeGenerator: Code generated in 18.596802 ms
2025-05-12 16:13:00,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:13:00,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:13:00,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:13:00,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 16:13:00,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:00,486 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:13:00,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.2 MiB)
2025-05-12 16:13:00,499 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.2 MiB)
2025-05-12 16:13:00,501 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 5e2bb930e569:38185 (size: 25.7 KiB, free: 434.3 MiB)
2025-05-12 16:13:00,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:00,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 5e2bb930e569:38185 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:00,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 16:13:00,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:13:00,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:37607 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 5e2bb930e569:38185 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,528 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:37607 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,536 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:37607 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:58480
2025-05-12 16:13:00,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 104 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:00,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 16:13:00,613 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.125 s
2025-05-12 16:13:00,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:00,615 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 16:13:00,616 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.132116 s
2025-05-12 16:13:00,619 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 4, 'weekly': 4, 'hourly': 8}
2025-05-12 16:13:00,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 16:13:00,644 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 16:13:00,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 5e2bb930e569:38185 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,646 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 5e2bb930e569:38185 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:13:00,647 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 16:13:00,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:37607 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:00,659 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:00,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:00,661 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Opened connection [connectionId{localValue:31, serverValue:43}] to mongodb:27017
2025-05-12 16:13:00,663 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2073049}
2025-05-12 16:13:00,666 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Opened connection [connectionId{localValue:32, serverValue:44}] to mongodb:27017
2025-05-12 16:13:00,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:00,669 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO connection: Closed connection [connectionId{localValue:32, serverValue:44}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:00,682 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:00,684 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:00,686 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:00,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:13:00,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:13:00,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:13:00,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:00,715 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:00,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:13:00,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 16:13:00,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 16:13:00,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 5e2bb930e569:38185 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:00,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:00,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 16:13:00,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:13:00,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:37607 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:00,791 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:37607 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:13:00,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 131 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:00,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 16:13:00,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.146 s
2025-05-12 16:13:00,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:00,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 16:13:00,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:00 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.155051 s
2025-05-12 16:13:01,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:01,041 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:01,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:01,044 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:34, serverValue:46}] to mongodb:27017
2025-05-12 16:13:01,045 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=897972}
2025-05-12 16:13:01,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:35, serverValue:47}] to mongodb:27017
2025-05-12 16:13:01,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:01,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Closed connection [connectionId{localValue:35, serverValue:47}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:01,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 16:13:01,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO CodeGenerator: Code generated in 52.002971 ms
2025-05-12 16:13:01,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:01,176 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:01,177 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:01,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:36, serverValue:48}] to mongodb:27017
2025-05-12 16:13:01,183 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1339662}
2025-05-12 16:13:01,185 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:37, serverValue:49}] to mongodb:27017
2025-05-12 16:13:01,187 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:01,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Closed connection [connectionId{localValue:37, serverValue:49}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:01,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:01,193 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:01,194 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:01,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:38, serverValue:50}] to mongodb:27017
2025-05-12 16:13:01,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1180555}
2025-05-12 16:13:01,203 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:39, serverValue:51}] to mongodb:27017
2025-05-12 16:13:01,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:01,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Closed connection [connectionId{localValue:39, serverValue:51}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:01,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:01,220 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:01,221 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:01,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 16:13:01,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:13:01,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 16:13:01,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:01,228 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:01,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:13:01,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.5 KiB, free 434.3 MiB)
2025-05-12 16:13:01,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.3 MiB)
2025-05-12 16:13:01,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 5e2bb930e569:38185 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,244 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:01,245 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 5e2bb930e569:38185 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,246 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:01,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 16:13:01,250 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:37607 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:13:01,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:37607 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 184 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:01,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 16:13:01,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.207 s
2025-05-12 16:13:01,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:13:01,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: running: Set()
2025-05-12 16:13:01,438 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: waiting: Set()
2025-05-12 16:13:01,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: failed: Set()
2025-05-12 16:13:01,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:13:01,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:13:01,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO CodeGenerator: Code generated in 23.144463 ms
2025-05-12 16:13:01,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:13:01,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Got job 14 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:13:01,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Final stage: ResultStage 21 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:13:01,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 16:13:01,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:01,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:13:01,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 52.5 KiB, free 434.3 MiB)
2025-05-12 16:13:01,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 434.3 MiB)
2025-05-12 16:13:01,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 5e2bb930e569:38185 (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:01,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:01,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 16:13:01,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:13:01,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:37607 (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:58480
2025-05-12 16:13:01,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 5e2bb930e569:38185 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:37607 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 177 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:01,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 16:13:01,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: ResultStage 21 (foreachPartition at MongoSpark.scala:120) finished in 0.196 s
2025-05-12 16:13:01,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:01,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 16:13:01,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Job 14 finished: foreachPartition at MongoSpark.scala:120, took 0.206237 s
2025-05-12 16:13:01,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:13:01,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:13:01,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 5e2bb930e569:38185 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:13:01,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO SparkContext: Created broadcast 20 from broadcast at MongoSpark.scala:530
2025-05-12 16:13:01,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:01,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:01,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:01,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:41, serverValue:53}] to mongodb:27017
2025-05-12 16:13:01,751 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=907137}
2025-05-12 16:13:01,753 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:42, serverValue:54}] to mongodb:27017
2025-05-12 16:13:01,755 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:01,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Closed connection [connectionId{localValue:42, serverValue:54}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:01,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 16:13:01,843 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO CodeGenerator: Code generated in 29.707179 ms
2025-05-12 16:13:01,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:01,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:01,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:01,850 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:43, serverValue:55}] to mongodb:27017
2025-05-12 16:13:01,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1347645}
2025-05-12 16:13:01,855 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:44, serverValue:56}] to mongodb:27017
2025-05-12 16:13:01,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:01,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Closed connection [connectionId{localValue:44, serverValue:56}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:01,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:01,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:01,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:01,862 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:45, serverValue:57}] to mongodb:27017
2025-05-12 16:13:01,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1072637}
2025-05-12 16:13:01,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Opened connection [connectionId{localValue:46, serverValue:58}] to mongodb:27017
2025-05-12 16:13:01,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:01,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO connection: Closed connection [connectionId{localValue:46, serverValue:58}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:01,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:01,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:01,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:01,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Registering RDD 91 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 16:13:01,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:13:01,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (rdd at MongoSpark.scala:169)
2025-05-12 16:13:01,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:01,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:01,891 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:13:01,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 39.6 KiB, free 434.3 MiB)
2025-05-12 16:13:01,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 16:13:01,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 5e2bb930e569:38185 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:01,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 5e2bb930e569:38185 in memory (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,909 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:01,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
2025-05-12 16:13:01,911 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:37607 in memory (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:01,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 15) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:13:01,927 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 5e2bb930e569:38185 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:13:01,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:01 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:37607 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 15) in 153 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:02,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
2025-05-12 16:13:02,065 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: ShuffleMapStage 22 (rdd at MongoSpark.scala:169) finished in 0.174 s
2025-05-12 16:13:02,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:13:02,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: running: Set()
2025-05-12 16:13:02,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: waiting: Set()
2025-05-12 16:13:02,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: failed: Set()
2025-05-12 16:13:02,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:13:02,076 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:13:02,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:13:02,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:13:02,101 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Final stage: ResultStage 24 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:13:02,102 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
2025-05-12 16:13:02,102 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:02,103 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:13:02,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 48.4 KiB, free 434.3 MiB)
2025-05-12 16:13:02,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:13:02,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 5e2bb930e569:38185 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:02,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:02,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
2025-05-12 16:13:02,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:13:02,137 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.23.0.8:37607 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:58480
2025-05-12 16:13:02,163 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 5e2bb930e569:38185 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:37607 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 16) in 95 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:02,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
2025-05-12 16:13:02,214 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: ResultStage 24 (foreachPartition at MongoSpark.scala:120) finished in 0.108 s
2025-05-12 16:13:02,214 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:02,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
2025-05-12 16:13:02,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.116287 s
2025-05-12 16:13:02,221 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:13:02,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:13:02,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 5e2bb930e569:38185 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:13:02,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO SparkContext: Created broadcast 23 from broadcast at MongoSpark.scala:530
2025-05-12 16:13:02,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 16:13:02,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:13:02,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO CodeGenerator: Code generated in 15.78996 ms
2025-05-12 16:13:02,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:02,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:02,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:02,321 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO connection: Opened connection [connectionId{localValue:48, serverValue:60}] to mongodb:27017
2025-05-12 16:13:02,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=964621}
2025-05-12 16:13:02,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO connection: Opened connection [connectionId{localValue:49, serverValue:61}] to mongodb:27017
2025-05-12 16:13:02,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:02,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO connection: Closed connection [connectionId{localValue:49, serverValue:61}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:02,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:02,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:02,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:02,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO connection: Opened connection [connectionId{localValue:50, serverValue:62}] to mongodb:27017
2025-05-12 16:13:02,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=715790}
2025-05-12 16:13:02,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO connection: Opened connection [connectionId{localValue:51, serverValue:63}] to mongodb:27017
2025-05-12 16:13:02,332 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:02,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO connection: Closed connection [connectionId{localValue:51, serverValue:63}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:02,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:02,335 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:02,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:02,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Registering RDD 103 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 8
2025-05-12 16:13:02,339 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Got map stage job 17 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:13:02,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:13:02,341 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:02,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:02,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:13:02,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 36.4 KiB, free 434.3 MiB)
2025-05-12 16:13:02,353 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 434.3 MiB)
2025-05-12 16:13:02,356 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 5e2bb930e569:38185 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:02,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 5e2bb930e569:38185 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:02,366 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
2025-05-12 16:13:02,367 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:13:02,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.23.0.8:37607 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 5e2bb930e569:38185 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:13:02,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:37607 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 195 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:02,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
2025-05-12 16:13:02,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.223 s
2025-05-12 16:13:02,574 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:13:02,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: running: Set()
2025-05-12 16:13:02,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: waiting: Set()
2025-05-12 16:13:02,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: failed: Set()
2025-05-12 16:13:02,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:13:02,688 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:13:02,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO CodeGenerator: Code generated in 25.559652 ms
2025-05-12 16:13:02,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Registering RDD 106 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 16:13:02,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Got map stage job 18 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:13:02,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:13:02,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
2025-05-12 16:13:02,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:02,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:13:02,752 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 45.0 KiB, free 434.3 MiB)
2025-05-12 16:13:02,754 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.3 MiB)
2025-05-12 16:13:02,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 5e2bb930e569:38185 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:02,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:02,758 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 16:13:02,760 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 16:13:02,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.23.0.8:37607 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:58480
2025-05-12 16:13:02,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 5e2bb930e569:38185 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,817 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:37607 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,855 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 18) in 96 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:02,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 16:13:02,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0) finished in 0.111 s
2025-05-12 16:13:02,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:13:02,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: running: Set()
2025-05-12 16:13:02,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: waiting: Set()
2025-05-12 16:13:02,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: failed: Set()
2025-05-12 16:13:02,883 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO CodeGenerator: Code generated in 9.544135 ms
2025-05-12 16:13:02,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:13:02,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Got job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:13:02,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Final stage: ResultStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:13:02,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
2025-05-12 16:13:02,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:02,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:13:02,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.3 MiB)
2025-05-12 16:13:02,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)
2025-05-12 16:13:02,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 5e2bb930e569:38185 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:02,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:02,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 16:13:02,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:13:02,930 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:37607 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:58480
2025-05-12 16:13:02,947 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 5e2bb930e569:38185 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.23.0.8:37607 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:13:02,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 63 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:02,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 16:13:02,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: ResultStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.074 s
2025-05-12 16:13:02,976 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:02,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
2025-05-12 16:13:02,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO DAGScheduler: Job 19 finished: count at NativeMethodAccessorImpl.java:0, took 0.080696 s
2025-05-12 16:13:02,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:02 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 16:13:03,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:03,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:03,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:03,030 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Opened connection [connectionId{localValue:53, serverValue:65}] to mongodb:27017
2025-05-12 16:13:03,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=807885}
2025-05-12 16:13:03,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Opened connection [connectionId{localValue:54, serverValue:66}] to mongodb:27017
2025-05-12 16:13:03,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:03,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Closed connection [connectionId{localValue:54, serverValue:66}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:03,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:03,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:03,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:03,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Opened connection [connectionId{localValue:55, serverValue:67}] to mongodb:27017
2025-05-12 16:13:03,041 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1070943}
2025-05-12 16:13:03,043 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Opened connection [connectionId{localValue:56, serverValue:68}] to mongodb:27017
2025-05-12 16:13:03,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:03,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Closed connection [connectionId{localValue:56, serverValue:68}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:03,054 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:03,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:03,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:03,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Registering RDD 114 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 16:13:03,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:13:03,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:13:03,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:03,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:03,062 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:13:03,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 39.7 KiB, free 434.3 MiB)
2025-05-12 16:13:03,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 16:13:03,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 5e2bb930e569:38185 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:03,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:03,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
2025-05-12 16:13:03,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 20) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:13:03,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:37607 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 20) in 83 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:03,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
2025-05-12 16:13:03,156 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0) finished in 0.094 s
2025-05-12 16:13:03,157 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:13:03,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: running: Set()
2025-05-12 16:13:03,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: waiting: Set()
2025-05-12 16:13:03,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: failed: Set()
2025-05-12 16:13:03,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:13:03,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:13:03,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:13:03,219 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:13:03,219 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Final stage: ResultStage 33 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:13:03,220 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
2025-05-12 16:13:03,221 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:03,222 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:13:03,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 46.3 KiB, free 434.3 MiB)
2025-05-12 16:13:03,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.3 MiB)
2025-05-12 16:13:03,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 5e2bb930e569:38185 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,237 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:03,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:03,240 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
2025-05-12 16:13:03,245 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 5e2bb930e569:38185 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,246 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.23.0.8:37607 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:13:03,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 5e2bb930e569:38185 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.23.0.8:37607 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:37607 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:58480
2025-05-12 16:13:03,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 21) in 65 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:03,317 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
2025-05-12 16:13:03,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: ResultStage 33 (count at NativeMethodAccessorImpl.java:0) finished in 0.095 s
2025-05-12 16:13:03,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:03,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
2025-05-12 16:13:03,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.102104 s
2025-05-12 16:13:03,323 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 1, 'active_users': 1}
2025-05-12 16:13:03,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 16:13:03,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 16:13:03,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 5e2bb930e569:38185 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,345 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 5e2bb930e569:38185 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:13:03,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Created broadcast 29 from broadcast at MongoSpark.scala:530
2025-05-12 16:13:03,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.23.0.8:37607 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:03,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:03,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:03,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Opened connection [connectionId{localValue:58, serverValue:70}] to mongodb:27017
2025-05-12 16:13:03,354 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1891703}
2025-05-12 16:13:03,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Opened connection [connectionId{localValue:59, serverValue:71}] to mongodb:27017
2025-05-12 16:13:03,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:03,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Closed connection [connectionId{localValue:59, serverValue:71}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:03,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:03,366 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:03,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:03,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:13:03,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Got job 22 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:13:03,386 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Final stage: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:13:03,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:03,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:03,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:13:03,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 16:13:03,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 16:13:03,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 5e2bb930e569:38185 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:03,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:03,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
2025-05-12 16:13:03,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:13:03,425 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:37607 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:37607 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:13:03,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 22) in 59 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:03,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
2025-05-12 16:13:03,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88) finished in 0.077 s
2025-05-12 16:13:03,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:03,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
2025-05-12 16:13:03,467 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Job 22 finished: treeAggregate at MongoInferSchema.scala:88, took 0.081800 s
2025-05-12 16:13:03,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 16:13:03,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 16:13:03,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 5e2bb930e569:38185 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,495 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 5e2bb930e569:38185 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:13:03,498 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.23.0.8:37607 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,499 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 16:13:03,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:03,501 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:03,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:03,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Opened connection [connectionId{localValue:61, serverValue:73}] to mongodb:27017
2025-05-12 16:13:03,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=999701}
2025-05-12 16:13:03,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Opened connection [connectionId{localValue:62, serverValue:74}] to mongodb:27017
2025-05-12 16:13:03,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:03,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO connection: Closed connection [connectionId{localValue:62, serverValue:74}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:03,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:03,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:03,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:03,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:13:03,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Got job 23 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:13:03,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Final stage: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:13:03,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:03,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:03,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:13:03,535 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 16:13:03,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 16:13:03,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 5e2bb930e569:38185 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:03,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:03,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 16:13:03,544 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 23) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:13:03,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:37607 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:03,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:37607 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:13:03,607 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 23) in 63 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:03,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 16:13:03,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88) finished in 0.075 s
2025-05-12 16:13:03,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:03,610 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 16:13:03,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO DAGScheduler: Job 23 finished: treeAggregate at MongoInferSchema.scala:88, took 0.079934 s
2025-05-12 16:13:03,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkUI: Stopped Spark web UI at http://5e2bb930e569:4040
2025-05-12 16:13:03,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 16:13:03,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 16:13:03,703 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 16:13:03,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO MemoryStore: MemoryStore cleared
2025-05-12 16:13:03,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManager: BlockManager stopped
2025-05-12 16:13:03,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 16:13:03,806 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 16:13:03,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:03 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 16:13:04,219 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 16:13:04,220 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 16:13:04,222 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 16:13:04,223 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 16:13:04,225 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 16:13:04,226 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 16:13:04,227 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 16:13:04,228 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 16:13:04,228 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 16:13:04,229 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 16:13:04,235 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 16:13:04,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:04 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 16:13:04,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-f79c3698-0acd-4032-9f1d-e9735e36182e
2025-05-12 16:13:04,299 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-6afcd5a9-a854-4e27-8244-9dd5aad087dd
2025-05-12 16:13:04,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:13:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-6afcd5a9-a854-4e27-8244-9dd5aad087dd/pyspark-921bf6b7-1978-4231-9986-41ba93fbd761
2025-05-12 16:13:04,394 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 16:13:04,395 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 16:13:04,395 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 16:13:04,396 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-a41151d2-7763-4158-8f27-ac7fd8555ba1;1.0
2025-05-12 16:13:04,396 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 16:13:04,396 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 16:13:04,397 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 16:13:04,397 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 16:13:04,397 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 16:13:04,398 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...
2025-05-12 16:13:04,398 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (2140ms)
2025-05-12 16:13:04,398 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...
2025-05-12 16:13:04,398 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (938ms)
2025-05-12 16:13:04,399 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...
2025-05-12 16:13:04,399 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (1898ms)
2025-05-12 16:13:04,399 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...
2025-05-12 16:13:04,399 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (28354ms)
2025-05-12 16:13:04,400 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 10059ms :: artifacts dl 33352ms
2025-05-12 16:13:04,400 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 16:13:04,400 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 16:13:04,401 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 16:13:04,402 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 16:13:04,402 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 16:13:04,403 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:13:04,404 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 16:13:04,404 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 16:13:04,405 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:13:04,405 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   4   |   4   |   0   ||   4   |   4   |
2025-05-12 16:13:04,406 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:13:04,406 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-a41151d2-7763-4158-8f27-ac7fd8555ba1
2025-05-12 16:13:04,406 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 16:13:04,407 - SparkScheduler - ERROR - [trend_analysis] 4 artifacts copied, 0 already retrieved (2728kB/35ms)
2025-05-12 16:13:04,412 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 16:13:04,414 - SparkScheduler - INFO - Job trend_analysis duration: 84.37 seconds
2025-05-12 16:13:04,430 - SparkScheduler - INFO - Starting job: user_recommender - Generate user recommendations
2025-05-12 16:13:04,431 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/user_recommender.py
2025-05-12 16:13:07,135 - SparkScheduler - INFO - [user_recommender] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 16:13:08,342 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 16:13:10,574 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 16:13:10,600 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO ResourceUtils: ==============================================================
2025-05-12 16:13:10,600 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 16:13:10,601 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO ResourceUtils: ==============================================================
2025-05-12 16:13:10,602 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO SparkContext: Submitted application: MiniTwitterUserRecommender
2025-05-12 16:13:10,673 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 16:13:10,713 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 16:13:10,715 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 16:13:10,827 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO SecurityManager: Changing view acls to: spark
2025-05-12 16:13:10,829 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 16:13:10,831 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO SecurityManager: Changing view acls groups to:
2025-05-12 16:13:10,834 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 16:13:10,835 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 16:13:11,327 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO Utils: Successfully started service 'sparkDriver' on port 42177.
2025-05-12 16:13:11,394 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 16:13:11,493 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 16:13:11,545 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 16:13:11,547 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 16:13:11,564 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 16:13:11,608 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2fcfbf1e-1546-45a1-8e34-e0d0a185a5e1
2025-05-12 16:13:11,648 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 16:13:11,693 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 16:13:12,127 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 16:13:12,287 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://5e2bb930e569:42177/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747066390565
2025-05-12 16:13:12,307 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://5e2bb930e569:42177/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747066390565
2025-05-12 16:13:12,308 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://5e2bb930e569:42177/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747066390565
2025-05-12 16:13:12,309 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://5e2bb930e569:42177/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747066390565
2025-05-12 16:13:12,318 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://5e2bb930e569:42177/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747066390565
2025-05-12 16:13:12,329 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-1ba5afba-3e66-4cc7-a280-21293bbf562d/userFiles-402c335a-3c7f-4221-b3db-7c4b68d935bd/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 16:13:12,418 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://5e2bb930e569:42177/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747066390565
2025-05-12 16:13:12,422 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-1ba5afba-3e66-4cc7-a280-21293bbf562d/userFiles-402c335a-3c7f-4221-b3db-7c4b68d935bd/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 16:13:12,508 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://5e2bb930e569:42177/files/org.mongodb_bson-4.0.5.jar with timestamp 1747066390565
2025-05-12 16:13:12,510 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-1ba5afba-3e66-4cc7-a280-21293bbf562d/userFiles-402c335a-3c7f-4221-b3db-7c4b68d935bd/org.mongodb_bson-4.0.5.jar
2025-05-12 16:13:12,562 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://5e2bb930e569:42177/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747066390565
2025-05-12 16:13:12,563 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-1ba5afba-3e66-4cc7-a280-21293bbf562d/userFiles-402c335a-3c7f-4221-b3db-7c4b68d935bd/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 16:13:12,785 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 16:13:12,873 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 52 ms (0 ms spent in bootstraps)
2025-05-12 16:13:13,083 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512161313-0001
2025-05-12 16:13:13,124 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512161313-0001/0 on worker-20250512161115-172.23.0.8-43101 (172.23.0.8:43101) with 2 core(s)
2025-05-12 16:13:13,128 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512161313-0001/0 on hostPort 172.23.0.8:43101 with 2 core(s), 1024.0 MiB RAM
2025-05-12 16:13:13,166 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45817.
2025-05-12 16:13:13,169 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO NettyBlockTransferService: Server created on 5e2bb930e569:45817
2025-05-12 16:13:13,176 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 16:13:13,190 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e2bb930e569, 45817, None)
2025-05-12 16:13:13,206 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO BlockManagerMasterEndpoint: Registering block manager 5e2bb930e569:45817 with 434.4 MiB RAM, BlockManagerId(driver, 5e2bb930e569, 45817, None)
2025-05-12 16:13:13,220 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e2bb930e569, 45817, None)
2025-05-12 16:13:13,225 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e2bb930e569, 45817, None)
2025-05-12 16:13:13,339 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512161313-0001/0 is now RUNNING
2025-05-12 16:13:14,200 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:14 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 16:13:11,530 - SparkScheduler - INFO - [user_recommender] Starting Mini Twitter User Recommender...
2025-05-12 16:13:11,562 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 16:13:11,572 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:11 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 16:13:17,905 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 16:13:17,964 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 16:13:17,971 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5e2bb930e569:45817 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:13:17,985 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:17 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 16:13:18,288 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:18 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:13:18,508 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:18 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:13:18,564 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:18 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:13:18,600 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:18 INFO connection: Opened connection [connectionId{localValue:1, serverValue:77}] to mongodb:27017
2025-05-12 16:13:18,627 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=16840004}
2025-05-12 16:13:18,767 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:18 INFO connection: Opened connection [connectionId{localValue:2, serverValue:78}] to mongodb:27017
2025-05-12 16:13:19,743 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:13:19,824 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO DAGScheduler: Got job 0 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:13:19,826 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:13:19,828 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:13:19,832 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:13:19,855 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:13:19,956 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 16:13:19,963 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 16:13:19,971 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5e2bb930e569:45817 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:19,975 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:13:20,031 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:13:20,034 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 16:13:21,007 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:50378) with ID 0,  ResourceProfileId 0
2025-05-12 16:13:21,230 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:21 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:43869 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 43869, None)
2025-05-12 16:13:21,651 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:13:22,310 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:43869 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:23,095 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:43869 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:13:23,505 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1877 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:13:23,508 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 16:13:23,515 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:23 INFO DAGScheduler: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88) finished in 3.599 s
2025-05-12 16:13:23,520 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:13:23,521 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
2025-05-12 16:13:23,590 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:23 INFO DAGScheduler: Job 0 finished: treeAggregate at MongoInferSchema.scala:88, took 3.852464 s
2025-05-12 16:13:24,253 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5e2bb930e569:45817 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:24,263 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:43869 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:13:24,684 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:13:24,687 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:24 INFO connection: Closed connection [connectionId{localValue:2, serverValue:78}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:13:26,250 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO SparkUI: Stopped Spark web UI at http://5e2bb930e569:4040
2025-05-12 16:13:26,255 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 16:13:26,257 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 16:13:26,292 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 16:13:26,331 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO MemoryStore: MemoryStore cleared
2025-05-12 16:13:26,332 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO BlockManager: BlockManager stopped
2025-05-12 16:13:26,338 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 16:13:26,342 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 16:13:26,367 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 16:13:26,740 - SparkScheduler - INFO - [user_recommender] Traceback (most recent call last):
2025-05-12 16:13:26,741 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 170, in <module>
2025-05-12 16:13:26,745 - SparkScheduler - INFO - [user_recommender] main()
2025-05-12 16:13:26,746 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 159, in main
2025-05-12 16:13:26,750 - SparkScheduler - INFO - [user_recommender] recommendation_results = generate_user_recommendations(spark)
2025-05-12 16:13:26,751 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 35, in generate_user_recommendations
2025-05-12 16:13:26,756 - SparkScheduler - INFO - [user_recommender] .load())
2025-05-12 16:13:26,757 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 184, in load
2025-05-12 16:13:26,759 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 16:13:26,760 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
2025-05-12 16:13:26,761 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
2025-05-12 16:13:26,764 - SparkScheduler - INFO - [user_recommender] py4j.protocol.Py4JJavaError: An error occurred while calling o45.load.
2025-05-12 16:13:26,767 - SparkScheduler - INFO - [user_recommender] : java.sql.SQLException: No suitable driver
2025-05-12 16:13:26,768 - SparkScheduler - INFO - [user_recommender] at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)
2025-05-12 16:13:26,770 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)
2025-05-12 16:13:26,771 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 16:13:26,772 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)
2025-05-12 16:13:26,773 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)
2025-05-12 16:13:26,774 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
2025-05-12 16:13:26,774 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
2025-05-12 16:13:26,776 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
2025-05-12 16:13:26,777 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
2025-05-12 16:13:26,778 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 16:13:26,778 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
2025-05-12 16:13:26,779 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
2025-05-12 16:13:26,780 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2025-05-12 16:13:26,781 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
2025-05-12 16:13:26,782 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2025-05-12 16:13:26,783 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.reflect.Method.invoke(Method.java:568)
2025-05-12 16:13:26,783 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2025-05-12 16:13:26,784 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2025-05-12 16:13:26,785 - SparkScheduler - INFO - [user_recommender] at py4j.Gateway.invoke(Gateway.java:282)
2025-05-12 16:13:26,785 - SparkScheduler - INFO - [user_recommender] at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2025-05-12 16:13:26,786 - SparkScheduler - INFO - [user_recommender] at py4j.commands.CallCommand.execute(CallCommand.java:79)
2025-05-12 16:13:26,786 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-05-12 16:13:26,787 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-05-12 16:13:26,788 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.Thread.run(Thread.java:840)
2025-05-12 16:13:26,788 - SparkScheduler - INFO - [user_recommender] 
2025-05-12 16:13:26,946 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 16:13:26,947 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-2c1c0427-f76c-4929-a350-a9da76193ab5
2025-05-12 16:13:26,963 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-1ba5afba-3e66-4cc7-a280-21293bbf562d/pyspark-2e2b8989-abdc-467e-8279-6c98db6011b2
2025-05-12 16:13:26,978 - SparkScheduler - INFO - [user_recommender] 25/05/12 16:13:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-1ba5afba-3e66-4cc7-a280-21293bbf562d
2025-05-12 16:13:27,041 - SparkScheduler - ERROR - [user_recommender] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 16:13:27,041 - SparkScheduler - ERROR - [user_recommender] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 16:13:27,042 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 16:13:27,043 - SparkScheduler - ERROR - [user_recommender] :: resolving dependencies :: org.apache.spark#spark-submit-parent-bb59f2aa-0a55-4066-8bd3-187735df40b0;1.0
2025-05-12 16:13:27,043 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 16:13:27,043 - SparkScheduler - ERROR - [user_recommender] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 16:13:27,044 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 16:13:27,044 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#bson;4.0.5 in central
2025-05-12 16:13:27,045 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 16:13:27,045 - SparkScheduler - ERROR - [user_recommender] :: resolution report :: resolve 454ms :: artifacts dl 28ms
2025-05-12 16:13:27,046 - SparkScheduler - ERROR - [user_recommender] :: modules in use:
2025-05-12 16:13:27,046 - SparkScheduler - ERROR - [user_recommender] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 16:13:27,046 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 16:13:27,047 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 16:13:27,047 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 16:13:27,048 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 16:13:27,049 - SparkScheduler - ERROR - [user_recommender] |                  |            modules            ||   artifacts   |
2025-05-12 16:13:27,049 - SparkScheduler - ERROR - [user_recommender] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 16:13:27,050 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 16:13:27,050 - SparkScheduler - ERROR - [user_recommender] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 16:13:27,050 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 16:13:27,051 - SparkScheduler - ERROR - [user_recommender] :: retrieving :: org.apache.spark#spark-submit-parent-bb59f2aa-0a55-4066-8bd3-187735df40b0
2025-05-12 16:13:27,052 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 16:13:27,052 - SparkScheduler - ERROR - [user_recommender] 0 artifacts copied, 4 already retrieved (0kB/19ms)
2025-05-12 16:13:27,052 - SparkScheduler - ERROR - Job user_recommender failed with exit code 1
2025-05-12 16:13:27,053 - SparkScheduler - INFO - Job user_recommender duration: 22.62 seconds
2025-05-12 16:13:27,054 - SparkScheduler - INFO - Starting job: content_analyzer - Analyze tweet content and topics
2025-05-12 16:13:27,054 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/content_analyzer.py
2025-05-12 16:13:29,980 - SparkScheduler - INFO - [content_analyzer] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 16:13:31,050 - SparkScheduler - INFO - [content_analyzer] 25/05/12 16:13:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 16:13:36,487 - SparkScheduler - INFO - [content_analyzer] [nltk_data] Downloading package punkt to /nltk_data...
2025-05-12 16:13:36,488 - SparkScheduler - INFO - [content_analyzer] Traceback (most recent call last):
2025-05-12 16:13:36,488 - SparkScheduler - INFO - [content_analyzer] File "/opt/spark-jobs/content_analyzer.py", line 11, in <module>
2025-05-12 16:13:36,490 - SparkScheduler - INFO - [content_analyzer] nltk.download('punkt')
2025-05-12 16:13:36,490 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 777, in download
2025-05-12 16:13:36,493 - SparkScheduler - INFO - [content_analyzer] for msg in self.incr_download(info_or_id, download_dir, force):
2025-05-12 16:13:36,493 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 642, in incr_download
2025-05-12 16:13:36,494 - SparkScheduler - INFO - [content_analyzer] yield from self._download_package(info, download_dir, force)
2025-05-12 16:13:36,494 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 699, in _download_package
2025-05-12 16:13:36,495 - SparkScheduler - INFO - [content_analyzer] os.makedirs(download_dir)
2025-05-12 16:13:36,496 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/os.py", line 225, in makedirs
2025-05-12 16:13:36,496 - SparkScheduler - INFO - [content_analyzer] mkdir(name, mode)
2025-05-12 16:13:36,497 - SparkScheduler - INFO - [content_analyzer] PermissionError: [Errno 13] Permission denied: '/nltk_data'
2025-05-12 16:13:36,768 - SparkScheduler - INFO - [content_analyzer] 25/05/12 16:13:36 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 16:13:36,770 - SparkScheduler - INFO - [content_analyzer] 25/05/12 16:13:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-cedd8401-beaa-44bd-b464-81e0e614ef55
2025-05-12 16:13:36,819 - SparkScheduler - ERROR - [content_analyzer] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 16:13:36,820 - SparkScheduler - ERROR - [content_analyzer] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 16:13:36,820 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 16:13:36,820 - SparkScheduler - ERROR - [content_analyzer] :: resolving dependencies :: org.apache.spark#spark-submit-parent-9e051b33-ede0-460f-ad27-7f2a60338c28;1.0
2025-05-12 16:13:36,821 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 16:13:36,821 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 16:13:36,822 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 16:13:36,822 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#bson;4.0.5 in central
2025-05-12 16:13:36,823 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 16:13:36,823 - SparkScheduler - ERROR - [content_analyzer] :: resolution report :: resolve 373ms :: artifacts dl 18ms
2025-05-12 16:13:36,824 - SparkScheduler - ERROR - [content_analyzer] :: modules in use:
2025-05-12 16:13:36,824 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 16:13:36,824 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 16:13:36,825 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 16:13:36,825 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 16:13:36,826 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 16:13:36,826 - SparkScheduler - ERROR - [content_analyzer] |                  |            modules            ||   artifacts   |
2025-05-12 16:13:36,827 - SparkScheduler - ERROR - [content_analyzer] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 16:13:36,827 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 16:13:36,827 - SparkScheduler - ERROR - [content_analyzer] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 16:13:36,828 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 16:13:36,828 - SparkScheduler - ERROR - [content_analyzer] :: retrieving :: org.apache.spark#spark-submit-parent-9e051b33-ede0-460f-ad27-7f2a60338c28
2025-05-12 16:13:36,829 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 16:13:36,829 - SparkScheduler - ERROR - [content_analyzer] 0 artifacts copied, 4 already retrieved (0kB/37ms)
2025-05-12 16:13:36,829 - SparkScheduler - ERROR - Job content_analyzer failed with exit code 1
2025-05-12 16:13:36,830 - SparkScheduler - INFO - Job content_analyzer duration: 9.78 seconds
2025-05-12 16:13:36,831 - SparkScheduler - INFO - Scheduler running. Press Ctrl+C to exit.
2025-05-12 16:41:56,415 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 16:41:56,417 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 16:41:58,952 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 16:41:59,784 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:41:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 16:42:01,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 16:42:01,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO ResourceUtils: ==============================================================
2025-05-12 16:42:01,240 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 16:42:01,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO ResourceUtils: ==============================================================
2025-05-12 16:42:01,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 16:42:01,260 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 16:42:01,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 16:42:01,272 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 16:42:01,373 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SecurityManager: Changing view acls to: spark
2025-05-12 16:42:01,373 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 16:42:01,374 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SecurityManager: Changing view acls groups to:
2025-05-12 16:42:01,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 16:42:01,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 16:42:01,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO Utils: Successfully started service 'sparkDriver' on port 42939.
2025-05-12 16:42:01,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 16:42:01,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 16:42:01,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 16:42:01,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 16:42:01,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 16:42:01,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4cf2be67-9c4a-42ee-8c1b-8ab3af0d937b
2025-05-12 16:42:02,014 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 16:42:02,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 16:42:02,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 16:42:02,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://5e2bb930e569:42939/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747068121213
2025-05-12 16:42:02,299 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://5e2bb930e569:42939/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747068121213
2025-05-12 16:42:02,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://5e2bb930e569:42939/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747068121213
2025-05-12 16:42:02,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://5e2bb930e569:42939/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747068121213
2025-05-12 16:42:02,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://5e2bb930e569:42939/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747068121213
2025-05-12 16:42:02,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-4a91dbec-a622-4078-ae4d-4848ed6e3679/userFiles-3ee20b81-3616-44c0-ab24-61df20d47b8e/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 16:42:02,329 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://5e2bb930e569:42939/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747068121213
2025-05-12 16:42:02,329 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-4a91dbec-a622-4078-ae4d-4848ed6e3679/userFiles-3ee20b81-3616-44c0-ab24-61df20d47b8e/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 16:42:02,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://5e2bb930e569:42939/files/org.mongodb_bson-4.0.5.jar with timestamp 1747068121213
2025-05-12 16:42:02,341 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-4a91dbec-a622-4078-ae4d-4848ed6e3679/userFiles-3ee20b81-3616-44c0-ab24-61df20d47b8e/org.mongodb_bson-4.0.5.jar
2025-05-12 16:42:02,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://5e2bb930e569:42939/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747068121213
2025-05-12 16:42:02,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-4a91dbec-a622-4078-ae4d-4848ed6e3679/userFiles-3ee20b81-3616-44c0-ab24-61df20d47b8e/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 16:42:02,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 16:42:02,477 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 21 ms (0 ms spent in bootstraps)
2025-05-12 16:42:02,573 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512164202-0002
2025-05-12 16:42:02,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512164202-0002/0 on worker-20250512161115-172.23.0.8-43101 (172.23.0.8:43101) with 2 core(s)
2025-05-12 16:42:02,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512164202-0002/0 on hostPort 172.23.0.8:43101 with 2 core(s), 1024.0 MiB RAM
2025-05-12 16:42:02,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37687.
2025-05-12 16:42:02,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO NettyBlockTransferService: Server created on 5e2bb930e569:37687
2025-05-12 16:42:02,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 16:42:02,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e2bb930e569, 37687, None)
2025-05-12 16:42:02,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO BlockManagerMasterEndpoint: Registering block manager 5e2bb930e569:37687 with 434.4 MiB RAM, BlockManagerId(driver, 5e2bb930e569, 37687, None)
2025-05-12 16:42:02,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e2bb930e569, 37687, None)
2025-05-12 16:42:02,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e2bb930e569, 37687, None)
2025-05-12 16:42:02,682 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512164202-0002/0 is now RUNNING
2025-05-12 16:42:02,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:02 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 16:42:03,214 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 16:42:03,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 16:42:03,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:03 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 16:42:07,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 16:42:07,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 16:42:07,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5e2bb930e569:37687 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:42:07,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:07 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:08,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:60106) with ID 0,  ResourceProfileId 0
2025-05-12 16:42:09,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:09 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:39769 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 39769, None)
2025-05-12 16:42:11,603 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:11 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:11,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:11 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:11,666 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:11 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:11,682 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:11 INFO connection: Opened connection [connectionId{localValue:1, serverValue:1055}] to mongodb:27017
2025-05-12 16:42:11,691 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:11 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=7242329}
2025-05-12 16:42:11,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:11 INFO connection: Opened connection [connectionId{localValue:2, serverValue:1056}] to mongodb:27017
2025-05-12 16:42:11,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:11 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:42:12,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO CodeGenerator: Code generated in 163.25932 ms
2025-05-12 16:42:12,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:12,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:12,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO connection: Opened connection [connectionId{localValue:3, serverValue:1057}] to mongodb:27017
2025-05-12 16:42:12,613 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1188781}
2025-05-12 16:42:12,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO connection: Opened connection [connectionId{localValue:4, serverValue:1058}] to mongodb:27017
2025-05-12 16:42:12,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 16:42:12,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:42:12,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 16:42:12,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:12,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:12,678 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:12,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 16:42:12,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:42:12,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5e2bb930e569:37687 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:12,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:12,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:12,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 16:42:12,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:13,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:39769 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:14,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:39769 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:42:16,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:16 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:16,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:16 INFO connection: Closed connection [connectionId{localValue:2, serverValue:1056}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:17,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4380 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:17,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 16:42:17,186 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 4.492 s
2025-05-12 16:42:17,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:17,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: running: Set()
2025-05-12 16:42:17,193 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:17,195 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:17,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:17,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO CodeGenerator: Code generated in 20.833809 ms
2025-05-12 16:42:17,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:17,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO CodeGenerator: Code generated in 31.242334 ms
2025-05-12 16:42:17,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:42:17,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:42:17,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:42:17,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 16:42:17,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:17,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:17,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:42:17,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 16:42:17,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5e2bb930e569:37687 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:17,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:17,569 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:17,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 16:42:17,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:17,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:39769 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:17,699 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:17,703 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:17 INFO connection: Closed connection [connectionId{localValue:4, serverValue:1058}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:18,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:60106
2025-05-12 16:42:18,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5e2bb930e569:37687 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:18,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:39769 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:18,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 826 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:18,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 16:42:18,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 0.863 s
2025-05-12 16:42:18,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:18,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 16:42:18,431 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 0.913495 s
2025-05-12 16:42:18,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:42:18,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 5e2bb930e569:37687 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:18,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:42:18,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5e2bb930e569:37687 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:18,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:39769 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:18,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:18,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:18,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:18,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:18,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO connection: Opened connection [connectionId{localValue:5, serverValue:1063}] to mongodb:27017
2025-05-12 16:42:18,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1294545}
2025-05-12 16:42:18,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO connection: Opened connection [connectionId{localValue:6, serverValue:1064}] to mongodb:27017
2025-05-12 16:42:18,666 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:42:18,784 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO CodeGenerator: Code generated in 61.297511 ms
2025-05-12 16:42:18,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:18,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:18,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:18,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO connection: Opened connection [connectionId{localValue:7, serverValue:1065}] to mongodb:27017
2025-05-12 16:42:18,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=890261}
2025-05-12 16:42:18,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO connection: Opened connection [connectionId{localValue:8, serverValue:1066}] to mongodb:27017
2025-05-12 16:42:18,817 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 16:42:18,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:42:18,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 16:42:18,820 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:18,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:18,822 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:18,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 16:42:18,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:18,839 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO connection: Closed connection [connectionId{localValue:8, serverValue:1066}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:18,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:18,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO connection: Closed connection [connectionId{localValue:6, serverValue:1064}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:18,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 5e2bb930e569:37687 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:18,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:42:18,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 5e2bb930e569:37687 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:18,853 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:18,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:18,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 16:42:18,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:18,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:39769 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:19,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 444 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:19,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 16:42:19,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.480 s
2025-05-12 16:42:19,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:19,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: running: Set()
2025-05-12 16:42:19,312 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:19,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:19,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:19,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:19,438 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:42:19,441 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:42:19,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:42:19,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 16:42:19,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:19,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:19,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:42:19,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 16:42:19,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 5e2bb930e569:37687 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:19,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 5e2bb930e569:37687 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:19,498 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:39769 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:19,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:19,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:19,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 16:42:19,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:19,569 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:39769 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:19,605 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:60106
2025-05-12 16:42:19,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 217 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:19,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 16:42:19,736 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.268 s
2025-05-12 16:42:19,737 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:19,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 16:42:19,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.291582 s
2025-05-12 16:42:19,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:42:19,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:42:19,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 5e2bb930e569:37687 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:19,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:19,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:39769 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:19,820 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 5e2bb930e569:37687 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:19,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:19,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:19,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:19,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO connection: Opened connection [connectionId{localValue:9, serverValue:1067}] to mongodb:27017
2025-05-12 16:42:19,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4338205}
2025-05-12 16:42:19,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO connection: Opened connection [connectionId{localValue:10, serverValue:1068}] to mongodb:27017
2025-05-12 16:42:19,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:19,911 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:19 INFO connection: Closed connection [connectionId{localValue:10, serverValue:1068}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:20,144 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:42:20,171 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:20,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO CodeGenerator: Code generated in 35.420399 ms
2025-05-12 16:42:20,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:20,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:20,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:20,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO connection: Opened connection [connectionId{localValue:11, serverValue:1069}] to mongodb:27017
2025-05-12 16:42:20,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1561821}
2025-05-12 16:42:20,259 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO connection: Opened connection [connectionId{localValue:12, serverValue:1070}] to mongodb:27017
2025-05-12 16:42:20,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:20,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO connection: Closed connection [connectionId{localValue:12, serverValue:1070}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:20,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:20,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:20,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:20,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO connection: Opened connection [connectionId{localValue:13, serverValue:1071}] to mongodb:27017
2025-05-12 16:42:20,267 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=708326}
2025-05-12 16:42:20,269 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO connection: Opened connection [connectionId{localValue:14, serverValue:1072}] to mongodb:27017
2025-05-12 16:42:20,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:20,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO connection: Closed connection [connectionId{localValue:14, serverValue:1072}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:20,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:20,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:20,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:20,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 16:42:20,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:42:20,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 16:42:20,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:20,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:20,288 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:20,291 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:42:20,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 16:42:20,299 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 5e2bb930e569:37687 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:20,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:20,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:20,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 16:42:20,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 5e2bb930e569:37687 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:20,307 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:20,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:39769 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:20,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 206 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:20,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 16:42:20,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.226 s
2025-05-12 16:42:20,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:20,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: running: Set()
2025-05-12 16:42:20,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:20,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:20,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:20,537 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO CodeGenerator: Code generated in 8.724203 ms
2025-05-12 16:42:20,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:20,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO CodeGenerator: Code generated in 22.630655 ms
2025-05-12 16:42:20,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:42:20,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:42:20,615 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:42:20,616 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 16:42:20,619 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:20,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:20,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.3 MiB)
2025-05-12 16:42:20,650 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)
2025-05-12 16:42:20,666 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 5e2bb930e569:37687 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:42:20,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 5e2bb930e569:37687 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:20,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:20,678 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:20,681 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 16:42:20,682 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:20,683 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:39769 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:20,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:39769 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:42:20,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:60106
2025-05-12 16:42:21,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 489 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:21,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 16:42:21,153 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.526 s
2025-05-12 16:42:21,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:21,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 16:42:21,156 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.543153 s
2025-05-12 16:42:21,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:42:21,177 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:42:21,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 5e2bb930e569:37687 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:21,203 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:21,207 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 5e2bb930e569:37687 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:42:21,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:39769 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 16:42:21,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:42:21,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:21,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:21,467 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:21,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO connection: Opened connection [connectionId{localValue:16, serverValue:1075}] to mongodb:27017
2025-05-12 16:42:21,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1736923}
2025-05-12 16:42:21,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO connection: Opened connection [connectionId{localValue:17, serverValue:1076}] to mongodb:27017
2025-05-12 16:42:21,477 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:21,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO connection: Closed connection [connectionId{localValue:17, serverValue:1076}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:21,479 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:21,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:21,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:21,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO connection: Opened connection [connectionId{localValue:18, serverValue:1077}] to mongodb:27017
2025-05-12 16:42:21,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1350297}
2025-05-12 16:42:21,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO connection: Opened connection [connectionId{localValue:19, serverValue:1078}] to mongodb:27017
2025-05-12 16:42:21,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:21,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO connection: Closed connection [connectionId{localValue:19, serverValue:1078}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:21,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:21,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:21,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:21,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 16:42:21,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:21,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:21,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:21,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:21,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:21,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.4 MiB)
2025-05-12 16:42:21,536 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:42:21,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 5e2bb930e569:37687 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:21,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 5e2bb930e569:37687 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:21,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:21,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:21,550 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 16:42:21,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:21,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:39769 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:21,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 174 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:21,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 16:42:21,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.209 s
2025-05-12 16:42:21,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:21,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: running: Set()
2025-05-12 16:42:21,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:21,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:21,754 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:21,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO CodeGenerator: Code generated in 23.126589 ms
2025-05-12 16:42:21,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO CodeGenerator: Code generated in 9.461153 ms
2025-05-12 16:42:21,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:21,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO CodeGenerator: Code generated in 16.487552 ms
2025-05-12 16:42:21,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:42:21,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:21,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:21,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 16:42:21,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:21,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:21,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 16:42:21,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 16:42:21,927 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 5e2bb930e569:37687 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:21,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 5e2bb930e569:37687 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:21,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:21,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:39769 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:21,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:21,935 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 16:42:21,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:21 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:22,020 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:39769 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:22,170 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:60106
2025-05-12 16:42:22,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 485 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:22,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 16:42:22,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.526 s
2025-05-12 16:42:22,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:22,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 16:42:22,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.537653 s
2025-05-12 16:42:22,649 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:42:22,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:22,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:22,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:22,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO connection: Opened connection [connectionId{localValue:21, serverValue:1080}] to mongodb:27017
2025-05-12 16:42:22,735 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1236142}
2025-05-12 16:42:22,738 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO connection: Opened connection [connectionId{localValue:22, serverValue:1081}] to mongodb:27017
2025-05-12 16:42:22,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:22,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO connection: Closed connection [connectionId{localValue:22, serverValue:1081}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:22,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:22,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:22,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:22,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO connection: Opened connection [connectionId{localValue:23, serverValue:1082}] to mongodb:27017
2025-05-12 16:42:22,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1393670}
2025-05-12 16:42:22,753 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO connection: Opened connection [connectionId{localValue:24, serverValue:1083}] to mongodb:27017
2025-05-12 16:42:22,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:22,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO connection: Closed connection [connectionId{localValue:24, serverValue:1083}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:22,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:22,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:22,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:22,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 16:42:22,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:22,775 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:22,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:22,777 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:22,780 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:22,782 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.3 MiB)
2025-05-12 16:42:22,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:42:22,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 5e2bb930e569:37687 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:22,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:22,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:22,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 16:42:22,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:22,812 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 5e2bb930e569:37687 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:22,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:39769 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:22,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:39769 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:22,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 128 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:22,940 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 16:42:22,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.161 s
2025-05-12 16:42:22,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:22,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: running: Set()
2025-05-12 16:42:22,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:22,944 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:22,949 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:22,972 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:23,001 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:42:23,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:23,005 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:23,005 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 16:42:23,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:23,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:23,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 16:42:23,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 16:42:23,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 5e2bb930e569:37687 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 5e2bb930e569:37687 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:23,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:23,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 16:42:23,040 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:23,041 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:39769 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:39769 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:60106
2025-05-12 16:42:23,134 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 98 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:23,135 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 16:42:23,137 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.127 s
2025-05-12 16:42:23,138 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:23,139 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 16:42:23,140 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.136613 s
2025-05-12 16:42:23,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 16:42:23,290 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:23,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:23,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:23,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:23,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Opened connection [connectionId{localValue:26, serverValue:1085}] to mongodb:27017
2025-05-12 16:42:23,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1429858}
2025-05-12 16:42:23,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Opened connection [connectionId{localValue:27, serverValue:1086}] to mongodb:27017
2025-05-12 16:42:23,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:23,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Closed connection [connectionId{localValue:27, serverValue:1086}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:23,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:23,345 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:23,351 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:23,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Opened connection [connectionId{localValue:28, serverValue:1087}] to mongodb:27017
2025-05-12 16:42:23,354 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1555817}
2025-05-12 16:42:23,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Opened connection [connectionId{localValue:29, serverValue:1088}] to mongodb:27017
2025-05-12 16:42:23,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:23,380 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Closed connection [connectionId{localValue:29, serverValue:1088}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:23,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:23,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:23,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:23,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 16:42:23,386 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:23,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:23,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:23,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:23,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:23,395 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 16:42:23,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 16:42:23,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 5e2bb930e569:37687 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:23,412 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:23,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 16:42:23,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 5e2bb930e569:37687 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:23,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:39769 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,461 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:39769 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 102 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:23,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 16:42:23,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.124 s
2025-05-12 16:42:23,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:23,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: running: Set()
2025-05-12 16:42:23,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:23,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:23,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:23,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:23,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO CodeGenerator: Code generated in 22.137104 ms
2025-05-12 16:42:23,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:42:23,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:23,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:23,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 16:42:23,678 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:23,679 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:23,684 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.3 MiB)
2025-05-12 16:42:23,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.2 MiB)
2025-05-12 16:42:23,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 5e2bb930e569:37687 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 5e2bb930e569:37687 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:23,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:23,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 16:42:23,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:23,703 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:39769 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,738 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:39769 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,761 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:60106
2025-05-12 16:42:23,822 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 119 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:23,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 16:42:23,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.141 s
2025-05-12 16:42:23,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:23,839 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 16:42:23,842 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.150506 s
2025-05-12 16:42:23,843 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 4, 'weekly': 4, 'hourly': 8}
2025-05-12 16:42:23,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 16:42:23,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 16:42:23,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 5e2bb930e569:37687 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:42:23,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:23,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 5e2bb930e569:37687 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:39769 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 16:42:23,940 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:23,954 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:23,956 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:23,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Opened connection [connectionId{localValue:31, serverValue:1090}] to mongodb:27017
2025-05-12 16:42:23,993 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1328793}
2025-05-12 16:42:23,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Opened connection [connectionId{localValue:32, serverValue:1091}] to mongodb:27017
2025-05-12 16:42:24,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:24,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:23 INFO connection: Closed connection [connectionId{localValue:32, serverValue:1091}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:24,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:24,070 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:24,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:24,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:42:24,185 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:42:24,186 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:42:24,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:24,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:24,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:42:24,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 16:42:24,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 16:42:24,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 5e2bb930e569:37687 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:24,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:24,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:24,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 16:42:24,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:42:24,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:39769 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:24,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:39769 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:42:24,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 336 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:24,562 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 16:42:24,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.374 s
2025-05-12 16:42:24,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:24,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 16:42:24,569 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.386496 s
2025-05-12 16:42:24,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:24,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:24,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:24,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Opened connection [connectionId{localValue:34, serverValue:1093}] to mongodb:27017
2025-05-12 16:42:24,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=942404}
2025-05-12 16:42:24,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Opened connection [connectionId{localValue:35, serverValue:1094}] to mongodb:27017
2025-05-12 16:42:24,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:24,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Closed connection [connectionId{localValue:35, serverValue:1094}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:24,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 16:42:24,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO CodeGenerator: Code generated in 39.133811 ms
2025-05-12 16:42:24,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:24,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:24,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:24,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Opened connection [connectionId{localValue:36, serverValue:1095}] to mongodb:27017
2025-05-12 16:42:24,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=757750}
2025-05-12 16:42:24,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Opened connection [connectionId{localValue:37, serverValue:1096}] to mongodb:27017
2025-05-12 16:42:24,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:24,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Closed connection [connectionId{localValue:37, serverValue:1096}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:24,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:24,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:24,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:24,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Opened connection [connectionId{localValue:38, serverValue:1097}] to mongodb:27017
2025-05-12 16:42:24,816 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=936792}
2025-05-12 16:42:24,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Opened connection [connectionId{localValue:39, serverValue:1098}] to mongodb:27017
2025-05-12 16:42:24,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:24,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO connection: Closed connection [connectionId{localValue:39, serverValue:1098}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:24,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:24,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:24,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:24,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 16:42:24,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:42:24,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 16:42:24,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:24,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:24,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:24,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.5 KiB, free 434.3 MiB)
2025-05-12 16:42:24,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.3 MiB)
2025-05-12 16:42:24,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 5e2bb930e569:37687 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:24,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 5e2bb930e569:37687 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:24,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:24,850 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:24,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 16:42:24,854 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:24,855 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:39769 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:24,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:24 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:39769 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:25,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 218 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:25,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 16:42:25,075 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.237 s
2025-05-12 16:42:25,082 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:25,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: running: Set()
2025-05-12 16:42:25,084 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:25,085 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:25,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:25,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:25,176 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO CodeGenerator: Code generated in 54.850263 ms
2025-05-12 16:42:25,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:42:25,200 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Got job 14 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:42:25,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Final stage: ResultStage 21 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:42:25,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 16:42:25,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:25,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:25,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 52.5 KiB, free 434.3 MiB)
2025-05-12 16:42:25,250 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 434.3 MiB)
2025-05-12 16:42:25,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 5e2bb930e569:37687 (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:25,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:25,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 5e2bb930e569:37687 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:25,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[85] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:25,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 16:42:25,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:25,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:39769 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:25,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:39769 (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:25,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:60106
2025-05-12 16:42:25,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 253 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:25,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 16:42:25,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: ResultStage 21 (foreachPartition at MongoSpark.scala:120) finished in 0.313 s
2025-05-12 16:42:25,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:25,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 16:42:25,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Job 14 finished: foreachPartition at MongoSpark.scala:120, took 0.321176 s
2025-05-12 16:42:25,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:42:25,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:42:25,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 5e2bb930e569:37687 in memory (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:25,556 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 5e2bb930e569:37687 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:25,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO SparkContext: Created broadcast 20 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:25,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:39769 in memory (size: 23.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:25,604 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:25,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:25,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:25,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Opened connection [connectionId{localValue:41, serverValue:1100}] to mongodb:27017
2025-05-12 16:42:25,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1426104}
2025-05-12 16:42:25,626 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Opened connection [connectionId{localValue:42, serverValue:1101}] to mongodb:27017
2025-05-12 16:42:25,628 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:25,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Closed connection [connectionId{localValue:42, serverValue:1101}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:25,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 16:42:25,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO CodeGenerator: Code generated in 33.879878 ms
2025-05-12 16:42:25,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:25,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:25,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:25,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Opened connection [connectionId{localValue:43, serverValue:1102}] to mongodb:27017
2025-05-12 16:42:25,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1243061}
2025-05-12 16:42:25,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Opened connection [connectionId{localValue:44, serverValue:1103}] to mongodb:27017
2025-05-12 16:42:25,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:25,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Closed connection [connectionId{localValue:44, serverValue:1103}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:25,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:25,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:25,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:25,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Opened connection [connectionId{localValue:45, serverValue:1104}] to mongodb:27017
2025-05-12 16:42:25,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=911048}
2025-05-12 16:42:25,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Opened connection [connectionId{localValue:46, serverValue:1105}] to mongodb:27017
2025-05-12 16:42:25,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:25,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO connection: Closed connection [connectionId{localValue:46, serverValue:1105}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:25,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:25,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:25,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:25,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Registering RDD 91 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 16:42:25,750 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 16:42:25,750 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (rdd at MongoSpark.scala:169)
2025-05-12 16:42:25,751 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:25,752 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:25,753 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:25,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 39.6 KiB, free 434.4 MiB)
2025-05-12 16:42:25,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 16:42:25,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 5e2bb930e569:37687 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:25,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 5e2bb930e569:37687 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:25,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:25,775 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[91] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:25,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
2025-05-12 16:42:25,781 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 15) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:25,820 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:25 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:39769 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 15) in 483 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:26,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
2025-05-12 16:42:26,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: ShuffleMapStage 22 (rdd at MongoSpark.scala:169) finished in 0.512 s
2025-05-12 16:42:26,274 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:26,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: running: Set()
2025-05-12 16:42:26,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:26,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:26,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:26,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:26,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 16:42:26,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 16:42:26,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Final stage: ResultStage 24 (foreachPartition at MongoSpark.scala:120)
2025-05-12 16:42:26,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
2025-05-12 16:42:26,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:26,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 16:42:26,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 48.4 KiB, free 434.3 MiB)
2025-05-12 16:42:26,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 16:42:26,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 5e2bb930e569:37687 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 5e2bb930e569:37687 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:26,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[97] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:26,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
2025-05-12 16:42:26,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:26,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:39769 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.23.0.8:39769 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:60106
2025-05-12 16:42:26,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 16) in 137 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:26,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
2025-05-12 16:42:26,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: ResultStage 24 (foreachPartition at MongoSpark.scala:120) finished in 0.189 s
2025-05-12 16:42:26,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:26,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
2025-05-12 16:42:26,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.198363 s
2025-05-12 16:42:26,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 16:42:26,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 16:42:26,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 5e2bb930e569:37687 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 5e2bb930e569:37687 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:26,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO SparkContext: Created broadcast 23 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:26,595 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.23.0.8:39769 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 16:42:26,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:26,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO CodeGenerator: Code generated in 26.143374 ms
2025-05-12 16:42:26,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:26,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:26,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:26,715 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO connection: Opened connection [connectionId{localValue:48, serverValue:1107}] to mongodb:27017
2025-05-12 16:42:26,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2185081}
2025-05-12 16:42:26,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO connection: Opened connection [connectionId{localValue:49, serverValue:1108}] to mongodb:27017
2025-05-12 16:42:26,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:26,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO connection: Closed connection [connectionId{localValue:49, serverValue:1108}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:26,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:26,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:26,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:26,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO connection: Opened connection [connectionId{localValue:50, serverValue:1109}] to mongodb:27017
2025-05-12 16:42:26,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=861445}
2025-05-12 16:42:26,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO connection: Opened connection [connectionId{localValue:51, serverValue:1110}] to mongodb:27017
2025-05-12 16:42:26,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:26,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO connection: Closed connection [connectionId{localValue:51, serverValue:1110}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:26,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:26,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:26,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:26,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Registering RDD 103 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 8
2025-05-12 16:42:26,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Got map stage job 17 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:26,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:26,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:26,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:26,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:26,752 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 36.4 KiB, free 434.4 MiB)
2025-05-12 16:42:26,760 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 434.3 MiB)
2025-05-12 16:42:26,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 5e2bb930e569:37687 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 5e2bb930e569:37687 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 16:42:26,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:26,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:26,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
2025-05-12 16:42:26,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:26,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:39769 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 16:42:26,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 114 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:26,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
2025-05-12 16:42:26,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.136 s
2025-05-12 16:42:26,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:26,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: running: Set()
2025-05-12 16:42:26,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:26,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:26,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:26,958 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:26 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:27,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO CodeGenerator: Code generated in 41.613774 ms
2025-05-12 16:42:27,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Registering RDD 106 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 16:42:27,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Got map stage job 18 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:27,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:27,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
2025-05-12 16:42:27,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:27,040 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:27,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 45.0 KiB, free 434.3 MiB)
2025-05-12 16:42:27,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.3 MiB)
2025-05-12 16:42:27,075 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 5e2bb930e569:37687 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 5e2bb930e569:37687 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,084 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:27,085 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:27,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 16:42:27,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 16:42:27,095 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:39769 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,104 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.23.0.8:39769 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,139 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:60106
2025-05-12 16:42:27,207 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 18) in 137 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:27,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 16:42:27,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: ShuffleMapStage 27 (count at NativeMethodAccessorImpl.java:0) finished in 0.169 s
2025-05-12 16:42:27,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:27,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: running: Set()
2025-05-12 16:42:27,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:27,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:27,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO CodeGenerator: Code generated in 19.251712 ms
2025-05-12 16:42:27,290 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:42:27,292 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Got job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:27,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Final stage: ResultStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:27,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
2025-05-12 16:42:27,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:27,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:27,299 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 434.3 MiB)
2025-05-12 16:42:27,317 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 5e2bb930e569:37687 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.4 MiB)
2025-05-12 16:42:27,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 5e2bb930e569:37687 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,321 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:27,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:27,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.23.0.8:39769 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 16:42:27,332 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:27,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:39769 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:60106
2025-05-12 16:42:27,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 85 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:27,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 16:42:27,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: ResultStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.122 s
2025-05-12 16:42:27,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:27,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
2025-05-12 16:42:27,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Job 19 finished: count at NativeMethodAccessorImpl.java:0, took 0.131693 s
2025-05-12 16:42:27,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 16:42:27,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:27,577 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:27,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:27,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO connection: Opened connection [connectionId{localValue:53, serverValue:1112}] to mongodb:27017
2025-05-12 16:42:27,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1814097}
2025-05-12 16:42:27,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO connection: Opened connection [connectionId{localValue:54, serverValue:1113}] to mongodb:27017
2025-05-12 16:42:27,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:27,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO connection: Closed connection [connectionId{localValue:54, serverValue:1113}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:27,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:27,600 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:27,603 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:27,606 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO connection: Opened connection [connectionId{localValue:55, serverValue:1114}] to mongodb:27017
2025-05-12 16:42:27,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1335900}
2025-05-12 16:42:27,616 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO connection: Opened connection [connectionId{localValue:56, serverValue:1115}] to mongodb:27017
2025-05-12 16:42:27,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:27,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO connection: Closed connection [connectionId{localValue:56, serverValue:1115}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:27,626 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:27,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:27,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:27,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Registering RDD 114 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 16:42:27,647 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:27,656 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:27,659 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:27,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:27,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:27,685 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 39.7 KiB, free 434.3 MiB)
2025-05-12 16:42:27,686 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 16:42:27,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 5e2bb930e569:37687 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,701 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:27,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:27,703 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
2025-05-12 16:42:27,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 20) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 16:42:27,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.23.0.8:39769 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 5e2bb930e569:37687 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:39769 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:27,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 20) in 239 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:27,926 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
2025-05-12 16:42:27,927 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0) finished in 0.277 s
2025-05-12 16:42:27,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 16:42:27,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: running: Set()
2025-05-12 16:42:27,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: waiting: Set()
2025-05-12 16:42:27,938 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO DAGScheduler: failed: Set()
2025-05-12 16:42:27,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 16:42:27,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:27 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 16:42:28,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 16:42:28,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 16:42:28,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Final stage: ResultStage 33 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 16:42:28,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
2025-05-12 16:42:28,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:28,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 16:42:28,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 46.3 KiB, free 434.3 MiB)
2025-05-12 16:42:28,081 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.3 MiB)
2025-05-12 16:42:28,084 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 5e2bb930e569:37687 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 5e2bb930e569:37687 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:28,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[119] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:28,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
2025-05-12 16:42:28,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.23.0.8:39769 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,093 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 16:42:28,135 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:39769 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,171 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:60106
2025-05-12 16:42:28,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 21) in 119 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:28,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
2025-05-12 16:42:28,214 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: ResultStage 33 (count at NativeMethodAccessorImpl.java:0) finished in 0.155 s
2025-05-12 16:42:28,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:28,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
2025-05-12 16:42:28,219 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.168307 s
2025-05-12 16:42:28,220 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 1, 'active_users': 1}
2025-05-12 16:42:28,240 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 16:42:28,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 16:42:28,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 5e2bb930e569:37687 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:42:28,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 5e2bb930e569:37687 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,259 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Created broadcast 29 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:28,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:28,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:28,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:28,267 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.23.0.8:39769 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO connection: Opened connection [connectionId{localValue:57, serverValue:1116}] to mongodb:27017
2025-05-12 16:42:28,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6012802}
2025-05-12 16:42:28,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO connection: Opened connection [connectionId{localValue:58, serverValue:1117}] to mongodb:27017
2025-05-12 16:42:28,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:28,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO connection: Closed connection [connectionId{localValue:58, serverValue:1117}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:28,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:28,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:28,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:28,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:42:28,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Got job 22 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:42:28,330 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Final stage: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:42:28,332 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:28,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:28,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:42:28,339 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 16:42:28,341 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 16:42:28,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 5e2bb930e569:37687 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,343 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:28,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[124] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:28,345 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
2025-05-12 16:42:28,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:42:28,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:39769 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:39769 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:42:28,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 22) in 104 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:28,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
2025-05-12 16:42:28,451 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: ResultStage 34 (treeAggregate at MongoInferSchema.scala:88) finished in 0.122 s
2025-05-12 16:42:28,452 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:28,455 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
2025-05-12 16:42:28,458 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Job 22 finished: treeAggregate at MongoInferSchema.scala:88, took 0.129082 s
2025-05-12 16:42:28,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 16:42:28,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 16:42:28,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 5e2bb930e569:37687 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:42:28,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 5e2bb930e569:37687 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 16:42:28,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:28,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:28,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 16:42:28,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.23.0.8:39769 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO connection: Opened connection [connectionId{localValue:60, serverValue:1119}] to mongodb:27017
2025-05-12 16:42:28,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1639687}
2025-05-12 16:42:28,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO connection: Opened connection [connectionId{localValue:61, serverValue:1120}] to mongodb:27017
2025-05-12 16:42:28,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:28,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO connection: Closed connection [connectionId{localValue:61, serverValue:1120}] to mongodb:27017 because the pool has been closed.
2025-05-12 16:42:28,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 16:42:28,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 16:42:28,556 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 16:42:28,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 16:42:28,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Got job 23 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 16:42:28,587 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Final stage: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 16:42:28,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 16:42:28,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Missing parents: List()
2025-05-12 16:42:28,595 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 16:42:28,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 16:42:28,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 16:42:28,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 5e2bb930e569:37687 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 16:42:28,600 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[129] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 16:42:28,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 16:42:28,602 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 23) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 16:42:28,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:39769 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 16:42:28,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:39769 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 16:42:28,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 23) in 66 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 16:42:28,669 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 16:42:28,670 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: ResultStage 35 (treeAggregate at MongoInferSchema.scala:88) finished in 0.080 s
2025-05-12 16:42:28,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 16:42:28,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 16:42:28,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO DAGScheduler: Job 23 finished: treeAggregate at MongoInferSchema.scala:88, took 0.085309 s
2025-05-12 16:42:28,737 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkUI: Stopped Spark web UI at http://5e2bb930e569:4040
2025-05-12 16:42:28,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 16:42:28,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 16:42:28,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 16:42:28,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO MemoryStore: MemoryStore cleared
2025-05-12 16:42:28,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManager: BlockManager stopped
2025-05-12 16:42:28,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 16:42:28,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 16:42:28,939 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:28 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 16:42:29,316 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 16:42:29,316 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 16:42:29,323 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 16:42:29,324 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 16:42:29,326 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 16:42:29,327 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 16:42:29,329 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 16:42:29,330 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 16:42:29,330 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 16:42:29,331 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 16:42:29,337 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 16:42:29,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:29 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 16:42:29,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-4a91dbec-a622-4078-ae4d-4848ed6e3679
2025-05-12 16:42:29,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-1c324578-1748-439a-97f0-f6421a65005a
2025-05-12 16:42:29,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 16:42:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-4a91dbec-a622-4078-ae4d-4848ed6e3679/pyspark-6b7a38a7-ab4b-475f-b653-e3f5765fa51d
2025-05-12 16:42:29,475 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 16:42:29,476 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 16:42:29,476 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 16:42:29,476 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-f18f8cb5-aff3-460f-98fb-3d59c9e9543b;1.0
2025-05-12 16:42:29,477 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 16:42:29,477 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 16:42:29,477 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 16:42:29,477 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 16:42:29,478 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 16:42:29,478 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 259ms :: artifacts dl 11ms
2025-05-12 16:42:29,478 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 16:42:29,479 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 16:42:29,479 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 16:42:29,479 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 16:42:29,480 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 16:42:29,480 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:42:29,480 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 16:42:29,481 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 16:42:29,481 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:42:29,481 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 16:42:29,482 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 16:42:29,482 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-f18f8cb5-aff3-460f-98fb-3d59c9e9543b
2025-05-12 16:42:29,482 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 16:42:29,483 - SparkScheduler - ERROR - [trend_analysis] 0 artifacts copied, 4 already retrieved (0kB/9ms)
2025-05-12 16:42:29,484 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 16:42:29,484 - SparkScheduler - INFO - Job trend_analysis duration: 33.07 seconds
2025-05-12 17:13:20,139 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 17:13:20,140 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 17:13:22,864 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 17:13:23,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 17:13:25,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 17:13:25,165 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO ResourceUtils: ==============================================================
2025-05-12 17:13:25,166 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 17:13:25,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO ResourceUtils: ==============================================================
2025-05-12 17:13:25,168 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 17:13:25,026 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 17:13:25,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 17:13:25,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 17:13:25,131 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO SecurityManager: Changing view acls to: spark
2025-05-12 17:13:25,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 17:13:25,134 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO SecurityManager: Changing view acls groups to:
2025-05-12 17:13:25,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 17:13:25,141 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 17:13:29,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO Utils: Successfully started service 'sparkDriver' on port 40165.
2025-05-12 17:13:29,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 17:13:29,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 17:13:29,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 17:13:29,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 17:13:29,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 17:13:29,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d422d70e-61df-4b54-b057-751f0b9755fb
2025-05-12 17:13:29,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 17:13:29,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 17:13:29,760 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 17:13:29,817 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://5e2bb930e569:40165/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747070005101
2025-05-12 17:13:29,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://5e2bb930e569:40165/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747070005101
2025-05-12 17:13:29,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://5e2bb930e569:40165/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747070005101
2025-05-12 17:13:29,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://5e2bb930e569:40165/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747070005101
2025-05-12 17:13:29,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://5e2bb930e569:40165/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747070005101
2025-05-12 17:13:29,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-2ff147fe-e5c7-4412-bb35-c84401925ce9/userFiles-3df0398f-7883-464b-aed2-c22091653df0/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 17:13:29,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://5e2bb930e569:40165/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747070005101
2025-05-12 17:13:29,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-2ff147fe-e5c7-4412-bb35-c84401925ce9/userFiles-3df0398f-7883-464b-aed2-c22091653df0/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 17:13:29,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://5e2bb930e569:40165/files/org.mongodb_bson-4.0.5.jar with timestamp 1747070005101
2025-05-12 17:13:29,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-2ff147fe-e5c7-4412-bb35-c84401925ce9/userFiles-3df0398f-7883-464b-aed2-c22091653df0/org.mongodb_bson-4.0.5.jar
2025-05-12 17:13:29,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://5e2bb930e569:40165/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747070005101
2025-05-12 17:13:29,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-2ff147fe-e5c7-4412-bb35-c84401925ce9/userFiles-3df0398f-7883-464b-aed2-c22091653df0/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 17:13:29,987 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:29 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 17:13:30,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 23 ms (0 ms spent in bootstraps)
2025-05-12 17:13:30,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512171330-0003
2025-05-12 17:13:30,156 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512171330-0003/0 on worker-20250512161115-172.23.0.8-43101 (172.23.0.8:43101) with 2 core(s)
2025-05-12 17:13:30,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512171330-0003/0 on hostPort 172.23.0.8:43101 with 2 core(s), 1024.0 MiB RAM
2025-05-12 17:13:30,163 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35153.
2025-05-12 17:13:30,164 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO NettyBlockTransferService: Server created on 5e2bb930e569:35153
2025-05-12 17:13:30,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 17:13:30,186 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e2bb930e569, 35153, None)
2025-05-12 17:13:30,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO BlockManagerMasterEndpoint: Registering block manager 5e2bb930e569:35153 with 434.4 MiB RAM, BlockManagerId(driver, 5e2bb930e569, 35153, None)
2025-05-12 17:13:30,231 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e2bb930e569, 35153, None)
2025-05-12 17:13:30,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e2bb930e569, 35153, None)
2025-05-12 17:13:30,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512171330-0003/0 is now RUNNING
2025-05-12 17:13:30,646 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 17:13:30,932 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 17:13:30,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 17:13:30,949 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:30 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 17:13:32,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 17:13:32,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 17:13:32,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5e2bb930e569:35153 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:13:32,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:32 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:33,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:35492) with ID 0,  ResourceProfileId 0
2025-05-12 17:13:34,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:43197 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 43197, None)
2025-05-12 17:13:36,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:36,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:36,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:36,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:36 INFO connection: Opened connection [connectionId{localValue:1, serverValue:1626}] to mongodb:27017
2025-05-12 17:13:36,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6832724}
2025-05-12 17:13:36,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:36 INFO connection: Opened connection [connectionId{localValue:2, serverValue:1627}] to mongodb:27017
2025-05-12 17:13:37,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:37 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:13:39,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO CodeGenerator: Code generated in 402.890732 ms
2025-05-12 17:13:39,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:39,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:39,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO connection: Opened connection [connectionId{localValue:3, serverValue:1628}] to mongodb:27017
2025-05-12 17:13:39,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1723809}
2025-05-12 17:13:39,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO connection: Opened connection [connectionId{localValue:4, serverValue:1629}] to mongodb:27017
2025-05-12 17:13:39,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 17:13:39,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:13:39,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 17:13:39,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:39,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:39,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:39,473 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 17:13:39,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 17:13:39,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5e2bb930e569:35153 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:39,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:39,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:39,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 17:13:39,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:39,961 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:43197 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:40,650 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:43197 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:13:41,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:41 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:41,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:41 INFO connection: Closed connection [connectionId{localValue:2, serverValue:1627}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:44,392 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:44,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO connection: Closed connection [connectionId{localValue:4, serverValue:1629}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:44,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4921 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:44,479 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 17:13:44,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 5.072 s
2025-05-12 17:13:44,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:44,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO DAGScheduler: running: Set()
2025-05-12 17:13:44,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:44,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:44,615 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:44,737 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO CodeGenerator: Code generated in 68.5637 ms
2025-05-12 17:13:44,755 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:44,958 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:44 INFO CodeGenerator: Code generated in 148.161082 ms
2025-05-12 17:13:45,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:13:45,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:13:45,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:13:45,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 17:13:45,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:45,470 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:45,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 17:13:45,536 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 17:13:45,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5e2bb930e569:35153 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:13:45,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:45,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:45,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 17:13:45,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:45,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:43197 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:13:46,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:35492
2025-05-12 17:13:47,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 2174 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:47,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 17:13:47,737 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 2.242 s
2025-05-12 17:13:47,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:47,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 17:13:47,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 2.332705 s
2025-05-12 17:13:47,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:13:47,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:13:47,777 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5e2bb930e569:35153 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:13:47,782 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:47,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:47,979 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO connection: Opened connection [connectionId{localValue:5, serverValue:1635}] to mongodb:27017
2025-05-12 17:13:47,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:47,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:47,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3812025}
2025-05-12 17:13:47,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:47 INFO connection: Opened connection [connectionId{localValue:6, serverValue:1636}] to mongodb:27017
2025-05-12 17:13:48,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:13:48,496 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO CodeGenerator: Code generated in 153.456069 ms
2025-05-12 17:13:48,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:48,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:48,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:48,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO connection: Opened connection [connectionId{localValue:7, serverValue:1637}] to mongodb:27017
2025-05-12 17:13:48,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1707577}
2025-05-12 17:13:48,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO connection: Opened connection [connectionId{localValue:8, serverValue:1638}] to mongodb:27017
2025-05-12 17:13:48,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 17:13:48,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:13:48,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 17:13:48,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:48,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:48,550 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:48,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.2 MiB)
2025-05-12 17:13:48,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 17:13:48,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 5e2bb930e569:35153 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:13:48,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:48,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:48,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 17:13:48,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:48,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:43197 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:13:48,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 307 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:48,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 17:13:48,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.335 s
2025-05-12 17:13:48,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:48,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: running: Set()
2025-05-12 17:13:48,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:48,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:48,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:48,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:48 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:49,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:13:49,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:13:49,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:13:49,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 17:13:49,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:49,040 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:49,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.1 MiB)
2025-05-12 17:13:49,101 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.1 MiB)
2025-05-12 17:13:49,105 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 5e2bb930e569:35153 (size: 24.4 KiB, free: 434.3 MiB)
2025-05-12 17:13:49,107 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:49,109 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:49,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 17:13:49,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:49,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:49,171 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Closed connection [connectionId{localValue:6, serverValue:1636}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:49,179 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:49,180 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:43197 (size: 24.4 KiB, free: 434.3 MiB)
2025-05-12 17:13:49,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Closed connection [connectionId{localValue:8, serverValue:1638}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:49,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 5e2bb930e569:35153 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:13:49,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:35492
2025-05-12 17:13:49,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 5e2bb930e569:35153 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:13:49,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:43197 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:13:49,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5e2bb930e569:35153 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:49,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:43197 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:49,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 5e2bb930e569:35153 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:13:49,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:43197 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:13:49,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 251 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:49,366 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 17:13:49,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.316 s
2025-05-12 17:13:49,369 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:49,370 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 17:13:49,371 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.341497 s
2025-05-12 17:13:49,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:13:49,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:13:49,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 5e2bb930e569:35153 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:13:49,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:49,460 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:49,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:49,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:49,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Opened connection [connectionId{localValue:9, serverValue:1639}] to mongodb:27017
2025-05-12 17:13:49,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1775862}
2025-05-12 17:13:49,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Opened connection [connectionId{localValue:10, serverValue:1640}] to mongodb:27017
2025-05-12 17:13:49,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:49,476 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Closed connection [connectionId{localValue:10, serverValue:1640}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:49,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:13:49,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:49,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO CodeGenerator: Code generated in 56.024775 ms
2025-05-12 17:13:49,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:49,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:49,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:49,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Opened connection [connectionId{localValue:11, serverValue:1641}] to mongodb:27017
2025-05-12 17:13:49,830 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1445311}
2025-05-12 17:13:49,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Opened connection [connectionId{localValue:12, serverValue:1642}] to mongodb:27017
2025-05-12 17:13:49,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:49,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Closed connection [connectionId{localValue:12, serverValue:1642}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:49,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:49,840 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:49,842 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:49,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Opened connection [connectionId{localValue:13, serverValue:1643}] to mongodb:27017
2025-05-12 17:13:49,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1084394}
2025-05-12 17:13:49,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Opened connection [connectionId{localValue:14, serverValue:1644}] to mongodb:27017
2025-05-12 17:13:49,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:49,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Closed connection [connectionId{localValue:14, serverValue:1644}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:49,865 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:49,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:49,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:49,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO connection: Opened connection [connectionId{localValue:15, serverValue:1645}] to mongodb:27017
2025-05-12 17:13:49,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 17:13:49,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:13:49,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 17:13:49,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:49,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:49,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:49,887 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 17:13:49,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 17:13:49,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 5e2bb930e569:35153 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:49,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:49,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:49,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 17:13:49,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:49,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:43197 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:50,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 226 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:50,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 17:13:50,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.244 s
2025-05-12 17:13:50,129 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:50,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: running: Set()
2025-05-12 17:13:50,137 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:50,138 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:50,140 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:50,157 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO CodeGenerator: Code generated in 8.635515 ms
2025-05-12 17:13:50,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:50,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO CodeGenerator: Code generated in 31.476538 ms
2025-05-12 17:13:50,240 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:13:50,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:13:50,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:13:50,245 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 17:13:50,246 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:50,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:50,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.2 MiB)
2025-05-12 17:13:50,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)
2025-05-12 17:13:50,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 5e2bb930e569:35153 (size: 27.2 KiB, free: 434.3 MiB)
2025-05-12 17:13:50,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:50,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:50,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 17:13:50,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:50,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:43197 (size: 27.2 KiB, free: 434.3 MiB)
2025-05-12 17:13:50,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:35492
2025-05-12 17:13:50,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 286 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:50,573 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 17:13:50,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.317 s
2025-05-12 17:13:50,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:50,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 17:13:50,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.337031 s
2025-05-12 17:13:50,587 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.2 MiB)
2025-05-12 17:13:50,604 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.2 MiB)
2025-05-12 17:13:50,606 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 5e2bb930e569:35153 (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:13:50,607 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:50,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 5e2bb930e569:35153 in memory (size: 24.4 KiB, free: 434.3 MiB)
2025-05-12 17:13:50,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:43197 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:13:50,659 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 5e2bb930e569:35153 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:13:50,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 5e2bb930e569:35153 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:50,684 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:43197 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:50,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 5e2bb930e569:35153 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 17:13:50,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:43197 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 17:13:50,739 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:13:50,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:50,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:50,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:50,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO connection: Opened connection [connectionId{localValue:16, serverValue:1646}] to mongodb:27017
2025-05-12 17:13:50,865 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1315423}
2025-05-12 17:13:50,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO connection: Opened connection [connectionId{localValue:17, serverValue:1647}] to mongodb:27017
2025-05-12 17:13:50,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:50,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO connection: Closed connection [connectionId{localValue:17, serverValue:1647}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:50,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:50,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:50,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:50,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO connection: Opened connection [connectionId{localValue:18, serverValue:1648}] to mongodb:27017
2025-05-12 17:13:50,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1199170}
2025-05-12 17:13:50,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO connection: Opened connection [connectionId{localValue:19, serverValue:1649}] to mongodb:27017
2025-05-12 17:13:50,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:50,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO connection: Closed connection [connectionId{localValue:19, serverValue:1649}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:50,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:50,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:50,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:50,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 17:13:50,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:50,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:50,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:50,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:50,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:50,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.4 MiB)
2025-05-12 17:13:50,911 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 17:13:50,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 5e2bb930e569:35153 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:50,914 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:50,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:50,916 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 17:13:50,919 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:50,940 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:43197 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,034 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 115 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:51,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 17:13:51,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.132 s
2025-05-12 17:13:51,040 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:51,041 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: running: Set()
2025-05-12 17:13:51,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:51,043 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:51,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:51,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO CodeGenerator: Code generated in 16.989284 ms
2025-05-12 17:13:51,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO CodeGenerator: Code generated in 9.354394 ms
2025-05-12 17:13:51,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:51,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO CodeGenerator: Code generated in 18.022312 ms
2025-05-12 17:13:51,180 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:13:51,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:51,183 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:51,184 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 17:13:51,184 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:51,185 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:51,193 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 17:13:51,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 17:13:51,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 5e2bb930e569:35153 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,200 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:51,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:51,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 17:13:51,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:51,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:43197 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:35492
2025-05-12 17:13:51,345 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 142 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:51,346 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 17:13:51,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.157 s
2025-05-12 17:13:51,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:51,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 17:13:51,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.168834 s
2025-05-12 17:13:51,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:13:51,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:51,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:51,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:51,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO connection: Opened connection [connectionId{localValue:21, serverValue:1651}] to mongodb:27017
2025-05-12 17:13:51,496 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1422239}
2025-05-12 17:13:51,499 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO connection: Opened connection [connectionId{localValue:22, serverValue:1652}] to mongodb:27017
2025-05-12 17:13:51,501 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:51,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO connection: Closed connection [connectionId{localValue:22, serverValue:1652}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:51,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:51,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:51,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:51,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO connection: Opened connection [connectionId{localValue:23, serverValue:1653}] to mongodb:27017
2025-05-12 17:13:51,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1350145}
2025-05-12 17:13:51,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO connection: Opened connection [connectionId{localValue:24, serverValue:1654}] to mongodb:27017
2025-05-12 17:13:51,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:51,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO connection: Closed connection [connectionId{localValue:24, serverValue:1654}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:51,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:51,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:51,528 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:51,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 17:13:51,534 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:51,535 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:51,535 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:51,537 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:51,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:51,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.2 MiB)
2025-05-12 17:13:51,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 17:13:51,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 5e2bb930e569:35153 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:13:51,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 5e2bb930e569:35153 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:51,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:51,562 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 17:13:51,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:43197 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:51,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 5e2bb930e569:35153 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:43197 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:43197 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 5e2bb930e569:35153 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:13:51,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 142 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:51,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 17:13:51,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.172 s
2025-05-12 17:13:51,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:51,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: running: Set()
2025-05-12 17:13:51,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:51,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:51,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:51,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:51,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:13:51,781 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:51,782 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:51,783 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 17:13:51,783 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:51,784 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:51,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 17:13:51,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 17:13:51,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 5e2bb930e569:35153 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:51,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:51,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 17:13:51,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:51,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:43197 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:51,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:35492
2025-05-12 17:13:51,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 70 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:51,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 17:13:51,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.087 s
2025-05-12 17:13:51,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:51,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 17:13:51,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.098177 s
2025-05-12 17:13:51,968 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:51 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:13:52,021 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:52,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:52,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:52,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:52,062 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Opened connection [connectionId{localValue:25, serverValue:1655}] to mongodb:27017
2025-05-12 17:13:52,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1718550}
2025-05-12 17:13:52,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Opened connection [connectionId{localValue:26, serverValue:1656}] to mongodb:27017
2025-05-12 17:13:52,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:52,070 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Closed connection [connectionId{localValue:26, serverValue:1656}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:52,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:52,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:52,074 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:52,075 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Opened connection [connectionId{localValue:27, serverValue:1657}] to mongodb:27017
2025-05-12 17:13:52,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1258058}
2025-05-12 17:13:52,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Opened connection [connectionId{localValue:28, serverValue:1658}] to mongodb:27017
2025-05-12 17:13:52,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:52,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Closed connection [connectionId{localValue:28, serverValue:1658}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:52,096 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:52,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:52,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:52,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 17:13:52,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:52,101 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:52,102 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:52,104 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:52,105 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:52,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.2 MiB)
2025-05-12 17:13:52,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 17:13:52,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 5e2bb930e569:35153 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:13:52,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:52,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:52,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 17:13:52,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:52,177 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:43197 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:13:52,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 186 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:52,309 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 17:13:52,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.204 s
2025-05-12 17:13:52,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:52,312 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: running: Set()
2025-05-12 17:13:52,314 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:52,317 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:52,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:52,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:52,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO CodeGenerator: Code generated in 30.606977 ms
2025-05-12 17:13:52,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:13:52,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:52,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:52,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 17:13:52,431 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:52,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:52,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.1 MiB)
2025-05-12 17:13:52,453 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.1 MiB)
2025-05-12 17:13:52,457 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 5e2bb930e569:35153 (size: 25.7 KiB, free: 434.3 MiB)
2025-05-12 17:13:52,460 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:52,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 5e2bb930e569:35153 in memory (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:13:52,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:52,467 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:43197 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:52,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 17:13:52,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:52,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 5e2bb930e569:35153 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:52,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:43197 (size: 25.7 KiB, free: 434.3 MiB)
2025-05-12 17:13:52,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:43197 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:52,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 5e2bb930e569:35153 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:52,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:43197 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:52,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:35492
2025-05-12 17:13:52,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 147 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:52,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 17:13:52,616 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.180 s
2025-05-12 17:13:52,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:52,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 17:13:52,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.188525 s
2025-05-12 17:13:52,619 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 5, 'weekly': 5, 'hourly': 10}
2025-05-12 17:13:52,633 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 17:13:52,636 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 17:13:52,638 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 5e2bb930e569:35153 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:13:52,639 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:52,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:52,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:52,654 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:52,655 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Opened connection [connectionId{localValue:30, serverValue:1660}] to mongodb:27017
2025-05-12 17:13:52,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1426251}
2025-05-12 17:13:52,661 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Opened connection [connectionId{localValue:31, serverValue:1661}] to mongodb:27017
2025-05-12 17:13:52,663 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:52,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO connection: Closed connection [connectionId{localValue:31, serverValue:1661}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:52,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:52,679 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:52,680 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:52,701 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:13:52,703 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:13:52,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:13:52,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:52,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:52,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:13:52,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 17:13:52,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.3 MiB)
2025-05-12 17:13:52,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 5e2bb930e569:35153 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:52,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:52,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:52,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 17:13:52,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:13:52,753 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:43197 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:52,806 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:43197 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:13:52,979 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 255 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:52,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 17:13:53,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.273 s
2025-05-12 17:13:53,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:53,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 17:13:53,022 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.298620 s
2025-05-12 17:13:53,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:53,344 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:53,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:53,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Opened connection [connectionId{localValue:33, serverValue:1663}] to mongodb:27017
2025-05-12 17:13:53,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1069216}
2025-05-12 17:13:53,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Opened connection [connectionId{localValue:34, serverValue:1664}] to mongodb:27017
2025-05-12 17:13:53,354 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:53,355 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Closed connection [connectionId{localValue:34, serverValue:1664}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:53,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 17:13:53,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO CodeGenerator: Code generated in 50.807533 ms
2025-05-12 17:13:53,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:53,488 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:53,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:53,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Opened connection [connectionId{localValue:35, serverValue:1665}] to mongodb:27017
2025-05-12 17:13:53,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1975838}
2025-05-12 17:13:53,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Opened connection [connectionId{localValue:36, serverValue:1666}] to mongodb:27017
2025-05-12 17:13:53,501 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:53,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Closed connection [connectionId{localValue:36, serverValue:1666}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:53,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:53,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:53,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:53,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Opened connection [connectionId{localValue:37, serverValue:1667}] to mongodb:27017
2025-05-12 17:13:53,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1219681}
2025-05-12 17:13:53,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Opened connection [connectionId{localValue:38, serverValue:1668}] to mongodb:27017
2025-05-12 17:13:53,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:53,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO connection: Closed connection [connectionId{localValue:38, serverValue:1668}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:53,528 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:53,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:53,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:53,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 17:13:53,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:13:53,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 17:13:53,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:53,544 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:53,546 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:53,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.5 KiB, free 434.3 MiB)
2025-05-12 17:13:53,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.3 MiB)
2025-05-12 17:13:53,569 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 5e2bb930e569:35153 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:53,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 5e2bb930e569:35153 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:53,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:53,577 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:53,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 17:13:53,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:53,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:43197 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:53,607 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 5e2bb930e569:35153 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:13:53,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:43197 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:13:53,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:43197 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:53,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 269 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:53,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 17:13:53,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.304 s
2025-05-12 17:13:53,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:53,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: running: Set()
2025-05-12 17:13:53,850 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:53,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:53,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:53,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:53,946 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO CodeGenerator: Code generated in 48.989535 ms
2025-05-12 17:13:53,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:53 INFO CodeGenerator: Code generated in 12.215685 ms
2025-05-12 17:13:54,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO SparkContext: Starting job: rdd at MongoSpark.scala:169
2025-05-12 17:13:54,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Got job 14 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:13:54,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Final stage: ResultStage 21 (rdd at MongoSpark.scala:169)
2025-05-12 17:13:54,062 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 17:13:54,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:54,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[84] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:54,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 45.8 KiB, free 434.3 MiB)
2025-05-12 17:13:54,085 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 434.3 MiB)
2025-05-12 17:13:54,088 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 5e2bb930e569:35153 (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:54,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[84] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:54,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 17:13:54,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:54,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:43197 (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,176 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:35492
2025-05-12 17:13:54,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 172 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:54,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 17:13:54,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: ResultStage 21 (rdd at MongoSpark.scala:169) finished in 0.205 s
2025-05-12 17:13:54,272 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:54,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 17:13:54,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Job 14 finished: rdd at MongoSpark.scala:169, took 0.218748 s
2025-05-12 17:13:54,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Registering RDD 85 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 17:13:54,297 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:13:54,299 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (rdd at MongoSpark.scala:169)
2025-05-12 17:13:54,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
2025-05-12 17:13:54,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:54,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[85] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:54,317 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 46.0 KiB, free 434.2 MiB)
2025-05-12 17:13:54,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 21.2 KiB, free 434.2 MiB)
2025-05-12 17:13:54,321 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 5e2bb930e569:35153 (size: 21.2 KiB, free: 434.3 MiB)
2025-05-12 17:13:54,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:54,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[85] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:54,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
2025-05-12 17:13:54,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 15) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 17:13:54,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.23.0.8:43197 (size: 21.2 KiB, free: 434.3 MiB)
2025-05-12 17:13:54,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 15) in 87 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:54,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
2025-05-12 17:13:54,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: ShuffleMapStage 23 (rdd at MongoSpark.scala:169) finished in 0.115 s
2025-05-12 17:13:54,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:54,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: running: Set()
2025-05-12 17:13:54,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:54,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:54,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:54,455 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO CodeGenerator: Code generated in 16.200691 ms
2025-05-12 17:13:54,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:13:54,488 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:13:54,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Final stage: ResultStage 26 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:13:54,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
2025-05-12 17:13:54,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:54,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[91] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:54,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 51.9 KiB, free 434.2 MiB)
2025-05-12 17:13:54,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 22.9 KiB, free 434.1 MiB)
2025-05-12 17:13:54,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 5e2bb930e569:35153 (size: 22.9 KiB, free: 434.3 MiB)
2025-05-12 17:13:54,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:54,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 5e2bb930e569:35153 in memory (size: 21.0 KiB, free: 434.3 MiB)
2025-05-12 17:13:54,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[91] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:54,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
2025-05-12 17:13:54,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:43197 in memory (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:54,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:43197 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 5e2bb930e569:35153 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:43197 (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 5e2bb930e569:35153 in memory (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.23.0.8:43197 in memory (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:35492
2025-05-12 17:13:54,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 16) in 203 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:54,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
2025-05-12 17:13:54,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: ResultStage 26 (foreachPartition at MongoSpark.scala:120) finished in 0.231 s
2025-05-12 17:13:54,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:54,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
2025-05-12 17:13:54,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.238382 s
2025-05-12 17:13:54,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:13:54,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:13:54,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 5e2bb930e569:35153 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:13:54,736 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO SparkContext: Created broadcast 22 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:54,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:54,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:54,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:54,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Opened connection [connectionId{localValue:40, serverValue:1670}] to mongodb:27017
2025-05-12 17:13:54,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=997954}
2025-05-12 17:13:54,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Opened connection [connectionId{localValue:41, serverValue:1671}] to mongodb:27017
2025-05-12 17:13:54,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:54,777 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Closed connection [connectionId{localValue:41, serverValue:1671}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:54,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 17:13:54,891 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO CodeGenerator: Code generated in 64.771229 ms
2025-05-12 17:13:54,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:54,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:54,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:54,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Opened connection [connectionId{localValue:42, serverValue:1672}] to mongodb:27017
2025-05-12 17:13:54,911 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3996348}
2025-05-12 17:13:54,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Opened connection [connectionId{localValue:43, serverValue:1673}] to mongodb:27017
2025-05-12 17:13:54,917 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:54,919 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Closed connection [connectionId{localValue:43, serverValue:1673}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:54,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:54,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:54,922 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:54,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Opened connection [connectionId{localValue:44, serverValue:1674}] to mongodb:27017
2025-05-12 17:13:54,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1119827}
2025-05-12 17:13:54,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Opened connection [connectionId{localValue:45, serverValue:1675}] to mongodb:27017
2025-05-12 17:13:54,946 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:54,947 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO connection: Closed connection [connectionId{localValue:45, serverValue:1675}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:54,947 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:54,949 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:54,954 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:54,959 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Registering RDD 97 (rdd at MongoSpark.scala:169) as input to shuffle 8
2025-05-12 17:13:54,961 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Got map stage job 17 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:13:54,962 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (rdd at MongoSpark.scala:169)
2025-05-12 17:13:54,963 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:54,964 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:54,964 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[97] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:54,970 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 39.6 KiB, free 434.3 MiB)
2025-05-12 17:13:54,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 17:13:54,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 5e2bb930e569:35153 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:54,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:54,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[97] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:54,979 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 17:13:54,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:54 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:55,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.23.0.8:43197 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 17) in 209 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:55,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 17:13:55,193 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: ShuffleMapStage 27 (rdd at MongoSpark.scala:169) finished in 0.230 s
2025-05-12 17:13:55,194 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:55,195 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: running: Set()
2025-05-12 17:13:55,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:55,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:55,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:55,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:55,245 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:13:55,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Got job 18 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:13:55,248 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Final stage: ResultStage 29 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:13:55,249 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
2025-05-12 17:13:55,250 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:55,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[104] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:13:55,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.9 KiB, free 434.2 MiB)
2025-05-12 17:13:55,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 434.2 MiB)
2025-05-12 17:13:55,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 5e2bb930e569:35153 (size: 22.2 KiB, free: 434.3 MiB)
2025-05-12 17:13:55,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:55,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[104] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:55,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
2025-05-12 17:13:55,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 5e2bb930e569:35153 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.23.0.8:43197 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:55,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 5e2bb930e569:35153 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:43197 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,313 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:43197 (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 5e2bb930e569:35153 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:13:55,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:35492
2025-05-12 17:13:55,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 18) in 120 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:55,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
2025-05-12 17:13:55,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: ResultStage 29 (foreachPartition at MongoSpark.scala:120) finished in 0.150 s
2025-05-12 17:13:55,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:55,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
2025-05-12 17:13:55,408 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Job 18 finished: foreachPartition at MongoSpark.scala:120, took 0.158184 s
2025-05-12 17:13:55,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:13:55,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:13:55,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 5e2bb930e569:35153 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:13:55,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO SparkContext: Created broadcast 25 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:55,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 17:13:55,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:55,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO CodeGenerator: Code generated in 27.567683 ms
2025-05-12 17:13:55,528 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:55,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:55,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:55,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO connection: Opened connection [connectionId{localValue:47, serverValue:1677}] to mongodb:27017
2025-05-12 17:13:55,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1264416}
2025-05-12 17:13:55,536 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO connection: Opened connection [connectionId{localValue:48, serverValue:1678}] to mongodb:27017
2025-05-12 17:13:55,538 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:55,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO connection: Closed connection [connectionId{localValue:48, serverValue:1678}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:55,540 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:55,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:55,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:55,543 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO connection: Opened connection [connectionId{localValue:49, serverValue:1679}] to mongodb:27017
2025-05-12 17:13:55,544 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=868360}
2025-05-12 17:13:55,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO connection: Opened connection [connectionId{localValue:50, serverValue:1680}] to mongodb:27017
2025-05-12 17:13:55,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:55,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO connection: Closed connection [connectionId{localValue:50, serverValue:1680}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:55,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:55,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:55,560 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:55,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Registering RDD 110 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 17:13:55,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Got map stage job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:55,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:55,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:55,571 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:55,572 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:55,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 36.4 KiB, free 434.3 MiB)
2025-05-12 17:13:55,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 434.3 MiB)
2025-05-12 17:13:55,587 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 5e2bb930e569:35153 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:55,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:55,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 17:13:55,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:55,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:43197 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 130 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:55,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 17:13:55,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: ShuffleMapStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.159 s
2025-05-12 17:13:55,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:55,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: running: Set()
2025-05-12 17:13:55,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:55,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:55,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:55,755 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:55,780 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO CodeGenerator: Code generated in 17.916311 ms
2025-05-12 17:13:55,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Registering RDD 113 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 17:13:55,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:55,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:55,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
2025-05-12 17:13:55,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:55,798 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:55,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 45.0 KiB, free 434.2 MiB)
2025-05-12 17:13:55,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.2 MiB)
2025-05-12 17:13:55,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 5e2bb930e569:35153 (size: 20.4 KiB, free: 434.3 MiB)
2025-05-12 17:13:55,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:55,832 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:55,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
2025-05-12 17:13:55,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 5e2bb930e569:35153 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:13:55,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 20) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 17:13:55,846 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 5e2bb930e569:35153 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.23.0.8:43197 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,867 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:43197 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 5e2bb930e569:35153 in memory (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:43197 in memory (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:13:55,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:35492
2025-05-12 17:13:55,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 20) in 147 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:55,976 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
2025-05-12 17:13:55,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0) finished in 0.179 s
2025-05-12 17:13:55,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:55,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: running: Set()
2025-05-12 17:13:55,980 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:55,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:55 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:56,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO CodeGenerator: Code generated in 41.299512 ms
2025-05-12 17:13:56,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:13:56,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:56,096 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Final stage: ResultStage 35 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:56,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
2025-05-12 17:13:56,108 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:56,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:56,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 11.1 KiB, free 434.3 MiB)
2025-05-12 17:13:56,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)
2025-05-12 17:13:56,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 5e2bb930e569:35153 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:56,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:56,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 17:13:56,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:56,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:43197 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,165 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:35492
2025-05-12 17:13:56,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 21) in 102 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:56,219 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 17:13:56,221 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: ResultStage 35 (count at NativeMethodAccessorImpl.java:0) finished in 0.127 s
2025-05-12 17:13:56,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:56,228 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 17:13:56,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.139796 s
2025-05-12 17:13:56,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 17:13:56,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:56,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:56,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:56,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO connection: Opened connection [connectionId{localValue:52, serverValue:1683}] to mongodb:27017
2025-05-12 17:13:56,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1239090}
2025-05-12 17:13:56,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO connection: Opened connection [connectionId{localValue:53, serverValue:1684}] to mongodb:27017
2025-05-12 17:13:56,441 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:56,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO connection: Closed connection [connectionId{localValue:53, serverValue:1684}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:56,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:56,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:56,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:56,449 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO connection: Opened connection [connectionId{localValue:54, serverValue:1685}] to mongodb:27017
2025-05-12 17:13:56,454 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2328848}
2025-05-12 17:13:56,458 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO connection: Opened connection [connectionId{localValue:55, serverValue:1686}] to mongodb:27017
2025-05-12 17:13:56,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:56,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO connection: Closed connection [connectionId{localValue:55, serverValue:1686}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:56,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:56,487 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:56,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:56,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Registering RDD 121 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 11
2025-05-12 17:13:56,498 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Got map stage job 22 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:56,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:56,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:56,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:56,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[121] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:56,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 39.7 KiB, free 434.3 MiB)
2025-05-12 17:13:56,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 17:13:56,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 5e2bb930e569:35153 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:56,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[121] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:56,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
2025-05-12 17:13:56,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:13:56,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:43197 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 22) in 122 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:56,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
2025-05-12 17:13:56,650 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: ShuffleMapStage 36 (count at NativeMethodAccessorImpl.java:0) finished in 0.143 s
2025-05-12 17:13:56,652 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:13:56,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: running: Set()
2025-05-12 17:13:56,654 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: waiting: Set()
2025-05-12 17:13:56,655 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: failed: Set()
2025-05-12 17:13:56,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO ShufflePartitionsUtil: For shuffle(11), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:13:56,686 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:13:56,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:13:56,815 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Got job 23 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:13:56,817 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Final stage: ResultStage 38 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:13:56,820 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
2025-05-12 17:13:56,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:56,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[126] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:13:56,831 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 46.3 KiB, free 434.2 MiB)
2025-05-12 17:13:56,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 5e2bb930e569:35153 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.23.0.8:43197 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 5e2bb930e569:35153 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.23.0.8:43197 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 5e2bb930e569:35153 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.23.0.8:43197 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.3 MiB)
2025-05-12 17:13:56,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 5e2bb930e569:35153 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:56,909 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[126] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:56,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
2025-05-12 17:13:56,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 23) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:13:56,945 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:43197 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:13:56,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 172.23.0.8:35492
2025-05-12 17:13:57,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 23) in 90 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:57,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
2025-05-12 17:13:57,005 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: ResultStage 38 (count at NativeMethodAccessorImpl.java:0) finished in 0.191 s
2025-05-12 17:13:57,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:57,008 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
2025-05-12 17:13:57,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Job 23 finished: count at NativeMethodAccessorImpl.java:0, took 0.222516 s
2025-05-12 17:13:57,012 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 2, 'active_users': 2}
2025-05-12 17:13:57,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 17:13:57,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 17:13:57,074 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 5e2bb930e569:35153 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:13:57,076 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:57,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:57,088 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:57,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:57,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO connection: Opened connection [connectionId{localValue:56, serverValue:1687}] to mongodb:27017
2025-05-12 17:13:57,106 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2841820}
2025-05-12 17:13:57,109 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO connection: Opened connection [connectionId{localValue:57, serverValue:1688}] to mongodb:27017
2025-05-12 17:13:57,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:57,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO connection: Closed connection [connectionId{localValue:57, serverValue:1688}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:57,130 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:57,131 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:57,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:57,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:13:57,172 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Got job 24 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:13:57,173 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Final stage: ResultStage 39 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:13:57,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:57,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:57,175 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[131] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:13:57,180 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 17:13:57,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.3 MiB)
2025-05-12 17:13:57,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 5e2bb930e569:35153 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:57,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:57,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[131] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:57,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
2025-05-12 17:13:57,206 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 24) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:13:57,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:43197 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:57,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:43197 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:13:57,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 24) in 105 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:57,312 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
2025-05-12 17:13:57,313 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: ResultStage 39 (treeAggregate at MongoInferSchema.scala:88) finished in 0.138 s
2025-05-12 17:13:57,314 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:57,315 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
2025-05-12 17:13:57,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Job 24 finished: treeAggregate at MongoInferSchema.scala:88, took 0.145730 s
2025-05-12 17:13:57,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 17:13:57,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 17:13:57,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 5e2bb930e569:35153 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:13:57,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 5e2bb930e569:35153 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:13:57,392 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO SparkContext: Created broadcast 33 from broadcast at MongoSpark.scala:530
2025-05-12 17:13:57,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.23.0.8:43197 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:13:57,397 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:57,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:57,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:13:57,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO connection: Opened connection [connectionId{localValue:59, serverValue:1690}] to mongodb:27017
2025-05-12 17:13:57,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1077656}
2025-05-12 17:13:57,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO connection: Opened connection [connectionId{localValue:60, serverValue:1691}] to mongodb:27017
2025-05-12 17:13:57,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 5e2bb930e569:35153 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:57,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:57,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO connection: Closed connection [connectionId{localValue:60, serverValue:1691}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:13:57,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:13:57,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:13:57,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:13:57,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.23.0.8:43197 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:57,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:13:57,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Got job 25 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:13:57,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Final stage: ResultStage 40 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:13:57,451 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:13:57,451 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:13:57,453 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[136] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:13:57,455 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 17:13:57,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 17:13:57,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 5e2bb930e569:35153 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:57,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:13:57,467 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[136] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:13:57,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
2025-05-12 17:13:57,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 25) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:13:57,496 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.23.0.8:43197 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:13:57,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.23.0.8:43197 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:13:57,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 25) in 76 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:13:57,550 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
2025-05-12 17:13:57,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: ResultStage 40 (treeAggregate at MongoInferSchema.scala:88) finished in 0.098 s
2025-05-12 17:13:57,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:13:57,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
2025-05-12 17:13:57,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO DAGScheduler: Job 25 finished: treeAggregate at MongoInferSchema.scala:88, took 0.104449 s
2025-05-12 17:13:57,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO SparkUI: Stopped Spark web UI at http://5e2bb930e569:4040
2025-05-12 17:13:57,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 17:13:57,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 17:13:57,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 17:13:57,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO MemoryStore: MemoryStore cleared
2025-05-12 17:13:57,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManager: BlockManager stopped
2025-05-12 17:13:57,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 17:13:57,831 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 17:13:57,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:57 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 17:13:58,260 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 17:13:58,260 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 17:13:58,264 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 17:13:58,264 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 17:13:58,267 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 17:13:58,268 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 17:13:58,270 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 17:13:58,271 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 17:13:58,271 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 17:13:58,272 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 17:13:58,277 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 17:13:58,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:58 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 17:13:58,329 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ff147fe-e5c7-4412-bb35-c84401925ce9
2025-05-12 17:13:58,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ff147fe-e5c7-4412-bb35-c84401925ce9/pyspark-c78b3639-f852-4cdb-bbf5-25be524d3831
2025-05-12 17:13:58,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:13:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-0403da3f-adcc-45e3-b265-36aca6159da2
2025-05-12 17:13:58,402 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 17:13:58,403 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 17:13:58,403 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 17:13:58,403 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-59eb9d0e-8d93-4a3f-bbd4-36d959d3bc2b;1.0
2025-05-12 17:13:58,404 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 17:13:58,404 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 17:13:58,405 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 17:13:58,405 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 17:13:58,406 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 17:13:58,407 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 259ms :: artifacts dl 14ms
2025-05-12 17:13:58,407 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 17:13:58,408 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 17:13:58,408 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 17:13:58,409 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 17:13:58,409 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 17:13:58,410 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:13:58,410 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 17:13:58,410 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 17:13:58,411 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:13:58,411 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 17:13:58,412 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:13:58,412 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-59eb9d0e-8d93-4a3f-bbd4-36d959d3bc2b
2025-05-12 17:13:58,412 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 17:13:58,413 - SparkScheduler - ERROR - [trend_analysis] 0 artifacts copied, 4 already retrieved (0kB/7ms)
2025-05-12 17:13:58,413 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 17:13:58,414 - SparkScheduler - INFO - Job trend_analysis duration: 38.27 seconds
2025-05-12 17:20:32,493 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 17:20:32,497 - SparkScheduler - INFO - Waiting for services to be ready...
2025-05-12 17:21:06,000 - SparkScheduler - INFO - Setting up job schedules
2025-05-12 17:21:06,006 - SparkScheduler - INFO - Scheduled trend_analysis to run every 30 minutes
2025-05-12 17:21:06,012 - SparkScheduler - INFO - Scheduled user_recommender to run every 3 hours
2025-05-12 17:21:06,015 - SparkScheduler - INFO - Scheduled content_analyzer to run daily at 02:00
2025-05-12 17:21:06,017 - SparkScheduler - INFO - All jobs scheduled
2025-05-12 17:21:06,022 - SparkScheduler - INFO - Running initial job executions...
2025-05-12 17:21:06,027 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 17:21:06,027 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 17:21:10,387 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 17:28:46,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 17:28:48,122 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 17:28:48,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO ResourceUtils: ==============================================================
2025-05-12 17:28:48,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 17:28:48,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO ResourceUtils: ==============================================================
2025-05-12 17:28:48,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 17:28:48,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 17:28:48,214 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 17:28:48,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 17:28:48,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SecurityManager: Changing view acls to: spark
2025-05-12 17:28:48,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 17:28:48,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SecurityManager: Changing view acls groups to:
2025-05-12 17:28:48,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 17:28:48,297 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 17:28:48,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO Utils: Successfully started service 'sparkDriver' on port 35851.
2025-05-12 17:28:48,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 17:28:48,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 17:28:48,763 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 17:28:48,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 17:28:48,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 17:28:48,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-262e5ab3-a5df-427a-b362-ca379a852009
2025-05-12 17:28:48,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 17:28:48,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:48 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 17:28:49,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 17:28:49,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://fed0a0e64209:35851/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747070928111
2025-05-12 17:28:49,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://fed0a0e64209:35851/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747070928111
2025-05-12 17:28:49,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://fed0a0e64209:35851/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747070928111
2025-05-12 17:28:49,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://fed0a0e64209:35851/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747070928111
2025-05-12 17:28:49,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://fed0a0e64209:35851/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747070928111
2025-05-12 17:28:49,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-6e7af992-0b9e-4e5c-9d5b-82c42285693d/userFiles-679d90f1-e7bf-4bb9-ac01-ed35fa62e9cc/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 17:28:49,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://fed0a0e64209:35851/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747070928111
2025-05-12 17:28:49,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-6e7af992-0b9e-4e5c-9d5b-82c42285693d/userFiles-679d90f1-e7bf-4bb9-ac01-ed35fa62e9cc/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 17:28:49,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://fed0a0e64209:35851/files/org.mongodb_bson-4.0.5.jar with timestamp 1747070928111
2025-05-12 17:28:49,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-6e7af992-0b9e-4e5c-9d5b-82c42285693d/userFiles-679d90f1-e7bf-4bb9-ac01-ed35fa62e9cc/org.mongodb_bson-4.0.5.jar
2025-05-12 17:28:49,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://fed0a0e64209:35851/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747070928111
2025-05-12 17:28:49,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-6e7af992-0b9e-4e5c-9d5b-82c42285693d/userFiles-679d90f1-e7bf-4bb9-ac01-ed35fa62e9cc/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 17:28:49,366 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 17:28:49,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.2:7077 after 47 ms (0 ms spent in bootstraps)
2025-05-12 17:28:49,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512172849-0000
2025-05-12 17:28:49,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33283.
2025-05-12 17:28:49,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO NettyBlockTransferService: Server created on fed0a0e64209:33283
2025-05-12 17:28:49,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 17:28:49,628 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fed0a0e64209, 33283, None)
2025-05-12 17:28:49,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO BlockManagerMasterEndpoint: Registering block manager fed0a0e64209:33283 with 434.4 MiB RAM, BlockManagerId(driver, fed0a0e64209, 33283, None)
2025-05-12 17:28:49,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fed0a0e64209, 33283, None)
2025-05-12 17:28:49,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fed0a0e64209, 33283, None)
2025-05-12 17:28:49,644 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512172849-0000/0 on worker-20250512172037-172.23.0.8-34371 (172.23.0.8:34371) with 2 core(s)
2025-05-12 17:28:49,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512172849-0000/0 on hostPort 172.23.0.8:34371 with 2 core(s), 1024.0 MiB RAM
2025-05-12 17:28:49,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512172849-0000/0 is now RUNNING
2025-05-12 17:28:50,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:50 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 17:28:50,512 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 17:28:50,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 17:28:50,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:50 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 17:28:52,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 17:28:52,819 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 17:28:52,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fed0a0e64209:33283 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:28:52,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:52 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 17:28:54,760 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:58454) with ID 0,  ResourceProfileId 0
2025-05-12 17:28:54,922 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:54 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:43063 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 43063, None)
2025-05-12 17:28:56,963 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:56 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:28:57,005 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:57 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:28:57,023 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:28:57,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:57 INFO connection: Opened connection [connectionId{localValue:1, serverValue:1030}] to mongodb:27017
2025-05-12 17:28:57,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5818065}
2025-05-12 17:28:57,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:57 INFO connection: Opened connection [connectionId{localValue:2, serverValue:1031}] to mongodb:27017
2025-05-12 17:28:57,314 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:57 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:28:58,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO CodeGenerator: Code generated in 193.672911 ms
2025-05-12 17:28:58,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:28:58,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:28:58,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO connection: Opened connection [connectionId{localValue:3, serverValue:1033}] to mongodb:27017
2025-05-12 17:28:58,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1427326}
2025-05-12 17:28:58,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO connection: Opened connection [connectionId{localValue:4, serverValue:1034}] to mongodb:27017
2025-05-12 17:28:58,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 17:28:58,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:28:58,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 17:28:58,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:28:58,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:28:58,354 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:28:58,166 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 17:28:58,172 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 17:28:58,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fed0a0e64209:33283 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:28:58,175 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:28:58,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:28:58,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 17:28:58,236 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:28:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:02,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:43063 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:02,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:43063 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:05,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6993 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:05,221 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 17:29:05,231 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 7.081 s
2025-05-12 17:29:05,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:05,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: running: Set()
2025-05-12 17:29:05,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:05,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:05,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:05,312 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO CodeGenerator: Code generated in 14.643421 ms
2025-05-12 17:29:05,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:05,354 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO CodeGenerator: Code generated in 24.961526 ms
2025-05-12 17:29:05,441 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:29:05,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:29:05,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:29:05,447 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 17:29:05,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:05,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:05,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 17:29:05,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 17:29:05,473 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fed0a0e64209:33283 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:05,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:05,476 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:05,476 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 17:29:05,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:05,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:43063 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:05,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:05,626 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO connection: Closed connection [connectionId{localValue:2, serverValue:1031}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:05,840 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:58454
2025-05-12 17:29:06,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 647 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:06,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 17:29:06,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 0.666 s
2025-05-12 17:29:06,131 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:06,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 17:29:06,135 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 0.693633 s
2025-05-12 17:29:06,142 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:29:06,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:29:06,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fed0a0e64209:33283 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:29:06,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:06,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:06,184 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:06,185 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:06,187 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO connection: Opened connection [connectionId{localValue:5, serverValue:1039}] to mongodb:27017
2025-05-12 17:29:06,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1300213}
2025-05-12 17:29:06,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO connection: Opened connection [connectionId{localValue:6, serverValue:1040}] to mongodb:27017
2025-05-12 17:29:06,260 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:29:06,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO CodeGenerator: Code generated in 45.793476 ms
2025-05-12 17:29:06,373 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 17:29:06,374 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:29:06,374 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 17:29:06,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:06,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:06,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:06,380 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.2 MiB)
2025-05-12 17:29:06,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 17:29:06,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fed0a0e64209:33283 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:29:06,386 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:06,387 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:06,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 17:29:06,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:06,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:43063 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:29:06,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 130 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:06,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 17:29:06,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.144 s
2025-05-12 17:29:06,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:06,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: running: Set()
2025-05-12 17:29:06,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:06,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:06,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:06,537 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:06,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:29:06,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:29:06,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:29:06,569 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 17:29:06,569 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:06,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:06,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.1 MiB)
2025-05-12 17:29:06,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.1 MiB)
2025-05-12 17:29:06,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fed0a0e64209:33283 (size: 24.4 KiB, free: 434.3 MiB)
2025-05-12 17:29:06,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:06,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:06,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 17:29:06,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:06,605 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:43063 (size: 24.4 KiB, free: 434.3 MiB)
2025-05-12 17:29:06,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:58454
2025-05-12 17:29:06,678 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 93 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:06,679 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 17:29:06,680 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.106 s
2025-05-12 17:29:06,680 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:06,682 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 17:29:06,683 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.115234 s
2025-05-12 17:29:06,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.1 MiB)
2025-05-12 17:29:06,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.1 MiB)
2025-05-12 17:29:06,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fed0a0e64209:33283 (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:29:06,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:06,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on fed0a0e64209:33283 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:29:06,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:43063 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:29:06,755 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on fed0a0e64209:33283 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:06,762 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:43063 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:06,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:06,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO connection: Closed connection [connectionId{localValue:4, serverValue:1034}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:06,788 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:06,790 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO connection: Closed connection [connectionId{localValue:6, serverValue:1040}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:06,795 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fed0a0e64209:33283 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:29:06,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fed0a0e64209:33283 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:06,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:43063 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:06,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fed0a0e64209:33283 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:06,830 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:43063 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:06,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:29:06,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:06 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:07,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO CodeGenerator: Code generated in 40.591778 ms
2025-05-12 17:29:07,018 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:07,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:07,020 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:07,021 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Opened connection [connectionId{localValue:7, serverValue:1041}] to mongodb:27017
2025-05-12 17:29:07,022 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=859673}
2025-05-12 17:29:07,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Opened connection [connectionId{localValue:8, serverValue:1042}] to mongodb:27017
2025-05-12 17:29:07,026 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:07,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Closed connection [connectionId{localValue:8, serverValue:1042}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:07,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:07,030 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:07,030 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:07,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Opened connection [connectionId{localValue:9, serverValue:1043}] to mongodb:27017
2025-05-12 17:29:07,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1526756}
2025-05-12 17:29:07,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Opened connection [connectionId{localValue:10, serverValue:1044}] to mongodb:27017
2025-05-12 17:29:07,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:07,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Closed connection [connectionId{localValue:10, serverValue:1044}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:07,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:07,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:07,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:07,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 17:29:07,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:29:07,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 17:29:07,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:07,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:07,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:07,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 17:29:07,070 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 17:29:07,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fed0a0e64209:33283 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:07,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:07,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:07,074 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 17:29:07,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:07,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:43063 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:07,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 196 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:07,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 17:29:07,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.211 s
2025-05-12 17:29:07,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:07,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: running: Set()
2025-05-12 17:29:07,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:07,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:07,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:07,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO CodeGenerator: Code generated in 11.505722 ms
2025-05-12 17:29:07,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:07,335 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO CodeGenerator: Code generated in 23.3617 ms
2025-05-12 17:29:07,366 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:29:07,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:29:07,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:29:07,369 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 17:29:07,372 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:07,374 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:07,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.3 MiB)
2025-05-12 17:29:07,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)
2025-05-12 17:29:07,386 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fed0a0e64209:33283 (size: 27.2 KiB, free: 434.3 MiB)
2025-05-12 17:29:07,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:07,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:07,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 17:29:07,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:07,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:43063 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:07,457 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:58454
2025-05-12 17:29:07,619 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 226 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:07,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 17:29:07,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.245 s
2025-05-12 17:29:07,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:07,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 17:29:07,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.255693 s
2025-05-12 17:29:07,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.2 MiB)
2025-05-12 17:29:07,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.2 MiB)
2025-05-12 17:29:07,641 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on fed0a0e64209:33283 (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:29:07,644 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:07,646 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Removed broadcast_8_piece0 on fed0a0e64209:33283 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:07,650 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:43063 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:07,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Removed broadcast_6_piece0 on fed0a0e64209:33283 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:29:07,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Removed broadcast_7_piece0 on fed0a0e64209:33283 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:07,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:43063 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:07,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:29:07,914 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:07,916 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:07,917 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:07,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Opened connection [connectionId{localValue:12, serverValue:1046}] to mongodb:27017
2025-05-12 17:29:07,919 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=881945}
2025-05-12 17:29:07,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Opened connection [connectionId{localValue:13, serverValue:1047}] to mongodb:27017
2025-05-12 17:29:07,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:07,924 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Closed connection [connectionId{localValue:13, serverValue:1047}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:07,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:07,926 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:07,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:07,930 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Opened connection [connectionId{localValue:14, serverValue:1048}] to mongodb:27017
2025-05-12 17:29:07,931 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1073162}
2025-05-12 17:29:07,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Opened connection [connectionId{localValue:15, serverValue:1049}] to mongodb:27017
2025-05-12 17:29:07,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:07,944 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO connection: Closed connection [connectionId{localValue:15, serverValue:1049}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:07,944 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:07,945 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:07,946 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:07,949 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 17:29:07,953 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:07,953 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:07,954 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:07,954 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:07,955 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:07,957 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.4 MiB)
2025-05-12 17:29:07,962 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 17:29:07,964 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on fed0a0e64209:33283 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:07,965 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:07,966 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:07,966 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 17:29:07,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:07,993 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:43063 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 96 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:08,065 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 17:29:08,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.114 s
2025-05-12 17:29:08,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:08,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: running: Set()
2025-05-12 17:29:08,070 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:08,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:08,081 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:08,139 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO CodeGenerator: Code generated in 22.56808 ms
2025-05-12 17:29:08,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO CodeGenerator: Code generated in 8.353592 ms
2025-05-12 17:29:08,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:08,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO CodeGenerator: Code generated in 24.802973 ms
2025-05-12 17:29:08,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:29:08,220 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:08,221 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:08,222 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 17:29:08,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:08,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:08,231 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 17:29:08,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 17:29:08,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on fed0a0e64209:33283 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,236 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:08,237 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:08,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 17:29:08,240 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:08,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:43063 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:58454
2025-05-12 17:29:08,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Removed broadcast_10_piece0 on fed0a0e64209:33283 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:43063 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Removed broadcast_9_piece0 on fed0a0e64209:33283 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:29:08,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 154 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:08,395 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 17:29:08,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.168 s
2025-05-12 17:29:08,397 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:08,397 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 17:29:08,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.178626 s
2025-05-12 17:29:08,456 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:29:08,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:08,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:08,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:08,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Opened connection [connectionId{localValue:17, serverValue:1051}] to mongodb:27017
2025-05-12 17:29:08,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1132015}
2025-05-12 17:29:08,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Opened connection [connectionId{localValue:18, serverValue:1052}] to mongodb:27017
2025-05-12 17:29:08,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:08,525 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Closed connection [connectionId{localValue:18, serverValue:1052}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:08,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:08,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:08,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:08,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Opened connection [connectionId{localValue:19, serverValue:1053}] to mongodb:27017
2025-05-12 17:29:08,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=991999}
2025-05-12 17:29:08,535 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Opened connection [connectionId{localValue:20, serverValue:1054}] to mongodb:27017
2025-05-12 17:29:08,544 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:08,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Closed connection [connectionId{localValue:20, serverValue:1054}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:08,545 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:08,547 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:08,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:08,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 17:29:08,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:08,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:08,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:08,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:08,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:08,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.3 MiB)
2025-05-12 17:29:08,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 17:29:08,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on fed0a0e64209:33283 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:08,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:08,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 17:29:08,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:08,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:43063 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,666 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 96 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:08,666 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 17:29:08,667 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.113 s
2025-05-12 17:29:08,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:08,669 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: running: Set()
2025-05-12 17:29:08,670 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:08,670 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:08,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:08,701 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:08,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:29:08,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:08,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:08,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 17:29:08,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:08,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:08,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.2 MiB)
2025-05-12 17:29:08,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 17:29:08,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on fed0a0e64209:33283 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:29:08,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:08,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:08,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 17:29:08,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:08,763 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:43063 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:29:08,777 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:58454
2025-05-12 17:29:08,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 55 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:08,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 17:29:08,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.068 s
2025-05-12 17:29:08,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:08,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 17:29:08,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.075433 s
2025-05-12 17:29:08,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:29:08,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:08,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:08,926 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:08,927 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:08,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Opened connection [connectionId{localValue:22, serverValue:1056}] to mongodb:27017
2025-05-12 17:29:08,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=777400}
2025-05-12 17:29:08,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Opened connection [connectionId{localValue:23, serverValue:1057}] to mongodb:27017
2025-05-12 17:29:08,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:08,935 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Closed connection [connectionId{localValue:23, serverValue:1057}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:08,936 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:08,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:08,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:08,939 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Opened connection [connectionId{localValue:24, serverValue:1058}] to mongodb:27017
2025-05-12 17:29:08,940 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=633823}
2025-05-12 17:29:08,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Opened connection [connectionId{localValue:25, serverValue:1059}] to mongodb:27017
2025-05-12 17:29:08,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:08,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO connection: Closed connection [connectionId{localValue:25, serverValue:1059}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:08,951 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:08,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:08,953 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:08,955 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 17:29:08,956 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:08,957 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:08,957 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:08,958 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:08,958 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:08,960 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.1 MiB)
2025-05-12 17:29:08,968 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.1 MiB)
2025-05-12 17:29:08,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on fed0a0e64209:33283 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:29:08,970 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:08,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Removed broadcast_12_piece0 on fed0a0e64209:33283 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:29:08,972 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:08,972 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 17:29:08,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:43063 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:08,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Removed broadcast_13_piece0 on fed0a0e64209:33283 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:08,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:08 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:43063 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:43063 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Removed broadcast_11_piece0 on fed0a0e64209:33283 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:43063 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,062 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 88 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:09,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 17:29:09,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.105 s
2025-05-12 17:29:09,065 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:09,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: running: Set()
2025-05-12 17:29:09,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:09,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:09,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:09,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:09,106 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO CodeGenerator: Code generated in 16.31338 ms
2025-05-12 17:29:09,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:29:09,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:09,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:09,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 17:29:09,127 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:09,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:09,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.3 MiB)
2025-05-12 17:29:09,134 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.2 MiB)
2025-05-12 17:29:09,135 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on fed0a0e64209:33283 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:09,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:09,139 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 17:29:09,140 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:09,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:43063 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,173 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:58454
2025-05-12 17:29:09,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 71 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:09,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 17:29:09,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.082 s
2025-05-12 17:29:09,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:09,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 17:29:09,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.088870 s
2025-05-12 17:29:09,217 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 5, 'weekly': 5, 'hourly': 10}
2025-05-12 17:29:09,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.2 MiB)
2025-05-12 17:29:09,237 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.2 MiB)
2025-05-12 17:29:09,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on fed0a0e64209:33283 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:09,240 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Removed broadcast_14_piece0 on fed0a0e64209:33283 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:09,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:43063 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:09,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Removed broadcast_15_piece0 on fed0a0e64209:33283 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:09,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:09,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Opened connection [connectionId{localValue:27, serverValue:1061}] to mongodb:27017
2025-05-12 17:29:09,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3157377}
2025-05-12 17:29:09,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:43063 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Opened connection [connectionId{localValue:28, serverValue:1062}] to mongodb:27017
2025-05-12 17:29:09,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:09,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Closed connection [connectionId{localValue:28, serverValue:1062}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:09,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:09,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:09,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:09,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:29:09,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:29:09,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:29:09,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:09,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:09,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:29:09,308 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 17:29:09,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 17:29:09,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on fed0a0e64209:33283 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,312 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:09,312 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:09,313 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 17:29:09,315 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:29:09,332 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:43063 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,370 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:43063 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:09,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 118 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:09,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 17:29:09,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.129 s
2025-05-12 17:29:09,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:09,438 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 17:29:09,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.137388 s
2025-05-12 17:29:09,574 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:09,577 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:09,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:09,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Opened connection [connectionId{localValue:30, serverValue:1064}] to mongodb:27017
2025-05-12 17:29:09,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=776136}
2025-05-12 17:29:09,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Opened connection [connectionId{localValue:31, serverValue:1065}] to mongodb:27017
2025-05-12 17:29:09,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:09,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Closed connection [connectionId{localValue:31, serverValue:1065}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:09,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 17:29:09,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO CodeGenerator: Code generated in 34.252422 ms
2025-05-12 17:29:09,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:09,679 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:09,680 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:09,684 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Opened connection [connectionId{localValue:32, serverValue:1066}] to mongodb:27017
2025-05-12 17:29:09,685 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1345396}
2025-05-12 17:29:09,686 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Opened connection [connectionId{localValue:33, serverValue:1067}] to mongodb:27017
2025-05-12 17:29:09,687 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:09,688 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Closed connection [connectionId{localValue:33, serverValue:1067}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:09,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:09,690 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:09,690 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:09,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Opened connection [connectionId{localValue:34, serverValue:1068}] to mongodb:27017
2025-05-12 17:29:09,693 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=944167}
2025-05-12 17:29:09,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Opened connection [connectionId{localValue:35, serverValue:1069}] to mongodb:27017
2025-05-12 17:29:09,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:09,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO connection: Closed connection [connectionId{localValue:35, serverValue:1069}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:09,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:09,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:09,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:09,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 17:29:09,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:29:09,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 17:29:09,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:09,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:09,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:09,737 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.5 KiB, free 434.3 MiB)
2025-05-12 17:29:09,738 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.3 MiB)
2025-05-12 17:29:09,739 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on fed0a0e64209:33283 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:09,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:09,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 17:29:09,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:09,755 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:43063 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:09,930 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 203 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:09,931 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 17:29:09,931 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.217 s
2025-05-12 17:29:09,932 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:09,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: running: Set()
2025-05-12 17:29:09,935 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:09,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:09,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:09,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:09,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO CodeGenerator: Code generated in 19.304913 ms
2025-05-12 17:29:09,997 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:09 INFO CodeGenerator: Code generated in 7.54694 ms
2025-05-12 17:29:10,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO SparkContext: Starting job: rdd at MongoSpark.scala:169
2025-05-12 17:29:10,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Got job 14 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:29:10,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Final stage: ResultStage 21 (rdd at MongoSpark.scala:169)
2025-05-12 17:29:10,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 17:29:10,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:10,062 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[84] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:10,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 45.8 KiB, free 434.3 MiB)
2025-05-12 17:29:10,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 434.3 MiB)
2025-05-12 17:29:10,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on fed0a0e64209:33283 (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_17_piece0 on fed0a0e64209:33283 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:10,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[84] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:10,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 17:29:10,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:43063 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:10,104 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_18_piece0 on fed0a0e64209:33283 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:43063 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:43063 (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:58454
2025-05-12 17:29:10,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 112 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:10,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 17:29:10,203 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: ResultStage 21 (rdd at MongoSpark.scala:169) finished in 0.137 s
2025-05-12 17:29:10,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:10,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 17:29:10,206 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Job 14 finished: rdd at MongoSpark.scala:169, took 0.142803 s
2025-05-12 17:29:10,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Registering RDD 85 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 17:29:10,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:29:10,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (rdd at MongoSpark.scala:169)
2025-05-12 17:29:10,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
2025-05-12 17:29:10,218 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:10,219 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[85] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:10,228 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 46.0 KiB, free 434.3 MiB)
2025-05-12 17:29:10,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 21.2 KiB, free 434.3 MiB)
2025-05-12 17:29:10,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on fed0a0e64209:33283 (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:10,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[85] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:10,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
2025-05-12 17:29:10,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 15) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 17:29:10,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.23.0.8:43063 (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 15) in 64 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:10,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
2025-05-12 17:29:10,304 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: ShuffleMapStage 23 (rdd at MongoSpark.scala:169) finished in 0.084 s
2025-05-12 17:29:10,305 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:10,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: running: Set()
2025-05-12 17:29:10,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:10,307 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:10,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:10,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO CodeGenerator: Code generated in 12.629226 ms
2025-05-12 17:29:10,364 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:29:10,366 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:29:10,367 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Final stage: ResultStage 26 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:29:10,368 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
2025-05-12 17:29:10,369 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:10,371 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[91] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:10,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 51.9 KiB, free 434.2 MiB)
2025-05-12 17:29:10,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 22.9 KiB, free 434.2 MiB)
2025-05-12 17:29:10,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on fed0a0e64209:33283 (size: 22.9 KiB, free: 434.3 MiB)
2025-05-12 17:29:10,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:10,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[91] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:10,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
2025-05-12 17:29:10,395 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_19_piece0 on fed0a0e64209:33283 in memory (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:43063 in memory (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:10,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_20_piece0 on fed0a0e64209:33283 in memory (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.23.0.8:43063 in memory (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:43063 (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,454 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:58454
2025-05-12 17:29:10,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 16) in 182 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:10,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
2025-05-12 17:29:10,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: ResultStage 26 (foreachPartition at MongoSpark.scala:120) finished in 0.210 s
2025-05-12 17:29:10,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:10,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
2025-05-12 17:29:10,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.216954 s
2025-05-12 17:29:10,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:29:10,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:29:10,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on fed0a0e64209:33283 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:29:10,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO SparkContext: Created broadcast 22 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:10,621 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:10,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:10,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:10,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Opened connection [connectionId{localValue:37, serverValue:1071}] to mongodb:27017
2025-05-12 17:29:10,627 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1346414}
2025-05-12 17:29:10,631 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Opened connection [connectionId{localValue:38, serverValue:1072}] to mongodb:27017
2025-05-12 17:29:10,633 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:10,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Closed connection [connectionId{localValue:38, serverValue:1072}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:10,655 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 17:29:10,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO CodeGenerator: Code generated in 32.677312 ms
2025-05-12 17:29:10,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:10,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:10,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:10,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Opened connection [connectionId{localValue:39, serverValue:1073}] to mongodb:27017
2025-05-12 17:29:10,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=951467}
2025-05-12 17:29:10,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Opened connection [connectionId{localValue:40, serverValue:1074}] to mongodb:27017
2025-05-12 17:29:10,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:10,726 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Closed connection [connectionId{localValue:40, serverValue:1074}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:10,727 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:10,728 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:10,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:10,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Opened connection [connectionId{localValue:41, serverValue:1075}] to mongodb:27017
2025-05-12 17:29:10,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1160545}
2025-05-12 17:29:10,735 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Opened connection [connectionId{localValue:42, serverValue:1076}] to mongodb:27017
2025-05-12 17:29:10,743 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:10,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO connection: Closed connection [connectionId{localValue:42, serverValue:1076}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:10,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:10,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:10,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:10,751 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Registering RDD 97 (rdd at MongoSpark.scala:169) as input to shuffle 8
2025-05-12 17:29:10,751 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Got map stage job 17 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:29:10,752 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (rdd at MongoSpark.scala:169)
2025-05-12 17:29:10,753 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:10,753 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:10,754 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[97] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:10,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 39.6 KiB, free 434.3 MiB)
2025-05-12 17:29:10,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 17:29:10,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on fed0a0e64209:33283 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_22_piece0 on fed0a0e64209:33283 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:29:10,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:10,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[97] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:10,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 17:29:10,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:10,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:43063 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Removed broadcast_21_piece0 on fed0a0e64209:33283 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,802 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.23.0.8:43063 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:10,945 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 17) in 167 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:10,946 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 17:29:10,948 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: ShuffleMapStage 27 (rdd at MongoSpark.scala:169) finished in 0.193 s
2025-05-12 17:29:10,948 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:10,949 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: running: Set()
2025-05-12 17:29:10,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:10,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:10,958 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:10,966 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:10,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:10 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:29:11,002 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Got job 18 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:29:11,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Final stage: ResultStage 29 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:29:11,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
2025-05-12 17:29:11,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:11,005 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[104] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:29:11,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.9 KiB, free 434.3 MiB)
2025-05-12 17:29:11,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 434.3 MiB)
2025-05-12 17:29:11,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on fed0a0e64209:33283 (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,018 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:11,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[104] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:11,020 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
2025-05-12 17:29:11,023 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:11,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:43063 (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,064 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:58454
2025-05-12 17:29:11,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 18) in 99 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:11,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
2025-05-12 17:29:11,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: ResultStage 29 (foreachPartition at MongoSpark.scala:120) finished in 0.115 s
2025-05-12 17:29:11,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:11,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
2025-05-12 17:29:11,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Job 18 finished: foreachPartition at MongoSpark.scala:120, took 0.125667 s
2025-05-12 17:29:11,130 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:29:11,141 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:29:11,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on fed0a0e64209:33283 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:29:11,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Removed broadcast_23_piece0 on fed0a0e64209:33283 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO SparkContext: Created broadcast 25 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:11,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.23.0.8:43063 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Removed broadcast_24_piece0 on fed0a0e64209:33283 in memory (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,171 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:43063 in memory (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,195 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 17:29:11,220 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:11,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO CodeGenerator: Code generated in 19.639163 ms
2025-05-12 17:29:11,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:11,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:11,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:11,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Opened connection [connectionId{localValue:44, serverValue:1078}] to mongodb:27017
2025-05-12 17:29:11,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1284460}
2025-05-12 17:29:11,259 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Opened connection [connectionId{localValue:45, serverValue:1079}] to mongodb:27017
2025-05-12 17:29:11,260 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:11,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Closed connection [connectionId{localValue:45, serverValue:1079}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:11,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:11,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:11,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:11,265 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Opened connection [connectionId{localValue:46, serverValue:1080}] to mongodb:27017
2025-05-12 17:29:11,267 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=990709}
2025-05-12 17:29:11,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Opened connection [connectionId{localValue:47, serverValue:1081}] to mongodb:27017
2025-05-12 17:29:11,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:11,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Closed connection [connectionId{localValue:47, serverValue:1081}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:11,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:11,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:11,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:11,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Registering RDD 110 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 17:29:11,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Got map stage job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:11,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:11,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:11,288 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:11,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:11,291 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 36.4 KiB, free 434.4 MiB)
2025-05-12 17:29:11,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 434.3 MiB)
2025-05-12 17:29:11,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on fed0a0e64209:33283 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:11,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:11,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 17:29:11,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:11,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:43063 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 126 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:11,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 17:29:11,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: ShuffleMapStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.138 s
2025-05-12 17:29:11,430 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:11,431 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: running: Set()
2025-05-12 17:29:11,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:11,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:11,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:11,456 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:11,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO CodeGenerator: Code generated in 20.792479 ms
2025-05-12 17:29:11,501 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Registering RDD 113 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 17:29:11,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:11,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:11,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
2025-05-12 17:29:11,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:11,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:11,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 45.0 KiB, free 434.3 MiB)
2025-05-12 17:29:11,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.3 MiB)
2025-05-12 17:29:11,519 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on fed0a0e64209:33283 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:11,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:11,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
2025-05-12 17:29:11,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 20) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 17:29:11,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:43063 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:58454
2025-05-12 17:29:11,593 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Removed broadcast_26_piece0 on fed0a0e64209:33283 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.23.0.8:43063 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Removed broadcast_25_piece0 on fed0a0e64209:33283 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:29:11,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 20) in 132 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:11,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
2025-05-12 17:29:11,661 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0) finished in 0.152 s
2025-05-12 17:29:11,662 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:11,663 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: running: Set()
2025-05-12 17:29:11,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:11,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:11,685 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO CodeGenerator: Code generated in 7.535127 ms
2025-05-12 17:29:11,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:29:11,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:11,703 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Final stage: ResultStage 35 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:11,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
2025-05-12 17:29:11,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:11,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:11,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 11.1 KiB, free 434.3 MiB)
2025-05-12 17:29:11,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)
2025-05-12 17:29:11,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on fed0a0e64209:33283 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:11,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:11,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 17:29:11,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:11,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:43063 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,752 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:58454
2025-05-12 17:29:11,781 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 21) in 65 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:11,782 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 17:29:11,783 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: ResultStage 35 (count at NativeMethodAccessorImpl.java:0) finished in 0.077 s
2025-05-12 17:29:11,784 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:11,785 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 17:29:11,785 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.084511 s
2025-05-12 17:29:11,817 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 17:29:11,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:11,865 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:11,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:11,867 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Opened connection [connectionId{localValue:49, serverValue:1084}] to mongodb:27017
2025-05-12 17:29:11,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1141542}
2025-05-12 17:29:11,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Opened connection [connectionId{localValue:50, serverValue:1085}] to mongodb:27017
2025-05-12 17:29:11,874 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:11,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Closed connection [connectionId{localValue:50, serverValue:1085}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:11,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:11,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:11,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:11,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Opened connection [connectionId{localValue:51, serverValue:1086}] to mongodb:27017
2025-05-12 17:29:11,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1547652}
2025-05-12 17:29:11,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Opened connection [connectionId{localValue:52, serverValue:1087}] to mongodb:27017
2025-05-12 17:29:11,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:11,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO connection: Closed connection [connectionId{localValue:52, serverValue:1087}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:11,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:11,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:11,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:11,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Registering RDD 121 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 11
2025-05-12 17:29:11,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Got map stage job 22 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:11,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:11,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:11,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:11,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[121] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:11,909 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 39.7 KiB, free 434.3 MiB)
2025-05-12 17:29:11,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 17:29:11,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on fed0a0e64209:33283 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:11,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:11,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[121] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:11,924 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
2025-05-12 17:29:11,926 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:29:11,951 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:11 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:43063 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 22) in 130 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:12,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
2025-05-12 17:29:12,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: ShuffleMapStage 36 (count at NativeMethodAccessorImpl.java:0) finished in 0.150 s
2025-05-12 17:29:12,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:29:12,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: running: Set()
2025-05-12 17:29:12,062 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: waiting: Set()
2025-05-12 17:29:12,065 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: failed: Set()
2025-05-12 17:29:12,073 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO ShufflePartitionsUtil: For shuffle(11), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:29:12,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:29:12,125 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:29:12,127 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Got job 23 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:29:12,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Final stage: ResultStage 38 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:29:12,129 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
2025-05-12 17:29:12,129 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:12,130 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[126] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:29:12,134 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 46.3 KiB, free 434.2 MiB)
2025-05-12 17:29:12,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.2 MiB)
2025-05-12 17:29:12,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on fed0a0e64209:33283 (size: 21.1 KiB, free: 434.3 MiB)
2025-05-12 17:29:12,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Removed broadcast_29_piece0 on fed0a0e64209:33283 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:12,153 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[126] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:12,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
2025-05-12 17:29:12,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 23) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:29:12,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.23.0.8:43063 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,166 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Removed broadcast_28_piece0 on fed0a0e64209:33283 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.23.0.8:43063 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Removed broadcast_27_piece0 on fed0a0e64209:33283 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,193 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:43063 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.23.0.8:43063 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,221 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 172.23.0.8:58454
2025-05-12 17:29:12,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 23) in 128 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:12,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
2025-05-12 17:29:12,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: ResultStage 38 (count at NativeMethodAccessorImpl.java:0) finished in 0.155 s
2025-05-12 17:29:12,291 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:12,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
2025-05-12 17:29:12,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Job 23 finished: count at NativeMethodAccessorImpl.java:0, took 0.162263 s
2025-05-12 17:29:12,298 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 2, 'active_users': 2}
2025-05-12 17:29:12,315 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 17:29:12,317 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 17:29:12,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on fed0a0e64209:33283 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:12,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:12,321 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:12,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:12,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:12,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO connection: Opened connection [connectionId{localValue:54, serverValue:1089}] to mongodb:27017
2025-05-12 17:29:12,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1231384}
2025-05-12 17:29:12,330 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO connection: Opened connection [connectionId{localValue:55, serverValue:1090}] to mongodb:27017
2025-05-12 17:29:12,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:12,332 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO connection: Closed connection [connectionId{localValue:55, serverValue:1090}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:12,337 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:12,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:12,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:12,356 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:29:12,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Got job 24 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:29:12,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Final stage: ResultStage 39 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:29:12,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:12,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:12,360 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[131] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:29:12,361 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 17:29:12,371 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.3 MiB)
2025-05-12 17:29:12,372 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on fed0a0e64209:33283 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:12,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[131] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:12,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Removed broadcast_30_piece0 on fed0a0e64209:33283 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
2025-05-12 17:29:12,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 24) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:29:12,384 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.23.0.8:43063 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:43063 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:43063 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:12,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 24) in 67 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:12,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
2025-05-12 17:29:12,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: ResultStage 39 (treeAggregate at MongoInferSchema.scala:88) finished in 0.085 s
2025-05-12 17:29:12,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:12,447 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
2025-05-12 17:29:12,447 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Job 24 finished: treeAggregate at MongoInferSchema.scala:88, took 0.090325 s
2025-05-12 17:29:12,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 17:29:12,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 17:29:12,470 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on fed0a0e64209:33283 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:12,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Created broadcast 33 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:12,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:12,476 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:12,477 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:12,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO connection: Opened connection [connectionId{localValue:57, serverValue:1092}] to mongodb:27017
2025-05-12 17:29:12,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=889209}
2025-05-12 17:29:12,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO connection: Opened connection [connectionId{localValue:58, serverValue:1093}] to mongodb:27017
2025-05-12 17:29:12,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:12,486 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO connection: Closed connection [connectionId{localValue:58, serverValue:1093}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:12,491 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:12,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:12,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:12,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:29:12,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Got job 25 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:29:12,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Final stage: ResultStage 40 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:29:12,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:12,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:12,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[136] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:29:12,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 17:29:12,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 17:29:12,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on fed0a0e64209:33283 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:12,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[136] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:12,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
2025-05-12 17:29:12,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 25) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:29:12,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.23.0.8:43063 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:12,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.23.0.8:43063 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:12,593 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 25) in 73 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:12,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
2025-05-12 17:29:12,596 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: ResultStage 40 (treeAggregate at MongoInferSchema.scala:88) finished in 0.085 s
2025-05-12 17:29:12,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:12,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
2025-05-12 17:29:12,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO DAGScheduler: Job 25 finished: treeAggregate at MongoInferSchema.scala:88, took 0.091338 s
2025-05-12 17:29:12,691 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkUI: Stopped Spark web UI at http://fed0a0e64209:4040
2025-05-12 17:29:12,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 17:29:12,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 17:29:12,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 17:29:12,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO MemoryStore: MemoryStore cleared
2025-05-12 17:29:12,865 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManager: BlockManager stopped
2025-05-12 17:29:12,885 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 17:29:12,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 17:29:12,961 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:12 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 17:29:13,193 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 17:29:13,194 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 17:29:13,194 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 17:29:13,195 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 17:29:13,197 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 17:29:13,198 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 17:29:13,200 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 17:29:13,201 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 17:29:13,203 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 17:29:13,205 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 17:29:13,216 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 17:29:13,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:13 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 17:29:13,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-bb51e020-ec46-4b3e-9ab3-70ce0a17d3aa
2025-05-12 17:29:13,313 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-6e7af992-0b9e-4e5c-9d5b-82c42285693d
2025-05-12 17:29:13,321 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:29:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-6e7af992-0b9e-4e5c-9d5b-82c42285693d/pyspark-7b13fc88-c030-44aa-961e-b2c61b400c0b
2025-05-12 17:29:13,408 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 17:29:13,409 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 17:29:13,409 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 17:29:13,410 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-b5ae3e82-9555-4fed-a698-16348b281e64;1.0
2025-05-12 17:29:13,410 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 17:29:13,410 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 17:29:13,411 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 17:29:13,412 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 17:29:13,413 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 17:29:13,414 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...
2025-05-12 17:29:13,414 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (75637ms)
2025-05-12 17:29:13,415 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...
2025-05-12 17:29:13,415 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (11202ms)
2025-05-12 17:29:13,416 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...
2025-05-12 17:29:13,416 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (63689ms)
2025-05-12 17:29:13,416 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...
2025-05-12 17:29:13,417 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (292204ms)
2025-05-12 17:29:13,417 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 12888ms :: artifacts dl 442749ms
2025-05-12 17:29:13,419 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 17:29:13,419 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 17:29:13,420 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 17:29:13,420 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 17:29:13,420 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 17:29:13,421 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:29:13,421 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 17:29:13,422 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 17:29:13,422 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:29:13,423 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   4   |   4   |   0   ||   4   |   4   |
2025-05-12 17:29:13,423 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:29:13,423 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-b5ae3e82-9555-4fed-a698-16348b281e64
2025-05-12 17:29:13,424 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 17:29:13,424 - SparkScheduler - ERROR - [trend_analysis] 4 artifacts copied, 0 already retrieved (2728kB/29ms)
2025-05-12 17:29:13,430 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 17:29:13,431 - SparkScheduler - INFO - Job trend_analysis duration: 487.40 seconds
2025-05-12 17:29:13,445 - SparkScheduler - INFO - Starting job: user_recommender - Generate user recommendations
2025-05-12 17:29:13,445 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/user_recommender.py
2025-05-12 17:29:16,289 - SparkScheduler - INFO - [user_recommender] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 17:29:17,036 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 17:29:19,032 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 17:29:19,061 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO ResourceUtils: ==============================================================
2025-05-12 17:29:19,062 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 17:29:19,063 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO ResourceUtils: ==============================================================
2025-05-12 17:29:19,065 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SparkContext: Submitted application: MiniTwitterUserRecommender
2025-05-12 17:29:19,090 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 17:29:19,099 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 17:29:19,099 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 17:29:19,147 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SecurityManager: Changing view acls to: spark
2025-05-12 17:29:19,147 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 17:29:19,148 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SecurityManager: Changing view acls groups to:
2025-05-12 17:29:19,149 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 17:29:19,150 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 17:29:19,462 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO Utils: Successfully started service 'sparkDriver' on port 42661.
2025-05-12 17:29:19,492 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 17:29:19,529 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 17:29:19,559 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 17:29:19,561 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 17:29:19,574 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 17:29:19,605 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2c31b911-5ba9-4d78-8f66-c11b3120af97
2025-05-12 17:29:19,645 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 17:29:19,681 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 17:29:19,973 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 17:29:20,029 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://fed0a0e64209:42661/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747070959021
2025-05-12 17:29:20,030 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://fed0a0e64209:42661/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747070959021
2025-05-12 17:29:20,030 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://fed0a0e64209:42661/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747070959021
2025-05-12 17:29:20,031 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://fed0a0e64209:42661/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747070959021
2025-05-12 17:29:20,035 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://fed0a0e64209:42661/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747070959021
2025-05-12 17:29:20,038 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-879e653b-c39c-4dc1-83ee-6ed57ba8ef9c/userFiles-5f71371e-178b-4368-b166-981f7a7e7d5d/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 17:29:20,057 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://fed0a0e64209:42661/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747070959021
2025-05-12 17:29:20,057 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-879e653b-c39c-4dc1-83ee-6ed57ba8ef9c/userFiles-5f71371e-178b-4368-b166-981f7a7e7d5d/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 17:29:20,070 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://fed0a0e64209:42661/files/org.mongodb_bson-4.0.5.jar with timestamp 1747070959021
2025-05-12 17:29:20,071 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-879e653b-c39c-4dc1-83ee-6ed57ba8ef9c/userFiles-5f71371e-178b-4368-b166-981f7a7e7d5d/org.mongodb_bson-4.0.5.jar
2025-05-12 17:29:20,086 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://fed0a0e64209:42661/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747070959021
2025-05-12 17:29:20,087 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-879e653b-c39c-4dc1-83ee-6ed57ba8ef9c/userFiles-5f71371e-178b-4368-b166-981f7a7e7d5d/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 17:29:20,207 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 17:29:20,251 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.2:7077 after 24 ms (0 ms spent in bootstraps)
2025-05-12 17:29:20,342 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512172920-0001
2025-05-12 17:29:20,346 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512172920-0001/0 on worker-20250512172037-172.23.0.8-34371 (172.23.0.8:34371) with 2 core(s)
2025-05-12 17:29:20,353 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512172920-0001/0 on hostPort 172.23.0.8:34371 with 2 core(s), 1024.0 MiB RAM
2025-05-12 17:29:20,356 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43949.
2025-05-12 17:29:20,357 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO NettyBlockTransferService: Server created on fed0a0e64209:43949
2025-05-12 17:29:20,361 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 17:29:20,373 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fed0a0e64209, 43949, None)
2025-05-12 17:29:20,381 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO BlockManagerMasterEndpoint: Registering block manager fed0a0e64209:43949 with 434.4 MiB RAM, BlockManagerId(driver, fed0a0e64209, 43949, None)
2025-05-12 17:29:20,385 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fed0a0e64209, 43949, None)
2025-05-12 17:29:20,389 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fed0a0e64209, 43949, None)
2025-05-12 17:29:20,409 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512172920-0001/0 is now RUNNING
2025-05-12 17:29:20,777 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:20 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 17:29:21,205 - SparkScheduler - INFO - [user_recommender] Starting Mini Twitter User Recommender...
2025-05-12 17:29:21,220 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 17:29:21,226 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:21 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 17:29:22,937 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 17:29:23,038 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 17:29:23,045 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fed0a0e64209:43949 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:23,059 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 17:29:23,283 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:29:23,371 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:29:23,389 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:29:23,419 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO connection: Opened connection [connectionId{localValue:1, serverValue:1096}] to mongodb:27017
2025-05-12 17:29:23,431 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=8554253}
2025-05-12 17:29:23,465 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:23 INFO connection: Opened connection [connectionId{localValue:2, serverValue:1097}] to mongodb:27017
2025-05-12 17:29:24,360 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:29:24,402 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO DAGScheduler: Got job 0 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:29:24,405 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:29:24,407 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:29:24,414 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:29:24,428 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:29:24,502 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 17:29:24,536 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 17:29:24,538 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fed0a0e64209:43949 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:24,541 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:29:24,586 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:29:24,589 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 17:29:25,197 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:54446) with ID 0,  ResourceProfileId 0
2025-05-12 17:29:25,339 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:44269 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 44269, None)
2025-05-12 17:29:25,748 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:29:26,284 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:44269 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:26,801 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:44269 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:29:27,759 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2042 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:29:27,762 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 17:29:27,773 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:27 INFO DAGScheduler: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88) finished in 3.294 s
2025-05-12 17:29:27,778 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:29:27,779 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
2025-05-12 17:29:27,846 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:27 INFO DAGScheduler: Job 0 finished: treeAggregate at MongoInferSchema.scala:88, took 3.484811 s
2025-05-12 17:29:29,319 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:29 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:29:29,325 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:29 INFO connection: Closed connection [connectionId{localValue:2, serverValue:1097}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:29:29,418 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fed0a0e64209:43949 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:29,435 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:44269 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:29:30,581 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO SparkUI: Stopped Spark web UI at http://fed0a0e64209:4040
2025-05-12 17:29:30,588 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 17:29:30,590 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 17:29:30,638 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 17:29:30,673 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO MemoryStore: MemoryStore cleared
2025-05-12 17:29:30,673 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO BlockManager: BlockManager stopped
2025-05-12 17:29:30,679 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 17:29:30,683 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 17:29:30,707 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 17:29:30,848 - SparkScheduler - INFO - [user_recommender] Traceback (most recent call last):
2025-05-12 17:29:30,849 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 170, in <module>
2025-05-12 17:29:30,853 - SparkScheduler - INFO - [user_recommender] main()
2025-05-12 17:29:30,854 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 159, in main
2025-05-12 17:29:30,858 - SparkScheduler - INFO - [user_recommender] recommendation_results = generate_user_recommendations(spark)
2025-05-12 17:29:30,859 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 35, in generate_user_recommendations
2025-05-12 17:29:30,861 - SparkScheduler - INFO - [user_recommender] .load())
2025-05-12 17:29:30,861 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 184, in load
2025-05-12 17:29:30,862 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 17:29:30,865 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
2025-05-12 17:29:30,866 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
2025-05-12 17:29:30,866 - SparkScheduler - INFO - [user_recommender] py4j.protocol.Py4JJavaError: An error occurred while calling o45.load.
2025-05-12 17:29:30,866 - SparkScheduler - INFO - [user_recommender] : java.sql.SQLException: No suitable driver
2025-05-12 17:29:30,867 - SparkScheduler - INFO - [user_recommender] at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)
2025-05-12 17:29:30,867 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)
2025-05-12 17:29:30,868 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 17:29:30,868 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)
2025-05-12 17:29:30,869 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)
2025-05-12 17:29:30,870 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
2025-05-12 17:29:30,871 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
2025-05-12 17:29:30,871 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
2025-05-12 17:29:30,872 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
2025-05-12 17:29:30,873 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 17:29:30,873 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
2025-05-12 17:29:30,874 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
2025-05-12 17:29:30,874 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2025-05-12 17:29:30,875 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
2025-05-12 17:29:30,875 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2025-05-12 17:29:30,875 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.reflect.Method.invoke(Method.java:568)
2025-05-12 17:29:30,876 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2025-05-12 17:29:30,877 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2025-05-12 17:29:30,877 - SparkScheduler - INFO - [user_recommender] at py4j.Gateway.invoke(Gateway.java:282)
2025-05-12 17:29:30,878 - SparkScheduler - INFO - [user_recommender] at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2025-05-12 17:29:30,878 - SparkScheduler - INFO - [user_recommender] at py4j.commands.CallCommand.execute(CallCommand.java:79)
2025-05-12 17:29:30,878 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-05-12 17:29:30,879 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-05-12 17:29:30,879 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.Thread.run(Thread.java:840)
2025-05-12 17:29:30,880 - SparkScheduler - INFO - [user_recommender] 
2025-05-12 17:29:30,996 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 17:29:30,997 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-879e653b-c39c-4dc1-83ee-6ed57ba8ef9c
2025-05-12 17:29:31,010 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee939b4f-401b-4fd2-b683-9fdd4e5612c0
2025-05-12 17:29:31,023 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:29:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-879e653b-c39c-4dc1-83ee-6ed57ba8ef9c/pyspark-5771c398-9875-4bc6-bfd3-1153a7f38425
2025-05-12 17:29:31,075 - SparkScheduler - ERROR - [user_recommender] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 17:29:31,076 - SparkScheduler - ERROR - [user_recommender] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 17:29:31,076 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 17:29:31,077 - SparkScheduler - ERROR - [user_recommender] :: resolving dependencies :: org.apache.spark#spark-submit-parent-aebf4c27-26de-4ded-9615-6acae1aa5570;1.0
2025-05-12 17:29:31,077 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 17:29:31,077 - SparkScheduler - ERROR - [user_recommender] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 17:29:31,078 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 17:29:31,078 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#bson;4.0.5 in central
2025-05-12 17:29:31,079 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 17:29:31,079 - SparkScheduler - ERROR - [user_recommender] :: resolution report :: resolve 257ms :: artifacts dl 13ms
2025-05-12 17:29:31,080 - SparkScheduler - ERROR - [user_recommender] :: modules in use:
2025-05-12 17:29:31,081 - SparkScheduler - ERROR - [user_recommender] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 17:29:31,082 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 17:29:31,083 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 17:29:31,083 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 17:29:31,084 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 17:29:31,084 - SparkScheduler - ERROR - [user_recommender] |                  |            modules            ||   artifacts   |
2025-05-12 17:29:31,084 - SparkScheduler - ERROR - [user_recommender] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 17:29:31,085 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 17:29:31,085 - SparkScheduler - ERROR - [user_recommender] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 17:29:31,085 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 17:29:31,086 - SparkScheduler - ERROR - [user_recommender] :: retrieving :: org.apache.spark#spark-submit-parent-aebf4c27-26de-4ded-9615-6acae1aa5570
2025-05-12 17:29:31,086 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 17:29:31,086 - SparkScheduler - ERROR - [user_recommender] 0 artifacts copied, 4 already retrieved (0kB/9ms)
2025-05-12 17:29:31,086 - SparkScheduler - ERROR - Job user_recommender failed with exit code 1
2025-05-12 17:29:31,087 - SparkScheduler - INFO - Job user_recommender duration: 17.64 seconds
2025-05-12 17:29:31,087 - SparkScheduler - INFO - Starting job: content_analyzer - Analyze tweet content and topics
2025-05-12 17:29:31,088 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/content_analyzer.py
2025-05-12 17:29:36,460 - SparkScheduler - INFO - [content_analyzer] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 17:29:37,251 - SparkScheduler - INFO - [content_analyzer] 25/05/12 17:29:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 17:29:41,434 - SparkScheduler - INFO - [content_analyzer] [nltk_data] Downloading package punkt to /nltk_data...
2025-05-12 17:29:41,434 - SparkScheduler - INFO - [content_analyzer] Traceback (most recent call last):
2025-05-12 17:29:41,435 - SparkScheduler - INFO - [content_analyzer] File "/opt/spark-jobs/content_analyzer.py", line 11, in <module>
2025-05-12 17:29:41,437 - SparkScheduler - INFO - [content_analyzer] nltk.download('punkt')
2025-05-12 17:29:41,437 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 777, in download
2025-05-12 17:29:41,439 - SparkScheduler - INFO - [content_analyzer] for msg in self.incr_download(info_or_id, download_dir, force):
2025-05-12 17:29:41,440 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 642, in incr_download
2025-05-12 17:29:41,440 - SparkScheduler - INFO - [content_analyzer] yield from self._download_package(info, download_dir, force)
2025-05-12 17:29:41,441 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 699, in _download_package
2025-05-12 17:29:41,441 - SparkScheduler - INFO - [content_analyzer] os.makedirs(download_dir)
2025-05-12 17:29:41,441 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/os.py", line 225, in makedirs
2025-05-12 17:29:41,442 - SparkScheduler - INFO - [content_analyzer] mkdir(name, mode)
2025-05-12 17:29:41,442 - SparkScheduler - INFO - [content_analyzer] PermissionError: [Errno 13] Permission denied: '/nltk_data'
2025-05-12 17:29:41,654 - SparkScheduler - INFO - [content_analyzer] 25/05/12 17:29:41 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 17:29:41,656 - SparkScheduler - INFO - [content_analyzer] 25/05/12 17:29:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b58ec2f-2b72-472e-be3b-90d0e008980d
2025-05-12 17:29:41,698 - SparkScheduler - ERROR - [content_analyzer] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 17:29:41,698 - SparkScheduler - ERROR - [content_analyzer] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 17:29:41,699 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 17:29:41,700 - SparkScheduler - ERROR - [content_analyzer] :: resolving dependencies :: org.apache.spark#spark-submit-parent-038a01c6-1245-4f85-8b77-3b4564638161;1.0
2025-05-12 17:29:41,701 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 17:29:41,702 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 17:29:41,702 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 17:29:41,703 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#bson;4.0.5 in central
2025-05-12 17:29:41,703 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 17:29:41,703 - SparkScheduler - ERROR - [content_analyzer] :: resolution report :: resolve 328ms :: artifacts dl 16ms
2025-05-12 17:29:41,704 - SparkScheduler - ERROR - [content_analyzer] :: modules in use:
2025-05-12 17:29:41,704 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 17:29:41,704 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 17:29:41,705 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 17:29:41,705 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 17:29:41,705 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 17:29:41,706 - SparkScheduler - ERROR - [content_analyzer] |                  |            modules            ||   artifacts   |
2025-05-12 17:29:41,706 - SparkScheduler - ERROR - [content_analyzer] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 17:29:41,706 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 17:29:41,707 - SparkScheduler - ERROR - [content_analyzer] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 17:29:41,707 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 17:29:41,707 - SparkScheduler - ERROR - [content_analyzer] :: retrieving :: org.apache.spark#spark-submit-parent-038a01c6-1245-4f85-8b77-3b4564638161
2025-05-12 17:29:41,708 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 17:29:41,708 - SparkScheduler - ERROR - [content_analyzer] 0 artifacts copied, 4 already retrieved (0kB/11ms)
2025-05-12 17:29:41,708 - SparkScheduler - ERROR - Job content_analyzer failed with exit code 1
2025-05-12 17:29:41,709 - SparkScheduler - INFO - Job content_analyzer duration: 10.62 seconds
2025-05-12 17:29:41,710 - SparkScheduler - INFO - Scheduler running. Press Ctrl+C to exit.
2025-05-12 17:54:28,679 - SparkScheduler - INFO - Starting Mini Twitter Spark Scheduler
2025-05-12 17:54:28,680 - SparkScheduler - INFO - Waiting for services to be ready...
2025-05-12 17:55:01,922 - SparkScheduler - INFO - Setting up job schedules
2025-05-12 17:55:01,927 - SparkScheduler - INFO - Scheduled trend_analysis to run every 30 minutes
2025-05-12 17:55:01,928 - SparkScheduler - INFO - Scheduled user_recommender to run every 3 hours
2025-05-12 17:55:01,930 - SparkScheduler - INFO - Scheduled content_analyzer to run daily at 02:00
2025-05-12 17:55:01,931 - SparkScheduler - INFO - Scheduled discover_feed_generator to run every 15 minutes
2025-05-12 17:55:01,932 - SparkScheduler - INFO - All jobs scheduled
2025-05-12 17:55:01,932 - SparkScheduler - INFO - Running initial job executions...
2025-05-12 17:55:01,933 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 17:55:01,933 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 17:55:07,760 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 17:59:15,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 17:59:16,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 17:59:16,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO ResourceUtils: ==============================================================
2025-05-12 17:59:16,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 17:59:16,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO ResourceUtils: ==============================================================
2025-05-12 17:59:16,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 17:59:16,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 17:59:16,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 17:59:16,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 17:59:16,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO SecurityManager: Changing view acls to: spark
2025-05-12 17:59:16,635 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 17:59:16,636 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO SecurityManager: Changing view acls groups to:
2025-05-12 17:59:16,637 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 17:59:16,639 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 17:59:20,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:20 INFO Utils: Successfully started service 'sparkDriver' on port 45069.
2025-05-12 17:59:20,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:20 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 17:59:20,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:20 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 17:59:20,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 17:59:20,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 17:59:20,933 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 17:59:20,961 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ecb6bba0-a1f4-4632-ac68-4cd1919562fa
2025-05-12 17:59:20,982 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 17:59:21,007 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 17:59:21,303 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 17:59:21,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:45069/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747072756747
2025-05-12 17:59:21,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:45069/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747072756747
2025-05-12 17:59:21,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:45069/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747072756747
2025-05-12 17:59:21,379 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:45069/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747072756747
2025-05-12 17:59:21,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:45069/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747072756747
2025-05-12 17:59:21,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-5244e648-8623-4eaa-af0d-2673cc89d0ec/userFiles-ffc6cbae-9d07-4d33-b25a-c9e2849fe107/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 17:59:21,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:45069/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747072756747
2025-05-12 17:59:21,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-5244e648-8623-4eaa-af0d-2673cc89d0ec/userFiles-ffc6cbae-9d07-4d33-b25a-c9e2849fe107/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 17:59:21,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:45069/files/org.mongodb_bson-4.0.5.jar with timestamp 1747072756747
2025-05-12 17:59:21,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-5244e648-8623-4eaa-af0d-2673cc89d0ec/userFiles-ffc6cbae-9d07-4d33-b25a-c9e2849fe107/org.mongodb_bson-4.0.5.jar
2025-05-12 17:59:21,451 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:45069/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747072756747
2025-05-12 17:59:21,452 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-5244e648-8623-4eaa-af0d-2673cc89d0ec/userFiles-ffc6cbae-9d07-4d33-b25a-c9e2849fe107/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 17:59:21,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 17:59:21,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 88 ms (0 ms spent in bootstraps)
2025-05-12 17:59:21,981 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512175921-0000
2025-05-12 17:59:21,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36485.
2025-05-12 17:59:21,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:21 INFO NettyBlockTransferService: Server created on 1939bc4966ae:36485
2025-05-12 17:59:22,002 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 17:59:22,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1939bc4966ae, 36485, None)
2025-05-12 17:59:22,046 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512175921-0000/0 on worker-20250512175431-172.23.0.8-44645 (172.23.0.8:44645) with 2 core(s)
2025-05-12 17:59:22,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512175921-0000/0 on hostPort 172.23.0.8:44645 with 2 core(s), 1024.0 MiB RAM
2025-05-12 17:59:22,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO BlockManagerMasterEndpoint: Registering block manager 1939bc4966ae:36485 with 434.4 MiB RAM, BlockManagerId(driver, 1939bc4966ae, 36485, None)
2025-05-12 17:59:22,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1939bc4966ae, 36485, None)
2025-05-12 17:59:22,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1939bc4966ae, 36485, None)
2025-05-12 17:59:22,392 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512175921-0000/0 is now RUNNING
2025-05-12 17:59:22,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 17:59:23,010 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 17:59:23,023 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 17:59:23,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:23 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 17:59:24,953 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 17:59:25,024 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 17:59:25,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1939bc4966ae:36485 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:25,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:25 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:26,567 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:57386) with ID 0,  ResourceProfileId 0
2025-05-12 17:59:26,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:46163 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 46163, None)
2025-05-12 17:59:28,438 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:28,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:28,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:28,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:28 INFO connection: Opened connection [connectionId{localValue:1, serverValue:640}] to mongodb:27017
2025-05-12 17:59:28,495 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3270862}
2025-05-12 17:59:28,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:28 INFO connection: Opened connection [connectionId{localValue:2, serverValue:641}] to mongodb:27017
2025-05-12 17:59:28,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:28 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:59:29,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO CodeGenerator: Code generated in 235.707653 ms
2025-05-12 17:59:29,574 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:29,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:29,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO connection: Opened connection [connectionId{localValue:3, serverValue:642}] to mongodb:27017
2025-05-12 17:59:29,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1072835}
2025-05-12 17:59:29,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO connection: Opened connection [connectionId{localValue:4, serverValue:643}] to mongodb:27017
2025-05-12 17:59:29,656 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 17:59:29,662 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:59:29,663 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 17:59:29,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:29,665 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:29,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:29,701 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 17:59:29,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 17:59:29,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1939bc4966ae:36485 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:29,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:29,737 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:29,739 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 17:59:29,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:30,186 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:46163 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:30,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:46163 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:33,044 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3279 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:33,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 17:59:33,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 3.373 s
2025-05-12 17:59:33,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:33,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: running: Set()
2025-05-12 17:59:33,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:33,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:33,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:33,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO CodeGenerator: Code generated in 17.35043 ms
2025-05-12 17:59:33,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:33,187 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO CodeGenerator: Code generated in 22.870162 ms
2025-05-12 17:59:33,278 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:59:33,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:59:33,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:59:33,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 17:59:33,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:33,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:33,302 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 17:59:33,312 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 17:59:33,315 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1939bc4966ae:36485 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:33,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:33,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:33,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 17:59:33,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:33,358 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1939bc4966ae:36485 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:33,365 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:46163 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:33,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:46163 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:33,548 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:33,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO connection: Closed connection [connectionId{localValue:2, serverValue:641}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:33,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:57386
2025-05-12 17:59:33,987 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 665 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:33,988 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 17:59:33,989 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 0.694 s
2025-05-12 17:59:33,993 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:33,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 17:59:33,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:33 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 0.719170 s
2025-05-12 17:59:34,008 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:59:34,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:59:34,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1939bc4966ae:36485 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:59:34,024 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:34,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 1939bc4966ae:36485 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:34,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:46163 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:34,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:34,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:34,088 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:34,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Opened connection [connectionId{localValue:5, serverValue:649}] to mongodb:27017
2025-05-12 17:59:34,091 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1063910}
2025-05-12 17:59:34,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Opened connection [connectionId{localValue:6, serverValue:650}] to mongodb:27017
2025-05-12 17:59:34,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:59:34,220 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO CodeGenerator: Code generated in 35.436717 ms
2025-05-12 17:59:34,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 17:59:34,240 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:59:34,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 17:59:34,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:34,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:34,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:34,246 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 17:59:34,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 17:59:34,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1939bc4966ae:36485 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:34,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:34,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Closed connection [connectionId{localValue:4, serverValue:643}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:34,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:34,260 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:34,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Closed connection [connectionId{localValue:6, serverValue:650}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:34,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:34,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 17:59:34,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1939bc4966ae:36485 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:59:34,264 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:34,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:46163 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:34,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 157 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:34,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 17:59:34,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.177 s
2025-05-12 17:59:34,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:34,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: running: Set()
2025-05-12 17:59:34,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:34,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:34,429 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:34,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:34,461 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:59:34,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:59:34,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:59:34,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 17:59:34,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:34,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:34,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 17:59:34,477 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 17:59:34,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1939bc4966ae:36485 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:34,479 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:34,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:34,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 17:59:34,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:34,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:46163 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:34,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:57386
2025-05-12 17:59:34,574 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 92 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:34,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 17:59:34,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.107 s
2025-05-12 17:59:34,577 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:34,578 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 17:59:34,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.115959 s
2025-05-12 17:59:34,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:59:34,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:59:34,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 1939bc4966ae:36485 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:59:34,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:34,614 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:34,615 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:34,616 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:34,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Opened connection [connectionId{localValue:7, serverValue:651}] to mongodb:27017
2025-05-12 17:59:34,619 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=963617}
2025-05-12 17:59:34,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Opened connection [connectionId{localValue:8, serverValue:652}] to mongodb:27017
2025-05-12 17:59:34,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:34,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Closed connection [connectionId{localValue:8, serverValue:652}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:34,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:59:34,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:34,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO CodeGenerator: Code generated in 41.075646 ms
2025-05-12 17:59:34,822 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:34,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:34,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:34,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Opened connection [connectionId{localValue:9, serverValue:653}] to mongodb:27017
2025-05-12 17:59:34,826 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=871349}
2025-05-12 17:59:34,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Opened connection [connectionId{localValue:10, serverValue:654}] to mongodb:27017
2025-05-12 17:59:34,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:34,830 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Closed connection [connectionId{localValue:10, serverValue:654}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:34,831 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:34,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:34,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:34,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Opened connection [connectionId{localValue:11, serverValue:655}] to mongodb:27017
2025-05-12 17:59:34,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=944648}
2025-05-12 17:59:34,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Opened connection [connectionId{localValue:12, serverValue:656}] to mongodb:27017
2025-05-12 17:59:34,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:34,848 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO connection: Closed connection [connectionId{localValue:12, serverValue:656}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:34,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:34,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:34,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:34,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 17:59:34,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:59:34,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 17:59:34,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:34,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:34,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:34,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.2 MiB)
2025-05-12 17:59:34,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 17:59:34,867 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 1939bc4966ae:36485 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:59:34,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:34,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:34,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 17:59:34,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:34,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:46163 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:59:35,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 182 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:35,054 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 17:59:35,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.195 s
2025-05-12 17:59:35,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:35,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: running: Set()
2025-05-12 17:59:35,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:35,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:35,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:35,074 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO CodeGenerator: Code generated in 6.81783 ms
2025-05-12 17:59:35,076 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:35,107 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO CodeGenerator: Code generated in 24.020772 ms
2025-05-12 17:59:35,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:59:35,138 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:59:35,138 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:59:35,139 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 17:59:35,140 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:35,141 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:35,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.1 MiB)
2025-05-12 17:59:35,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.1 MiB)
2025-05-12 17:59:35,165 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 1939bc4966ae:36485 (size: 27.2 KiB, free: 434.3 MiB)
2025-05-12 17:59:35,172 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:35,173 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:35,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 17:59:35,175 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 1939bc4966ae:36485 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:59:35,175 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:35,180 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:46163 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 1939bc4966ae:36485 in memory (size: 24.4 KiB, free: 434.3 MiB)
2025-05-12 17:59:35,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:46163 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:46163 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,214 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 1939bc4966ae:36485 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:46163 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 1939bc4966ae:36485 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:59:35,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:57386
2025-05-12 17:59:35,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 218 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:35,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 17:59:35,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.246 s
2025-05-12 17:59:35,392 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:35,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 17:59:35,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.256317 s
2025-05-12 17:59:35,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:59:35,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:59:35,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 1939bc4966ae:36485 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:59:35,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:35,460 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 1939bc4966ae:36485 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:59:35,471 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 1939bc4966ae:36485 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:46163 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,479 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:59:35,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:35,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:35,600 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:35,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO connection: Opened connection [connectionId{localValue:14, serverValue:658}] to mongodb:27017
2025-05-12 17:59:35,603 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1007758}
2025-05-12 17:59:35,605 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO connection: Opened connection [connectionId{localValue:15, serverValue:659}] to mongodb:27017
2025-05-12 17:59:35,606 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:35,607 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO connection: Closed connection [connectionId{localValue:15, serverValue:659}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:35,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:35,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:35,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:35,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO connection: Opened connection [connectionId{localValue:16, serverValue:660}] to mongodb:27017
2025-05-12 17:59:35,613 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=939002}
2025-05-12 17:59:35,615 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO connection: Opened connection [connectionId{localValue:17, serverValue:661}] to mongodb:27017
2025-05-12 17:59:35,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:35,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO connection: Closed connection [connectionId{localValue:17, serverValue:661}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:35,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:35,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:35,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:35,631 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 17:59:35,631 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:35,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:35,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:35,633 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:35,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:35,636 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.4 MiB)
2025-05-12 17:59:35,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 17:59:35,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 1939bc4966ae:36485 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,646 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:35,647 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:35,647 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 17:59:35,649 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:35,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:46163 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 94 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:35,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 17:59:35,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.112 s
2025-05-12 17:59:35,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:35,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: running: Set()
2025-05-12 17:59:35,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:35,750 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:35,760 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:35,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO CodeGenerator: Code generated in 12.591707 ms
2025-05-12 17:59:35,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO CodeGenerator: Code generated in 5.69548 ms
2025-05-12 17:59:35,809 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:35,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO CodeGenerator: Code generated in 14.332693 ms
2025-05-12 17:59:35,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:59:35,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:35,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:35,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 17:59:35,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:35,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:35,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 17:59:35,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 17:59:35,869 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 1939bc4966ae:36485 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:35,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:35,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 17:59:35,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:35,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:46163 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:35,925 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:57386
2025-05-12 17:59:35,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 101 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:35,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 17:59:35,976 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.113 s
2025-05-12 17:59:35,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:35,977 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 17:59:35,978 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:35 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.120361 s
2025-05-12 17:59:36,036 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:59:36,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:36,087 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:36,088 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:36,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:19, serverValue:663}] to mongodb:27017
2025-05-12 17:59:36,092 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1559651}
2025-05-12 17:59:36,094 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:20, serverValue:664}] to mongodb:27017
2025-05-12 17:59:36,096 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:36,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Closed connection [connectionId{localValue:20, serverValue:664}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:36,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:36,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:36,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:36,101 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:21, serverValue:665}] to mongodb:27017
2025-05-12 17:59:36,102 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=807904}
2025-05-12 17:59:36,105 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:22, serverValue:666}] to mongodb:27017
2025-05-12 17:59:36,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:36,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Closed connection [connectionId{localValue:22, serverValue:666}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:36,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:36,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:36,116 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:36,119 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 17:59:36,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:36,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:36,122 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:36,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:36,124 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:36,126 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.2 MiB)
2025-05-12 17:59:36,128 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.2 MiB)
2025-05-12 17:59:36,130 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 1939bc4966ae:36485 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,131 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:36,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:36,133 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 17:59:36,134 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:36,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:46163 (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 89 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:36,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 17:59:36,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.102 s
2025-05-12 17:59:36,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:36,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: running: Set()
2025-05-12 17:59:36,228 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:36,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:36,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:36,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:36,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:59:36,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:36,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:36,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 17:59:36,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:36,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:36,290 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.1 MiB)
2025-05-12 17:59:36,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.1 MiB)
2025-05-12 17:59:36,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 1939bc4966ae:36485 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:36,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:36,296 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 17:59:36,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:36,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:46163 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:57386
2025-05-12 17:59:36,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 58 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:36,357 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 17:59:36,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.072 s
2025-05-12 17:59:36,361 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:36,362 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 17:59:36,363 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.080014 s
2025-05-12 17:59:36,425 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 17:59:36,457 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:36,477 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:36,479 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:36,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:36,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:24, serverValue:668}] to mongodb:27017
2025-05-12 17:59:36,484 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=969463}
2025-05-12 17:59:36,486 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:25, serverValue:669}] to mongodb:27017
2025-05-12 17:59:36,488 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:36,488 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Closed connection [connectionId{localValue:25, serverValue:669}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:36,489 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:36,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:36,490 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:36,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:26, serverValue:670}] to mongodb:27017
2025-05-12 17:59:36,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=877493}
2025-05-12 17:59:36,495 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:27, serverValue:671}] to mongodb:27017
2025-05-12 17:59:36,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:36,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Closed connection [connectionId{localValue:27, serverValue:671}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:36,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:36,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:36,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:36,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 17:59:36,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:36,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:36,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:36,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:36,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:36,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.1 MiB)
2025-05-12 17:59:36,524 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.0 MiB)
2025-05-12 17:59:36,527 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 1939bc4966ae:36485 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 1939bc4966ae:36485 in memory (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:36,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:36,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 17:59:36,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:46163 in memory (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,533 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:36,544 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 1939bc4966ae:36485 in memory (size: 21.6 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,550 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:46163 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:46163 (size: 23.5 KiB, free: 434.3 MiB)
2025-05-12 17:59:36,570 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 1939bc4966ae:36485 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,573 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:46163 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 1939bc4966ae:36485 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,607 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:46163 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,638 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 106 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:36,639 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 17:59:36,641 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.125 s
2025-05-12 17:59:36,641 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:36,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: running: Set()
2025-05-12 17:59:36,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:36,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:36,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:36,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:36,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO CodeGenerator: Code generated in 19.664955 ms
2025-05-12 17:59:36,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 1939bc4966ae:36485 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:46163 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:59:36,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:36,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:36,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 17:59:36,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:36,720 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:36,724 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.3 MiB)
2025-05-12 17:59:36,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.3 MiB)
2025-05-12 17:59:36,731 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 1939bc4966ae:36485 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:36,733 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:36,734 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 17:59:36,736 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:36,760 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:46163 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,785 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:57386
2025-05-12 17:59:36,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 86 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:36,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 17:59:36,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.103 s
2025-05-12 17:59:36,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:36,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 17:59:36,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.110046 s
2025-05-12 17:59:36,829 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 5, 'weekly': 5, 'hourly': 10}
2025-05-12 17:59:36,838 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 17:59:36,842 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 17:59:36,843 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 1939bc4966ae:36485 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:36,844 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:36,850 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:36,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:36,852 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:36,853 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:29, serverValue:673}] to mongodb:27017
2025-05-12 17:59:36,855 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1453444}
2025-05-12 17:59:36,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Opened connection [connectionId{localValue:30, serverValue:674}] to mongodb:27017
2025-05-12 17:59:36,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:36,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO connection: Closed connection [connectionId{localValue:30, serverValue:674}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:36,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:36,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:36,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:36,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:59:36,891 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:59:36,891 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:59:36,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:36,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:36,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:59:36,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.3 MiB)
2025-05-12 17:59:36,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.3 MiB)
2025-05-12 17:59:36,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 1939bc4966ae:36485 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:36,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:36,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 17:59:36,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:59:36,920 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:46163 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:36,956 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:36 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:46163 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:37,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 113 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:37,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 17:59:37,018 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.125 s
2025-05-12 17:59:37,020 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:37,021 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 17:59:37,023 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.132991 s
2025-05-12 17:59:37,149 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:37,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:37,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:37,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Opened connection [connectionId{localValue:32, serverValue:676}] to mongodb:27017
2025-05-12 17:59:37,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=671145}
2025-05-12 17:59:37,155 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Opened connection [connectionId{localValue:33, serverValue:677}] to mongodb:27017
2025-05-12 17:59:37,158 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:37,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Closed connection [connectionId{localValue:33, serverValue:677}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:37,184 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 17:59:37,244 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO CodeGenerator: Code generated in 32.952956 ms
2025-05-12 17:59:37,248 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:37,249 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:37,250 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:37,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Opened connection [connectionId{localValue:34, serverValue:678}] to mongodb:27017
2025-05-12 17:59:37,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=892456}
2025-05-12 17:59:37,254 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Opened connection [connectionId{localValue:35, serverValue:679}] to mongodb:27017
2025-05-12 17:59:37,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:37,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Closed connection [connectionId{localValue:35, serverValue:679}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:37,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:37,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:37,260 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:37,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Opened connection [connectionId{localValue:36, serverValue:680}] to mongodb:27017
2025-05-12 17:59:37,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1413967}
2025-05-12 17:59:37,265 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Opened connection [connectionId{localValue:37, serverValue:681}] to mongodb:27017
2025-05-12 17:59:37,272 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:37,272 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO connection: Closed connection [connectionId{localValue:37, serverValue:681}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:37,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:37,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:37,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:37,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 17:59:37,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:59:37,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 17:59:37,281 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:37,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:37,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:37,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.5 KiB, free 434.3 MiB)
2025-05-12 17:59:37,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.2 MiB)
2025-05-12 17:59:37,287 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 1939bc4966ae:36485 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,288 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:37,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:37,290 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 17:59:37,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:37,311 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:46163 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 150 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:37,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 17:59:37,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.162 s
2025-05-12 17:59:37,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:37,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: running: Set()
2025-05-12 17:59:37,447 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:37,448 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:37,459 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:37,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:37,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO CodeGenerator: Code generated in 17.971155 ms
2025-05-12 17:59:37,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO CodeGenerator: Code generated in 5.478733 ms
2025-05-12 17:59:37,550 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO SparkContext: Starting job: rdd at MongoSpark.scala:169
2025-05-12 17:59:37,551 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Got job 14 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:59:37,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Final stage: ResultStage 21 (rdd at MongoSpark.scala:169)
2025-05-12 17:59:37,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 17:59:37,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:37,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[84] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:37,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 45.8 KiB, free 434.2 MiB)
2025-05-12 17:59:37,562 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 434.2 MiB)
2025-05-12 17:59:37,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 1939bc4966ae:36485 (size: 21.0 KiB, free: 434.3 MiB)
2025-05-12 17:59:37,564 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:37,565 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[84] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:37,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 17:59:37,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:37,587 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:46163 (size: 21.0 KiB, free: 434.3 MiB)
2025-05-12 17:59:37,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:57386
2025-05-12 17:59:37,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 105 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:37,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 17:59:37,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: ResultStage 21 (rdd at MongoSpark.scala:169) finished in 0.119 s
2025-05-12 17:59:37,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:37,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 17:59:37,675 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Job 14 finished: rdd at MongoSpark.scala:169, took 0.124546 s
2025-05-12 17:59:37,688 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Registering RDD 85 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 17:59:37,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:59:37,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (rdd at MongoSpark.scala:169)
2025-05-12 17:59:37,690 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
2025-05-12 17:59:37,690 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:37,691 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[85] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:37,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 46.0 KiB, free 434.1 MiB)
2025-05-12 17:59:37,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 21.2 KiB, free 434.1 MiB)
2025-05-12 17:59:37,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 1939bc4966ae:36485 (size: 21.2 KiB, free: 434.3 MiB)
2025-05-12 17:59:37,703 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:37,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[85] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:37,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
2025-05-12 17:59:37,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 15) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 17:59:37,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.23.0.8:46163 (size: 21.2 KiB, free: 434.3 MiB)
2025-05-12 17:59:37,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 15) in 57 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:37,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
2025-05-12 17:59:37,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: ShuffleMapStage 23 (rdd at MongoSpark.scala:169) finished in 0.074 s
2025-05-12 17:59:37,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:37,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: running: Set()
2025-05-12 17:59:37,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:37,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:37,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:37,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 1939bc4966ae:36485 in memory (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 17:59:37,797 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:46163 in memory (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 17:59:37,806 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 1939bc4966ae:36485 in memory (size: 20.0 KiB, free: 434.3 MiB)
2025-05-12 17:59:37,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO CodeGenerator: Code generated in 25.423424 ms
2025-05-12 17:59:37,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:46163 in memory (size: 20.0 KiB, free: 434.3 MiB)
2025-05-12 17:59:37,822 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 1939bc4966ae:36485 in memory (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,825 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:46163 in memory (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,837 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 1939bc4966ae:36485 in memory (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,841 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.23.0.8:46163 in memory (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,843 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:59:37,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:59:37,847 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Final stage: ResultStage 26 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:59:37,850 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
2025-05-12 17:59:37,851 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:37,853 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[91] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:37,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 51.9 KiB, free 434.3 MiB)
2025-05-12 17:59:37,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 22.9 KiB, free 434.2 MiB)
2025-05-12 17:59:37,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 1939bc4966ae:36485 (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:37,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 1939bc4966ae:36485 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[91] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:37,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:46163 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
2025-05-12 17:59:37,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:37,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:46163 (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:59:37,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:57386
2025-05-12 17:59:38,016 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 16) in 143 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:38,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
2025-05-12 17:59:38,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: ResultStage 26 (foreachPartition at MongoSpark.scala:120) finished in 0.168 s
2025-05-12 17:59:38,018 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:38,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
2025-05-12 17:59:38,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.175045 s
2025-05-12 17:59:38,023 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 17:59:38,026 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 17:59:38,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 1939bc4966ae:36485 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 17:59:38,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Created broadcast 22 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:38,047 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:38,048 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:38,049 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:38,050 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:38, serverValue:682}] to mongodb:27017
2025-05-12 17:59:38,051 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=687902}
2025-05-12 17:59:38,053 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:39, serverValue:683}] to mongodb:27017
2025-05-12 17:59:38,055 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:38,056 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Closed connection [connectionId{localValue:39, serverValue:683}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:38,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 17:59:38,132 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO CodeGenerator: Code generated in 41.799806 ms
2025-05-12 17:59:38,136 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:38,137 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:38,138 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:38,139 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:40, serverValue:684}] to mongodb:27017
2025-05-12 17:59:38,140 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=801213}
2025-05-12 17:59:38,142 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:41, serverValue:685}] to mongodb:27017
2025-05-12 17:59:38,144 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:38,144 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Closed connection [connectionId{localValue:41, serverValue:685}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:38,145 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:38,146 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:38,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:38,148 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:42, serverValue:686}] to mongodb:27017
2025-05-12 17:59:38,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1119448}
2025-05-12 17:59:38,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:43, serverValue:687}] to mongodb:27017
2025-05-12 17:59:38,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:38,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Closed connection [connectionId{localValue:43, serverValue:687}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:38,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:38,163 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:38,164 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:38,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Registering RDD 97 (rdd at MongoSpark.scala:169) as input to shuffle 8
2025-05-12 17:59:38,168 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Got map stage job 17 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 17:59:38,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (rdd at MongoSpark.scala:169)
2025-05-12 17:59:38,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:38,170 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:38,171 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[97] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:38,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 39.6 KiB, free 434.3 MiB)
2025-05-12 17:59:38,177 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 17:59:38,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 1939bc4966ae:36485 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,179 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:38,180 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[97] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:38,181 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 17:59:38,184 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:38,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.23.0.8:46163 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 17) in 136 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:38,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 17:59:38,321 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: ShuffleMapStage 27 (rdd at MongoSpark.scala:169) finished in 0.150 s
2025-05-12 17:59:38,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:38,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: running: Set()
2025-05-12 17:59:38,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:38,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:38,334 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:38,345 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:38,394 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 17:59:38,395 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Got job 18 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 17:59:38,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Final stage: ResultStage 29 (foreachPartition at MongoSpark.scala:120)
2025-05-12 17:59:38,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
2025-05-12 17:59:38,399 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:38,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[104] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 17:59:38,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.9 KiB, free 434.2 MiB)
2025-05-12 17:59:38,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 434.2 MiB)
2025-05-12 17:59:38,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 1939bc4966ae:36485 (size: 22.2 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:38,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[104] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:38,412 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
2025-05-12 17:59:38,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:38,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:46163 (size: 22.2 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,460 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:57386
2025-05-12 17:59:38,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 18) in 92 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:38,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
2025-05-12 17:59:38,507 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: ResultStage 29 (foreachPartition at MongoSpark.scala:120) finished in 0.107 s
2025-05-12 17:59:38,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:38,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
2025-05-12 17:59:38,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Job 18 finished: foreachPartition at MongoSpark.scala:120, took 0.114543 s
2025-05-12 17:59:38,513 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 200.0 B, free 434.2 MiB)
2025-05-12 17:59:38,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.2 MiB)
2025-05-12 17:59:38,516 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 1939bc4966ae:36485 (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:59:38,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Created broadcast 25 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:38,539 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 17:59:38,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:38,586 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO CodeGenerator: Code generated in 18.906841 ms
2025-05-12 17:59:38,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:38,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:38,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:38,592 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:45, serverValue:689}] to mongodb:27017
2025-05-12 17:59:38,593 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1045597}
2025-05-12 17:59:38,595 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:46, serverValue:690}] to mongodb:27017
2025-05-12 17:59:38,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:38,597 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Closed connection [connectionId{localValue:46, serverValue:690}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:38,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:38,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:38,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:38,600 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:47, serverValue:691}] to mongodb:27017
2025-05-12 17:59:38,602 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=873128}
2025-05-12 17:59:38,603 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:48, serverValue:692}] to mongodb:27017
2025-05-12 17:59:38,610 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:38,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Closed connection [connectionId{localValue:48, serverValue:692}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:38,612 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:38,613 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:38,613 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:38,616 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Registering RDD 110 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 17:59:38,616 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Got map stage job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:38,617 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:38,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:38,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:38,618 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:38,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 36.4 KiB, free 434.2 MiB)
2025-05-12 17:59:38,622 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 434.1 MiB)
2025-05-12 17:59:38,623 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 1939bc4966ae:36485 (size: 16.9 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,624 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:38,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:38,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 17:59:38,627 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:38,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:46163 (size: 16.9 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 65 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:38,693 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 17:59:38,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: ShuffleMapStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.075 s
2025-05-12 17:59:38,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:38,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: running: Set()
2025-05-12 17:59:38,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:38,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:38,701 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:38,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:38,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 1939bc4966ae:36485 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:59:38,735 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO CodeGenerator: Code generated in 22.88974 ms
2025-05-12 17:59:38,745 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 1939bc4966ae:36485 in memory (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,746 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Registering RDD 113 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 17:59:38,747 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:38,748 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:38,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
2025-05-12 17:59:38,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:38,750 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:38,751 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.23.0.8:46163 in memory (size: 18.6 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 45.0 KiB, free 434.2 MiB)
2025-05-12 17:59:38,760 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.1 MiB)
2025-05-12 17:59:38,761 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 1939bc4966ae:36485 in memory (size: 410.0 B, free: 434.3 MiB)
2025-05-12 17:59:38,762 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 1939bc4966ae:36485 (size: 20.4 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:38,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:38,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
2025-05-12 17:59:38,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 20) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 17:59:38,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 1939bc4966ae:36485 in memory (size: 16.9 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,785 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.23.0.8:46163 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,789 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:46163 (size: 20.4 KiB, free: 434.3 MiB)
2025-05-12 17:59:38,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 1939bc4966ae:36485 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,806 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:46163 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:57386
2025-05-12 17:59:38,815 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 1939bc4966ae:36485 in memory (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:46163 in memory (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,856 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 20) in 89 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:38,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
2025-05-12 17:59:38,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0) finished in 0.108 s
2025-05-12 17:59:38,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:38,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: running: Set()
2025-05-12 17:59:38,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:38,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:38,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO CodeGenerator: Code generated in 7.31986 ms
2025-05-12 17:59:38,888 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:59:38,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:38,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Final stage: ResultStage 35 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:38,890 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
2025-05-12 17:59:38,891 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:38,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:38,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 11.1 KiB, free 434.3 MiB)
2025-05-12 17:59:38,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)
2025-05-12 17:59:38,897 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 1939bc4966ae:36485 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:38,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:38,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 17:59:38,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:38,916 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:46163 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:38,922 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:57386
2025-05-12 17:59:38,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 21) in 41 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:38,942 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 17:59:38,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: ResultStage 35 (count at NativeMethodAccessorImpl.java:0) finished in 0.051 s
2025-05-12 17:59:38,944 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:38,945 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 17:59:38,945 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.056248 s
2025-05-12 17:59:38,965 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 17:59:38,994 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:38,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:38,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:38,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO connection: Opened connection [connectionId{localValue:50, serverValue:694}] to mongodb:27017
2025-05-12 17:59:38,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=938929}
2025-05-12 17:59:39,001 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Opened connection [connectionId{localValue:51, serverValue:695}] to mongodb:27017
2025-05-12 17:59:39,003 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:39,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Closed connection [connectionId{localValue:51, serverValue:695}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:39,004 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:39,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:39,009 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:39,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Opened connection [connectionId{localValue:52, serverValue:696}] to mongodb:27017
2025-05-12 17:59:39,010 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=955609}
2025-05-12 17:59:39,012 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Opened connection [connectionId{localValue:53, serverValue:697}] to mongodb:27017
2025-05-12 17:59:39,019 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:39,021 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Closed connection [connectionId{localValue:53, serverValue:697}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:39,022 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:39,025 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:39,027 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:39,029 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Registering RDD 121 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 11
2025-05-12 17:59:39,030 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Got map stage job 22 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:39,031 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:39,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:39,032 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:39,033 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[121] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:39,035 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 39.7 KiB, free 434.3 MiB)
2025-05-12 17:59:39,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 17:59:39,038 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 1939bc4966ae:36485 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:39,039 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[121] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:39,040 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
2025-05-12 17:59:39,042 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 17:59:39,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:46163 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,108 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 22) in 66 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:39,108 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
2025-05-12 17:59:39,109 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: ShuffleMapStage 36 (count at NativeMethodAccessorImpl.java:0) finished in 0.076 s
2025-05-12 17:59:39,110 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 17:59:39,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: running: Set()
2025-05-12 17:59:39,114 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: waiting: Set()
2025-05-12 17:59:39,115 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: failed: Set()
2025-05-12 17:59:39,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO ShufflePartitionsUtil: For shuffle(11), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 17:59:39,133 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 17:59:39,150 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 17:59:39,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Got job 23 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 17:59:39,152 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Final stage: ResultStage 38 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 17:59:39,153 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
2025-05-12 17:59:39,153 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:39,154 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[126] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 17:59:39,157 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 46.3 KiB, free 434.2 MiB)
2025-05-12 17:59:39,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.2 MiB)
2025-05-12 17:59:39,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 1939bc4966ae:36485 (size: 21.1 KiB, free: 434.3 MiB)
2025-05-12 17:59:39,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:39,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[126] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:39,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
2025-05-12 17:59:39,164 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 23) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 17:59:39,181 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:46163 (size: 21.1 KiB, free: 434.3 MiB)
2025-05-12 17:59:39,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 172.23.0.8:57386
2025-05-12 17:59:39,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 23) in 45 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:39,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
2025-05-12 17:59:39,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: ResultStage 38 (count at NativeMethodAccessorImpl.java:0) finished in 0.055 s
2025-05-12 17:59:39,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:39,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
2025-05-12 17:59:39,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Job 23 finished: count at NativeMethodAccessorImpl.java:0, took 0.060977 s
2025-05-12 17:59:39,213 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 2, 'active_users': 2}
2025-05-12 17:59:39,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.2 MiB)
2025-05-12 17:59:39,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.2 MiB)
2025-05-12 17:59:39,228 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 1939bc4966ae:36485 (size: 401.0 B, free: 434.3 MiB)
2025-05-12 17:59:39,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:39,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:39,231 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:39,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:39,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Opened connection [connectionId{localValue:55, serverValue:699}] to mongodb:27017
2025-05-12 17:59:39,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=887848}
2025-05-12 17:59:39,236 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Opened connection [connectionId{localValue:56, serverValue:700}] to mongodb:27017
2025-05-12 17:59:39,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:39,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Closed connection [connectionId{localValue:56, serverValue:700}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:39,242 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:39,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:39,244 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:39,254 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:59:39,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Got job 24 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:59:39,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Final stage: ResultStage 39 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:59:39,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:39,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:39,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[131] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:59:39,259 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.2 MiB)
2025-05-12 17:59:39,265 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.2 MiB)
2025-05-12 17:59:39,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 1939bc4966ae:36485 (size: 4.0 KiB, free: 434.3 MiB)
2025-05-12 17:59:39,269 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:39,271 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[131] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:39,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 1939bc4966ae:36485 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,274 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
2025-05-12 17:59:39,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.23.0.8:46163 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,276 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 24) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:59:39,285 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 1939bc4966ae:36485 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,288 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.23.0.8:46163 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 1939bc4966ae:36485 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.23.0.8:46163 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,300 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:46163 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,306 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 1939bc4966ae:36485 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,310 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.23.0.8:46163 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,323 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:46163 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:39,346 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 24) in 71 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:39,346 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
2025-05-12 17:59:39,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: ResultStage 39 (treeAggregate at MongoInferSchema.scala:88) finished in 0.090 s
2025-05-12 17:59:39,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:39,349 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
2025-05-12 17:59:39,350 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Job 24 finished: treeAggregate at MongoInferSchema.scala:88, took 0.094603 s
2025-05-12 17:59:39,367 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 17:59:39,370 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 17:59:39,371 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 1939bc4966ae:36485 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:39,372 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Created broadcast 33 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:39,374 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:39,375 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:39,376 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:39,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Opened connection [connectionId{localValue:58, serverValue:702}] to mongodb:27017
2025-05-12 17:59:39,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=837234}
2025-05-12 17:59:39,380 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Opened connection [connectionId{localValue:59, serverValue:703}] to mongodb:27017
2025-05-12 17:59:39,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:39,383 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO connection: Closed connection [connectionId{localValue:59, serverValue:703}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:39,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:39,389 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:39,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:39,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 1939bc4966ae:36485 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.23.0.8:46163 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,410 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:59:39,411 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Got job 25 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:59:39,412 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Final stage: ResultStage 40 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:59:39,412 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:39,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:39,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[136] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:59:39,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 17:59:39,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 17:59:39,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 1939bc4966ae:36485 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:39,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[136] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:39,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
2025-05-12 17:59:39,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 25) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:59:39,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.23.0.8:46163 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:39,450 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.23.0.8:46163 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:39,461 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 25) in 40 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:39,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
2025-05-12 17:59:39,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: ResultStage 40 (treeAggregate at MongoInferSchema.scala:88) finished in 0.049 s
2025-05-12 17:59:39,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:39,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
2025-05-12 17:59:39,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO DAGScheduler: Job 25 finished: treeAggregate at MongoInferSchema.scala:88, took 0.053516 s
2025-05-12 17:59:39,508 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkUI: Stopped Spark web UI at http://1939bc4966ae:4040
2025-05-12 17:59:39,511 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 17:59:39,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 17:59:39,530 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 17:59:39,566 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO MemoryStore: MemoryStore cleared
2025-05-12 17:59:39,568 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManager: BlockManager stopped
2025-05-12 17:59:39,573 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 17:59:39,577 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 17:59:39,620 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 17:59:39,697 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 17:59:39,698 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 17:59:39,702 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 17:59:39,710 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 17:59:39,713 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 17:59:39,713 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 17:59:39,717 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 17:59:39,718 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 17:59:39,721 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 17:59:39,724 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 17:59:39,745 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 17:59:39,817 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 17:59:39,818 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-5244e648-8623-4eaa-af0d-2673cc89d0ec
2025-05-12 17:59:39,827 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-5244e648-8623-4eaa-af0d-2673cc89d0ec/pyspark-75d4d792-6aea-4da2-83c1-10748c7961a6
2025-05-12 17:59:39,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 17:59:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-5ad4e258-9b78-4462-a2ff-5f58def7ce9a
2025-05-12 17:59:39,886 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 17:59:39,886 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 17:59:39,887 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 17:59:39,887 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-197bfca2-0433-4a3c-a819-5952595494b7;1.0
2025-05-12 17:59:39,887 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 17:59:39,888 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 17:59:39,888 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 17:59:39,888 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 17:59:39,889 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 17:59:39,889 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...
2025-05-12 17:59:39,890 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (42695ms)
2025-05-12 17:59:39,890 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...
2025-05-12 17:59:39,890 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (10922ms)
2025-05-12 17:59:39,890 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...
2025-05-12 17:59:39,891 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (54088ms)
2025-05-12 17:59:39,891 - SparkScheduler - ERROR - [trend_analysis] downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...
2025-05-12 17:59:39,892 - SparkScheduler - ERROR - [trend_analysis] [SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (132700ms)
2025-05-12 17:59:39,892 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 6571ms :: artifacts dl 240418ms
2025-05-12 17:59:39,892 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 17:59:39,892 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 17:59:39,893 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 17:59:39,893 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 17:59:39,893 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 17:59:39,894 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:59:39,894 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 17:59:39,895 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 17:59:39,895 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:59:39,895 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   4   |   4   |   0   ||   4   |   4   |
2025-05-12 17:59:39,896 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 17:59:39,896 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-197bfca2-0433-4a3c-a819-5952595494b7
2025-05-12 17:59:39,897 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 17:59:39,897 - SparkScheduler - ERROR - [trend_analysis] 4 artifacts copied, 0 already retrieved (2728kB/21ms)
2025-05-12 17:59:39,900 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 17:59:39,901 - SparkScheduler - INFO - Job trend_analysis duration: 277.97 seconds
2025-05-12 17:59:39,908 - SparkScheduler - INFO - Starting job: user_recommender - Generate user recommendations
2025-05-12 17:59:39,908 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/user_recommender.py
2025-05-12 17:59:42,181 - SparkScheduler - INFO - [user_recommender] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 17:59:42,805 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 17:59:44,755 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 17:59:44,780 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO ResourceUtils: ==============================================================
2025-05-12 17:59:44,780 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 17:59:44,782 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO ResourceUtils: ==============================================================
2025-05-12 17:59:44,783 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO SparkContext: Submitted application: MiniTwitterUserRecommender
2025-05-12 17:59:44,803 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 17:59:44,811 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 17:59:44,813 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 17:59:44,903 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO SecurityManager: Changing view acls to: spark
2025-05-12 17:59:44,905 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 17:59:44,906 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO SecurityManager: Changing view acls groups to:
2025-05-12 17:59:44,907 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 17:59:44,908 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 17:59:45,242 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO Utils: Successfully started service 'sparkDriver' on port 33433.
2025-05-12 17:59:45,284 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 17:59:45,334 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 17:59:45,361 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 17:59:45,362 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 17:59:45,369 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 17:59:45,400 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2fd4ee64-6ce5-4830-a25b-0c430ded15ca
2025-05-12 17:59:45,416 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 17:59:45,435 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 17:59:45,639 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 17:59:45,684 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:33433/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747072784745
2025-05-12 17:59:45,684 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:33433/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747072784745
2025-05-12 17:59:45,685 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:33433/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747072784745
2025-05-12 17:59:45,685 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:33433/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747072784745
2025-05-12 17:59:45,689 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:33433/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747072784745
2025-05-12 17:59:45,691 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-77f8c34b-4ee1-4c73-ae06-0c1b54060184/userFiles-d15a7029-2172-44ea-ad21-8549a2165ed9/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 17:59:45,717 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:33433/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747072784745
2025-05-12 17:59:45,718 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-77f8c34b-4ee1-4c73-ae06-0c1b54060184/userFiles-d15a7029-2172-44ea-ad21-8549a2165ed9/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 17:59:45,727 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:33433/files/org.mongodb_bson-4.0.5.jar with timestamp 1747072784745
2025-05-12 17:59:45,728 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-77f8c34b-4ee1-4c73-ae06-0c1b54060184/userFiles-d15a7029-2172-44ea-ad21-8549a2165ed9/org.mongodb_bson-4.0.5.jar
2025-05-12 17:59:45,737 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:33433/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747072784745
2025-05-12 17:59:45,737 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-77f8c34b-4ee1-4c73-ae06-0c1b54060184/userFiles-d15a7029-2172-44ea-ad21-8549a2165ed9/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 17:59:45,835 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 17:59:45,881 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 23 ms (0 ms spent in bootstraps)
2025-05-12 17:59:45,968 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512175945-0001
2025-05-12 17:59:45,972 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512175945-0001/0 on worker-20250512175431-172.23.0.8-44645 (172.23.0.8:44645) with 2 core(s)
2025-05-12 17:59:45,974 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512175945-0001/0 on hostPort 172.23.0.8:44645 with 2 core(s), 1024.0 MiB RAM
2025-05-12 17:59:45,978 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40361.
2025-05-12 17:59:45,979 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO NettyBlockTransferService: Server created on 1939bc4966ae:40361
2025-05-12 17:59:45,982 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 17:59:45,990 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1939bc4966ae, 40361, None)
2025-05-12 17:59:45,994 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO BlockManagerMasterEndpoint: Registering block manager 1939bc4966ae:40361 with 434.4 MiB RAM, BlockManagerId(driver, 1939bc4966ae, 40361, None)
2025-05-12 17:59:45,996 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1939bc4966ae, 40361, None)
2025-05-12 17:59:45,998 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1939bc4966ae, 40361, None)
2025-05-12 17:59:46,048 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512175945-0001/0 is now RUNNING
2025-05-12 17:59:46,307 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 17:59:46,608 - SparkScheduler - INFO - [user_recommender] Starting Mini Twitter User Recommender...
2025-05-12 17:59:46,617 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 17:59:46,621 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:46 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 17:59:47,995 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 17:59:48,085 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 17:59:48,092 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1939bc4966ae:40361 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:48,105 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 17:59:48,284 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 17:59:48,345 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 17:59:48,370 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 17:59:48,413 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO connection: Opened connection [connectionId{localValue:1, serverValue:706}] to mongodb:27017
2025-05-12 17:59:48,427 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=8917551}
2025-05-12 17:59:48,470 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:48 INFO connection: Opened connection [connectionId{localValue:2, serverValue:707}] to mongodb:27017
2025-05-12 17:59:49,304 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 17:59:49,349 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO DAGScheduler: Got job 0 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 17:59:49,353 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 17:59:49,356 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 17:59:49,359 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO DAGScheduler: Missing parents: List()
2025-05-12 17:59:49,377 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 17:59:49,430 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 17:59:49,437 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 17:59:49,449 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1939bc4966ae:40361 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:49,453 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 17:59:49,511 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 17:59:49,524 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 17:59:49,812 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:54350) with ID 0,  ResourceProfileId 0
2025-05-12 17:59:49,951 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:41009 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 41009, None)
2025-05-12 17:59:54,225 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 17:59:54,783 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:41009 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:55,142 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:41009 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 17:59:55,503 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1342 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 17:59:55,505 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 17:59:55,511 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:55 INFO DAGScheduler: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88) finished in 6.103 s
2025-05-12 17:59:55,516 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 17:59:55,516 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
2025-05-12 17:59:55,554 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:55 INFO DAGScheduler: Job 0 finished: treeAggregate at MongoInferSchema.scala:88, took 2.922250 s
2025-05-12 17:59:56,407 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1939bc4966ae:40361 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:56,417 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:41009 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 17:59:57,128 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO SparkUI: Stopped Spark web UI at http://1939bc4966ae:4040
2025-05-12 17:59:57,131 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 17:59:57,131 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 17:59:57,153 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 17:59:57,182 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 17:59:57,186 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO connection: Closed connection [connectionId{localValue:2, serverValue:707}] to mongodb:27017 because the pool has been closed.
2025-05-12 17:59:57,188 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO MemoryStore: MemoryStore cleared
2025-05-12 17:59:57,189 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO BlockManager: BlockManager stopped
2025-05-12 17:59:57,194 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 17:59:57,198 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 17:59:57,219 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 17:59:57,690 - SparkScheduler - INFO - [user_recommender] Traceback (most recent call last):
2025-05-12 17:59:57,691 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 170, in <module>
2025-05-12 17:59:57,694 - SparkScheduler - INFO - [user_recommender] main()
2025-05-12 17:59:57,695 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 159, in main
2025-05-12 17:59:57,697 - SparkScheduler - INFO - [user_recommender] recommendation_results = generate_user_recommendations(spark)
2025-05-12 17:59:57,697 - SparkScheduler - INFO - [user_recommender] File "/opt/spark-jobs/user_recommender.py", line 35, in generate_user_recommendations
2025-05-12 17:59:57,700 - SparkScheduler - INFO - [user_recommender] .load())
2025-05-12 17:59:57,700 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 184, in load
2025-05-12 17:59:57,700 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 17:59:57,703 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
2025-05-12 17:59:57,704 - SparkScheduler - INFO - [user_recommender] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
2025-05-12 17:59:57,704 - SparkScheduler - INFO - [user_recommender] py4j.protocol.Py4JJavaError: An error occurred while calling o45.load.
2025-05-12 17:59:57,705 - SparkScheduler - INFO - [user_recommender] : java.sql.SQLException: No suitable driver
2025-05-12 17:59:57,705 - SparkScheduler - INFO - [user_recommender] at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)
2025-05-12 17:59:57,705 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)
2025-05-12 17:59:57,706 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 17:59:57,706 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)
2025-05-12 17:59:57,707 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)
2025-05-12 17:59:57,708 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
2025-05-12 17:59:57,708 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
2025-05-12 17:59:57,709 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
2025-05-12 17:59:57,710 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
2025-05-12 17:59:57,710 - SparkScheduler - INFO - [user_recommender] at scala.Option.getOrElse(Option.scala:189)
2025-05-12 17:59:57,711 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
2025-05-12 17:59:57,711 - SparkScheduler - INFO - [user_recommender] at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
2025-05-12 17:59:57,712 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2025-05-12 17:59:57,712 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
2025-05-12 17:59:57,713 - SparkScheduler - INFO - [user_recommender] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2025-05-12 17:59:57,713 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.reflect.Method.invoke(Method.java:568)
2025-05-12 17:59:57,714 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2025-05-12 17:59:57,714 - SparkScheduler - INFO - [user_recommender] at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2025-05-12 17:59:57,715 - SparkScheduler - INFO - [user_recommender] at py4j.Gateway.invoke(Gateway.java:282)
2025-05-12 17:59:57,715 - SparkScheduler - INFO - [user_recommender] at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2025-05-12 17:59:57,716 - SparkScheduler - INFO - [user_recommender] at py4j.commands.CallCommand.execute(CallCommand.java:79)
2025-05-12 17:59:57,716 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
2025-05-12 17:59:57,716 - SparkScheduler - INFO - [user_recommender] at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
2025-05-12 17:59:57,717 - SparkScheduler - INFO - [user_recommender] at java.base/java.lang.Thread.run(Thread.java:840)
2025-05-12 17:59:57,717 - SparkScheduler - INFO - [user_recommender] 
2025-05-12 17:59:57,819 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 17:59:57,820 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-c6de367c-f502-444e-882d-348523ccdf24
2025-05-12 17:59:57,830 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-77f8c34b-4ee1-4c73-ae06-0c1b54060184
2025-05-12 17:59:57,838 - SparkScheduler - INFO - [user_recommender] 25/05/12 17:59:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-77f8c34b-4ee1-4c73-ae06-0c1b54060184/pyspark-77d5111f-246f-4050-97ee-173c0c0eef77
2025-05-12 17:59:57,892 - SparkScheduler - ERROR - [user_recommender] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 17:59:57,893 - SparkScheduler - ERROR - [user_recommender] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 17:59:57,893 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 17:59:57,893 - SparkScheduler - ERROR - [user_recommender] :: resolving dependencies :: org.apache.spark#spark-submit-parent-249ebb1d-8750-4cb8-9ffd-52d9ea1cdda8;1.0
2025-05-12 17:59:57,894 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 17:59:57,894 - SparkScheduler - ERROR - [user_recommender] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 17:59:57,894 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 17:59:57,895 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#bson;4.0.5 in central
2025-05-12 17:59:57,895 - SparkScheduler - ERROR - [user_recommender] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 17:59:57,895 - SparkScheduler - ERROR - [user_recommender] :: resolution report :: resolve 231ms :: artifacts dl 14ms
2025-05-12 17:59:57,896 - SparkScheduler - ERROR - [user_recommender] :: modules in use:
2025-05-12 17:59:57,896 - SparkScheduler - ERROR - [user_recommender] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 17:59:57,896 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 17:59:57,897 - SparkScheduler - ERROR - [user_recommender] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 17:59:57,897 - SparkScheduler - ERROR - [user_recommender] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 17:59:57,897 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 17:59:57,898 - SparkScheduler - ERROR - [user_recommender] |                  |            modules            ||   artifacts   |
2025-05-12 17:59:57,898 - SparkScheduler - ERROR - [user_recommender] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 17:59:57,899 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 17:59:57,899 - SparkScheduler - ERROR - [user_recommender] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 17:59:57,899 - SparkScheduler - ERROR - [user_recommender] ---------------------------------------------------------------------
2025-05-12 17:59:57,900 - SparkScheduler - ERROR - [user_recommender] :: retrieving :: org.apache.spark#spark-submit-parent-249ebb1d-8750-4cb8-9ffd-52d9ea1cdda8
2025-05-12 17:59:57,900 - SparkScheduler - ERROR - [user_recommender] confs: [default]
2025-05-12 17:59:57,900 - SparkScheduler - ERROR - [user_recommender] 0 artifacts copied, 4 already retrieved (0kB/7ms)
2025-05-12 17:59:57,901 - SparkScheduler - ERROR - Job user_recommender failed with exit code 1
2025-05-12 17:59:57,901 - SparkScheduler - INFO - Job user_recommender duration: 17.99 seconds
2025-05-12 17:59:57,902 - SparkScheduler - INFO - Starting job: content_analyzer - Analyze tweet content and topics
2025-05-12 17:59:57,903 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/content_analyzer.py
2025-05-12 18:00:00,186 - SparkScheduler - INFO - [content_analyzer] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 18:00:00,765 - SparkScheduler - INFO - [content_analyzer] 25/05/12 18:00:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 18:00:04,116 - SparkScheduler - INFO - [content_analyzer] [nltk_data] Downloading package punkt to /nltk_data...
2025-05-12 18:00:04,117 - SparkScheduler - INFO - [content_analyzer] Traceback (most recent call last):
2025-05-12 18:00:04,117 - SparkScheduler - INFO - [content_analyzer] File "/opt/spark-jobs/content_analyzer.py", line 11, in <module>
2025-05-12 18:00:04,119 - SparkScheduler - INFO - [content_analyzer] nltk.download('punkt')
2025-05-12 18:00:04,119 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 777, in download
2025-05-12 18:00:04,120 - SparkScheduler - INFO - [content_analyzer] for msg in self.incr_download(info_or_id, download_dir, force):
2025-05-12 18:00:04,121 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 642, in incr_download
2025-05-12 18:00:04,121 - SparkScheduler - INFO - [content_analyzer] yield from self._download_package(info, download_dir, force)
2025-05-12 18:00:04,121 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/site-packages/nltk/downloader.py", line 699, in _download_package
2025-05-12 18:00:04,122 - SparkScheduler - INFO - [content_analyzer] os.makedirs(download_dir)
2025-05-12 18:00:04,122 - SparkScheduler - INFO - [content_analyzer] File "/opt/bitnami/python/lib/python3.10/os.py", line 225, in makedirs
2025-05-12 18:00:04,123 - SparkScheduler - INFO - [content_analyzer] mkdir(name, mode)
2025-05-12 18:00:04,123 - SparkScheduler - INFO - [content_analyzer] PermissionError: [Errno 13] Permission denied: '/nltk_data'
2025-05-12 18:00:04,303 - SparkScheduler - INFO - [content_analyzer] 25/05/12 18:00:04 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 18:00:04,304 - SparkScheduler - INFO - [content_analyzer] 25/05/12 18:00:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ed0ddca-4d5f-41f2-8705-2b36be3bb487
2025-05-12 18:00:04,342 - SparkScheduler - ERROR - [content_analyzer] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 18:00:04,343 - SparkScheduler - ERROR - [content_analyzer] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 18:00:04,345 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 18:00:04,345 - SparkScheduler - ERROR - [content_analyzer] :: resolving dependencies :: org.apache.spark#spark-submit-parent-92d52cc2-3660-43f7-972a-180b154a794e;1.0
2025-05-12 18:00:04,346 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 18:00:04,347 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 18:00:04,347 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 18:00:04,348 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#bson;4.0.5 in central
2025-05-12 18:00:04,348 - SparkScheduler - ERROR - [content_analyzer] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 18:00:04,349 - SparkScheduler - ERROR - [content_analyzer] :: resolution report :: resolve 204ms :: artifacts dl 12ms
2025-05-12 18:00:04,349 - SparkScheduler - ERROR - [content_analyzer] :: modules in use:
2025-05-12 18:00:04,350 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 18:00:04,350 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 18:00:04,350 - SparkScheduler - ERROR - [content_analyzer] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 18:00:04,351 - SparkScheduler - ERROR - [content_analyzer] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 18:00:04,351 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 18:00:04,352 - SparkScheduler - ERROR - [content_analyzer] |                  |            modules            ||   artifacts   |
2025-05-12 18:00:04,352 - SparkScheduler - ERROR - [content_analyzer] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 18:00:04,352 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 18:00:04,353 - SparkScheduler - ERROR - [content_analyzer] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 18:00:04,353 - SparkScheduler - ERROR - [content_analyzer] ---------------------------------------------------------------------
2025-05-12 18:00:04,353 - SparkScheduler - ERROR - [content_analyzer] :: retrieving :: org.apache.spark#spark-submit-parent-92d52cc2-3660-43f7-972a-180b154a794e
2025-05-12 18:00:04,354 - SparkScheduler - ERROR - [content_analyzer] confs: [default]
2025-05-12 18:00:04,354 - SparkScheduler - ERROR - [content_analyzer] 0 artifacts copied, 4 already retrieved (0kB/8ms)
2025-05-12 18:00:04,354 - SparkScheduler - ERROR - Job content_analyzer failed with exit code 1
2025-05-12 18:00:04,355 - SparkScheduler - INFO - Job content_analyzer duration: 6.45 seconds
2025-05-12 18:00:04,355 - SparkScheduler - INFO - Starting job: discover_feed_generator - Generate discover feed recommendations
2025-05-12 18:00:04,355 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/discover_feed_generator.py
2025-05-12 18:00:06,289 - SparkScheduler - INFO - [discover_feed_generator] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 18:00:07,099 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 18:00:08,682 - SparkScheduler - INFO - [discover_feed_generator] Starting discover feed generation...
2025-05-12 18:00:08,805 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 18:00:08,854 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO ResourceUtils: ==============================================================
2025-05-12 18:00:08,862 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 18:00:08,863 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO ResourceUtils: ==============================================================
2025-05-12 18:00:08,863 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO SparkContext: Submitted application: DiscoverFeedGenerator
2025-05-12 18:00:08,905 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 18:00:08,917 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 18:00:08,919 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 18:00:08,992 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO SecurityManager: Changing view acls to: spark
2025-05-12 18:00:08,993 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 18:00:08,994 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO SecurityManager: Changing view acls groups to:
2025-05-12 18:00:08,995 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 18:00:08,997 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 18:00:09,422 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO Utils: Successfully started service 'sparkDriver' on port 33137.
2025-05-12 18:00:09,486 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 18:00:09,569 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 18:00:09,601 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 18:00:09,603 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 18:00:09,611 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 18:00:09,655 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd0587a8-16d1-403d-8eca-acecaa666a86
2025-05-12 18:00:09,680 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 18:00:09,708 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:09 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 18:00:10,074 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 18:00:10,195 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:33137/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747072808792
2025-05-12 18:00:10,196 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:33137/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747072808792
2025-05-12 18:00:10,197 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:33137/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747072808792
2025-05-12 18:00:10,198 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:33137/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747072808792
2025-05-12 18:00:10,204 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:33137/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747072808792
2025-05-12 18:00:10,209 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-4808ca78-b7a9-449e-bf06-419329ebaa11/userFiles-c913d063-5905-4b12-ad8b-8a382273eac7/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 18:00:10,229 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:33137/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747072808792
2025-05-12 18:00:10,229 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-4808ca78-b7a9-449e-bf06-419329ebaa11/userFiles-c913d063-5905-4b12-ad8b-8a382273eac7/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 18:00:10,243 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:33137/files/org.mongodb_bson-4.0.5.jar with timestamp 1747072808792
2025-05-12 18:00:10,244 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-4808ca78-b7a9-449e-bf06-419329ebaa11/userFiles-c913d063-5905-4b12-ad8b-8a382273eac7/org.mongodb_bson-4.0.5.jar
2025-05-12 18:00:10,258 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:33137/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747072808792
2025-05-12 18:00:10,258 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-4808ca78-b7a9-449e-bf06-419329ebaa11/userFiles-c913d063-5905-4b12-ad8b-8a382273eac7/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 18:00:10,412 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 18:00:10,480 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 41 ms (0 ms spent in bootstraps)
2025-05-12 18:00:10,601 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512180010-0002
2025-05-12 18:00:10,605 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512180010-0002/0 on worker-20250512175431-172.23.0.8-44645 (172.23.0.8:44645) with 2 core(s)
2025-05-12 18:00:10,608 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512180010-0002/0 on hostPort 172.23.0.8:44645 with 2 core(s), 1024.0 MiB RAM
2025-05-12 18:00:10,614 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43003.
2025-05-12 18:00:10,615 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO NettyBlockTransferService: Server created on 1939bc4966ae:43003
2025-05-12 18:00:10,619 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 18:00:10,637 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1939bc4966ae, 43003, None)
2025-05-12 18:00:10,644 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO BlockManagerMasterEndpoint: Registering block manager 1939bc4966ae:43003 with 434.4 MiB RAM, BlockManagerId(driver, 1939bc4966ae, 43003, None)
2025-05-12 18:00:10,648 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1939bc4966ae, 43003, None)
2025-05-12 18:00:10,651 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1939bc4966ae, 43003, None)
2025-05-12 18:00:10,661 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512180010-0002/0 is now RUNNING
2025-05-12 18:00:11,106 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:11 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 18:00:11,602 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 18:00:11,608 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:11 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 18:00:13,333 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:00:13,415 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:00:13,430 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1939bc4966ae:43003 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:00:13,449 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 18:00:13,774 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:00:13,887 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:00:13,917 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:00:13,954 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO connection: Opened connection [connectionId{localValue:1, serverValue:712}] to mongodb:27017
2025-05-12 18:00:13,972 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:13 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=12078744}
2025-05-12 18:00:14,014 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:14 INFO connection: Opened connection [connectionId{localValue:2, serverValue:713}] to mongodb:27017
2025-05-12 18:00:14,957 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:14 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:00:15,007 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO DAGScheduler: Got job 0 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:00:15,010 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:00:15,012 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:00:15,018 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:00:15,030 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:00:15,081 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:00:15,087 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:00:15,090 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1939bc4966ae:43003 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:15,093 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:00:15,132 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:00:15,136 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 18:00:16,089 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:57326) with ID 0,  ResourceProfileId 0
2025-05-12 18:00:16,226 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:16 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:41141 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 41141, None)
2025-05-12 18:00:16,628 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:00:17,151 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:41141 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:17,661 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:41141 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:00:18,202 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1613 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:00:18,206 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 18:00:18,214 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:18 INFO DAGScheduler: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88) finished in 3.151 s
2025-05-12 18:00:18,219 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:00:18,221 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
2025-05-12 18:00:18,277 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:18 INFO DAGScheduler: Job 0 finished: treeAggregate at MongoInferSchema.scala:88, took 3.319304 s
2025-05-12 18:00:19,343 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1939bc4966ae:43003 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:19,355 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:41141 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:19,905 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:19 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:00:19,910 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:19 INFO connection: Closed connection [connectionId{localValue:2, serverValue:713}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:00:20,600 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:00:20,604 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:00:20,606 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1939bc4966ae:43003 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:00:20,608 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO SparkContext: Created broadcast 2 from broadcast at MongoSpark.scala:530
2025-05-12 18:00:20,610 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:00:20,612 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:00:20,613 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:00:20,615 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO connection: Opened connection [connectionId{localValue:3, serverValue:717}] to mongodb:27017
2025-05-12 18:00:20,617 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1601668}
2025-05-12 18:00:20,620 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO connection: Opened connection [connectionId{localValue:4, serverValue:718}] to mongodb:27017
2025-05-12 18:00:20,643 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:00:20,645 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Got job 1 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:00:20,645 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Final stage: ResultStage 1 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:00:20,646 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:00:20,647 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:00:20,648 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:00:20,654 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:00:20,660 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:00:20,661 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1939bc4966ae:43003 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:20,662 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:00:20,665 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:00:20,666 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
2025-05-12 18:00:20,669 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:00:20,697 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.8:41141 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:20,728 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:41141 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:00:20,748 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 80 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:00:20,759 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
2025-05-12 18:00:20,759 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: ResultStage 1 (treeAggregate at MongoInferSchema.scala:88) finished in 0.101 s
2025-05-12 18:00:20,760 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:00:20,761 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
2025-05-12 18:00:20,762 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Job 1 finished: treeAggregate at MongoInferSchema.scala:88, took 0.110233 s
2025-05-12 18:00:20,777 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:00:20,792 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:00:20,793 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1939bc4966ae:43003 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:00:20,796 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1939bc4966ae:43003 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:20,798 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO SparkContext: Created broadcast 4 from broadcast at MongoSpark.scala:530
2025-05-12 18:00:20,800 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.23.0.8:41141 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:20,822 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:00:20,824 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Got job 2 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:00:20,825 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Final stage: ResultStage 2 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:00:20,825 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:00:20,826 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:00:20,827 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:00:20,832 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:00:20,840 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:00:20,842 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1939bc4966ae:43003 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:20,843 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:00:20,845 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:00:20,845 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 18:00:20,849 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:00:20,878 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:41141 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:00:20,908 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:41141 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:00:20,931 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 82 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:00:20,932 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 18:00:20,933 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: ResultStage 2 (treeAggregate at MongoInferSchema.scala:88) finished in 0.104 s
2025-05-12 18:00:20,936 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:00:20,937 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 18:00:20,938 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:20 INFO DAGScheduler: Job 2 finished: treeAggregate at MongoInferSchema.scala:88, took 0.112531 s
2025-05-12 18:00:21,334 - SparkScheduler - INFO - [discover_feed_generator] Error generating discover feed: Column 'likes_count' does not exist. Did you mean one of the following? [content, created_at, user_id, _id, hashtags, id, media_urls, retweeted_from, original_user_id]; line 1 pos 10;
2025-05-12 18:00:21,335 - SparkScheduler - INFO - [discover_feed_generator] 'Project [_id#0, content#1, created_at#2, hashtags#3, id#4, media_urls#5, original_user_id#6, retweeted_from#7, user_id#8, ((('COALESCE('likes_count, 0) * 1) + ('COALESCE('comments_count, 0) * 2)) + ('COALESCE('retweet_count, 0) * 3)) AS engagement_score#18]
2025-05-12 18:00:21,338 - SparkScheduler - INFO - [discover_feed_generator] +- Relation [_id#0,content#1,created_at#2,hashtags#3,id#4,media_urls#5,original_user_id#6,retweeted_from#7,user_id#8] MongoRelation(MongoRDD[0] at RDD at MongoRDD.scala:51,Some(StructType(StructField(_id,StructType(StructField(oid,StringType,true)),true),StructField(content,StringType,true),StructField(created_at,TimestampType,true),StructField(hashtags,ArrayType(StringType,true),true),StructField(id,StringType,true),StructField(media_urls,ArrayType(StringType,true),true),StructField(original_user_id,StringType,true),StructField(retweeted_from,StringType,true),StructField(user_id,StringType,true))))
2025-05-12 18:00:21,340 - SparkScheduler - INFO - [discover_feed_generator] 
2025-05-12 18:00:21,356 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO SparkUI: Stopped Spark web UI at http://1939bc4966ae:4040
2025-05-12 18:00:21,362 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 18:00:21,363 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 18:00:21,404 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 18:00:21,457 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:00:21,459 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO connection: Closed connection [connectionId{localValue:4, serverValue:718}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:00:21,461 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO MemoryStore: MemoryStore cleared
2025-05-12 18:00:21,463 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO BlockManager: BlockManager stopped
2025-05-12 18:00:21,471 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 18:00:21,478 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 18:00:21,506 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 18:00:21,705 - SparkScheduler - INFO - [discover_feed_generator] Discover feed generation completed
2025-05-12 18:00:21,867 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 18:00:21,870 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-4808ca78-b7a9-449e-bf06-419329ebaa11
2025-05-12 18:00:21,888 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-4808ca78-b7a9-449e-bf06-419329ebaa11/pyspark-0a807733-8521-41a5-a2b8-57fe29ee9423
2025-05-12 18:00:21,907 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:00:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-86731547-239e-4ddd-9c62-bb9e0663c0d9
2025-05-12 18:00:22,009 - SparkScheduler - ERROR - [discover_feed_generator] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 18:00:22,009 - SparkScheduler - ERROR - [discover_feed_generator] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 18:00:22,010 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 18:00:22,011 - SparkScheduler - ERROR - [discover_feed_generator] :: resolving dependencies :: org.apache.spark#spark-submit-parent-54fd804f-3fbd-4cff-99c6-b849444ee5da;1.0
2025-05-12 18:00:22,011 - SparkScheduler - ERROR - [discover_feed_generator] confs: [default]
2025-05-12 18:00:22,012 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 18:00:22,012 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 18:00:22,013 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#bson;4.0.5 in central
2025-05-12 18:00:22,015 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 18:00:22,016 - SparkScheduler - ERROR - [discover_feed_generator] :: resolution report :: resolve 325ms :: artifacts dl 16ms
2025-05-12 18:00:22,017 - SparkScheduler - ERROR - [discover_feed_generator] :: modules in use:
2025-05-12 18:00:22,018 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 18:00:22,018 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 18:00:22,019 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 18:00:22,019 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 18:00:22,020 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:00:22,021 - SparkScheduler - ERROR - [discover_feed_generator] |                  |            modules            ||   artifacts   |
2025-05-12 18:00:22,021 - SparkScheduler - ERROR - [discover_feed_generator] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 18:00:22,022 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:00:22,022 - SparkScheduler - ERROR - [discover_feed_generator] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 18:00:22,022 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:00:22,023 - SparkScheduler - ERROR - [discover_feed_generator] :: retrieving :: org.apache.spark#spark-submit-parent-54fd804f-3fbd-4cff-99c6-b849444ee5da
2025-05-12 18:00:22,023 - SparkScheduler - ERROR - [discover_feed_generator] confs: [default]
2025-05-12 18:00:22,023 - SparkScheduler - ERROR - [discover_feed_generator] 0 artifacts copied, 4 already retrieved (0kB/7ms)
2025-05-12 18:00:22,024 - SparkScheduler - INFO - Job discover_feed_generator completed successfully
2025-05-12 18:00:22,025 - SparkScheduler - INFO - Job discover_feed_generator duration: 17.67 seconds
2025-05-12 18:00:22,026 - SparkScheduler - INFO - Scheduler running. Press Ctrl+C to exit.
2025-05-12 18:10:06,713 - SparkScheduler - INFO - Starting job: discover_feed_generator - Generate discover feed recommendations
2025-05-12 18:10:06,714 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/discover_feed_generator.py
2025-05-12 18:10:12,482 - SparkScheduler - INFO - [discover_feed_generator] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 18:10:13,188 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 18:10:14,279 - SparkScheduler - INFO - [discover_feed_generator] Starting discover feed generation...
2025-05-12 18:10:14,397 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 18:10:14,428 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO ResourceUtils: ==============================================================
2025-05-12 18:10:14,429 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 18:10:14,430 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO ResourceUtils: ==============================================================
2025-05-12 18:10:14,431 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SparkContext: Submitted application: DiscoverFeedGenerator
2025-05-12 18:10:14,461 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 18:10:14,471 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 18:10:14,472 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 18:10:14,538 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SecurityManager: Changing view acls to: spark
2025-05-12 18:10:14,539 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 18:10:14,540 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SecurityManager: Changing view acls groups to:
2025-05-12 18:10:14,541 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 18:10:14,542 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 18:10:14,885 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO Utils: Successfully started service 'sparkDriver' on port 41555.
2025-05-12 18:10:14,922 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 18:10:14,967 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 18:10:15,000 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 18:10:15,003 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 18:10:15,016 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 18:10:15,077 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b8748da7-ab4c-401d-a841-1bce3eb141e3
2025-05-12 18:10:15,125 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 18:10:15,184 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 18:10:15,479 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 18:10:15,546 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:41555/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747073414391
2025-05-12 18:10:15,547 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:41555/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747073414391
2025-05-12 18:10:15,548 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:41555/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747073414391
2025-05-12 18:10:15,549 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:41555/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747073414391
2025-05-12 18:10:15,553 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:41555/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747073414391
2025-05-12 18:10:15,557 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-640ed239-bb65-464b-b100-8642566e096a/userFiles-4bafb72b-3fa6-42f8-917a-400c621ef1ac/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 18:10:15,575 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:41555/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747073414391
2025-05-12 18:10:15,576 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-640ed239-bb65-464b-b100-8642566e096a/userFiles-4bafb72b-3fa6-42f8-917a-400c621ef1ac/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 18:10:15,588 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:41555/files/org.mongodb_bson-4.0.5.jar with timestamp 1747073414391
2025-05-12 18:10:15,589 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-640ed239-bb65-464b-b100-8642566e096a/userFiles-4bafb72b-3fa6-42f8-917a-400c621ef1ac/org.mongodb_bson-4.0.5.jar
2025-05-12 18:10:15,600 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:41555/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747073414391
2025-05-12 18:10:15,601 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-640ed239-bb65-464b-b100-8642566e096a/userFiles-4bafb72b-3fa6-42f8-917a-400c621ef1ac/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 18:10:15,712 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 18:10:15,758 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 25 ms (0 ms spent in bootstraps)
2025-05-12 18:10:15,865 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512181015-0003
2025-05-12 18:10:15,867 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512181015-0003/0 on worker-20250512175431-172.23.0.8-44645 (172.23.0.8:44645) with 2 core(s)
2025-05-12 18:10:15,870 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512181015-0003/0 on hostPort 172.23.0.8:44645 with 2 core(s), 1024.0 MiB RAM
2025-05-12 18:10:15,874 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43849.
2025-05-12 18:10:15,874 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO NettyBlockTransferService: Server created on 1939bc4966ae:43849
2025-05-12 18:10:15,876 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 18:10:15,889 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1939bc4966ae, 43849, None)
2025-05-12 18:10:15,897 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO BlockManagerMasterEndpoint: Registering block manager 1939bc4966ae:43849 with 434.4 MiB RAM, BlockManagerId(driver, 1939bc4966ae, 43849, None)
2025-05-12 18:10:15,906 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1939bc4966ae, 43849, None)
2025-05-12 18:10:15,909 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1939bc4966ae, 43849, None)
2025-05-12 18:10:15,924 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512181015-0003/0 is now RUNNING
2025-05-12 18:10:16,253 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:16 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 18:10:16,561 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 18:10:16,565 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:16 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 18:10:17,965 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:10:18,033 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:10:18,039 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1939bc4966ae:43849 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:10:18,050 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 18:10:18,194 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:10:18,250 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:10:18,268 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:10:18,304 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO connection: Opened connection [connectionId{localValue:1, serverValue:773}] to mongodb:27017
2025-05-12 18:10:18,313 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6448829}
2025-05-12 18:10:18,344 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO connection: Opened connection [connectionId{localValue:2, serverValue:774}] to mongodb:27017
2025-05-12 18:10:18,901 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:10:18,940 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO DAGScheduler: Got job 0 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:10:18,942 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:10:18,943 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:10:18,947 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:10:18,959 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:10:19,006 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:10:19,014 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:10:19,016 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1939bc4966ae:43849 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:19,019 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:10:19,051 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:10:19,053 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 18:10:19,836 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:41770) with ID 0,  ResourceProfileId 0
2025-05-12 18:10:19,941 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:40785 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 40785, None)
2025-05-12 18:10:20,576 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:10:21,430 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:40785 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:22,145 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:40785 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:10:23,056 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2501 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:10:23,059 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 18:10:23,073 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:23 INFO DAGScheduler: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88) finished in 4.083 s
2025-05-12 18:10:23,081 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:10:23,082 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
2025-05-12 18:10:23,142 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:23 INFO DAGScheduler: Job 0 finished: treeAggregate at MongoInferSchema.scala:88, took 4.240026 s
2025-05-12 18:10:23,851 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:10:23,861 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:23 INFO connection: Closed connection [connectionId{localValue:2, serverValue:774}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:10:24,676 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1939bc4966ae:43849 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:24,689 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:40785 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:25,554 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:10:25,560 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:10:25,562 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1939bc4966ae:43849 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:10:25,564 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO SparkContext: Created broadcast 2 from broadcast at MongoSpark.scala:530
2025-05-12 18:10:25,567 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:10:25,569 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:10:25,569 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:10:25,571 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO connection: Opened connection [connectionId{localValue:3, serverValue:778}] to mongodb:27017
2025-05-12 18:10:25,574 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2253518}
2025-05-12 18:10:25,577 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO connection: Opened connection [connectionId{localValue:4, serverValue:779}] to mongodb:27017
2025-05-12 18:10:25,600 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:10:25,602 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Got job 1 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:10:25,603 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Final stage: ResultStage 1 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:10:25,604 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:10:25,605 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:10:25,606 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:10:25,609 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:10:25,613 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:10:25,614 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1939bc4966ae:43849 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:25,616 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:10:25,618 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:10:25,619 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
2025-05-12 18:10:25,626 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:10:25,687 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.8:40785 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:25,731 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:40785 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:10:25,757 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 134 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:10:25,758 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
2025-05-12 18:10:25,759 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: ResultStage 1 (treeAggregate at MongoInferSchema.scala:88) finished in 0.153 s
2025-05-12 18:10:25,760 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:10:25,761 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
2025-05-12 18:10:25,762 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Job 1 finished: treeAggregate at MongoInferSchema.scala:88, took 0.160508 s
2025-05-12 18:10:25,780 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:10:25,794 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:10:25,796 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1939bc4966ae:43849 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:10:25,797 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO SparkContext: Created broadcast 4 from broadcast at MongoSpark.scala:530
2025-05-12 18:10:25,798 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1939bc4966ae:43849 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:25,807 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.23.0.8:40785 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:25,826 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:10:25,828 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Got job 2 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:10:25,829 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Final stage: ResultStage 2 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:10:25,830 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:10:25,830 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:10:25,832 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:10:25,838 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:10:25,842 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:10:25,844 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1939bc4966ae:43849 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:25,846 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:10:25,848 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:10:25,850 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 18:10:25,854 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:10:25,886 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:40785 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:10:25,925 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:40785 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:10:25,952 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 98 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:10:25,952 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 18:10:25,953 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: ResultStage 2 (treeAggregate at MongoInferSchema.scala:88) finished in 0.120 s
2025-05-12 18:10:25,954 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:10:25,955 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 18:10:25,956 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:25 INFO DAGScheduler: Job 2 finished: treeAggregate at MongoInferSchema.scala:88, took 0.129254 s
2025-05-12 18:10:26,504 - SparkScheduler - INFO - [discover_feed_generator] Error generating discover feed: Column 'likes_count' does not exist. Did you mean one of the following? [content, created_at, user_id, _id, hashtags, id, media_urls, retweeted_from, original_user_id]; line 1 pos 10;
2025-05-12 18:10:26,504 - SparkScheduler - INFO - [discover_feed_generator] 'Project [_id#0, content#1, created_at#2, hashtags#3, id#4, media_urls#5, original_user_id#6, retweeted_from#7, user_id#8, ((('COALESCE('likes_count, 0) * 1) + ('COALESCE('comments_count, 0) * 2)) + ('COALESCE('retweet_count, 0) * 3)) AS engagement_score#18]
2025-05-12 18:10:26,505 - SparkScheduler - INFO - [discover_feed_generator] +- Relation [_id#0,content#1,created_at#2,hashtags#3,id#4,media_urls#5,original_user_id#6,retweeted_from#7,user_id#8] MongoRelation(MongoRDD[0] at RDD at MongoRDD.scala:51,Some(StructType(StructField(_id,StructType(StructField(oid,StringType,true)),true),StructField(content,StringType,true),StructField(created_at,TimestampType,true),StructField(hashtags,ArrayType(StringType,true),true),StructField(id,StringType,true),StructField(media_urls,ArrayType(StringType,true),true),StructField(original_user_id,StringType,true),StructField(retweeted_from,StringType,true),StructField(user_id,StringType,true))))
2025-05-12 18:10:26,506 - SparkScheduler - INFO - [discover_feed_generator] 
2025-05-12 18:10:26,523 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO SparkUI: Stopped Spark web UI at http://1939bc4966ae:4040
2025-05-12 18:10:26,529 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 18:10:26,530 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 18:10:26,576 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 18:10:26,623 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:10:26,624 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO connection: Closed connection [connectionId{localValue:4, serverValue:779}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:10:26,626 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO MemoryStore: MemoryStore cleared
2025-05-12 18:10:26,627 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO BlockManager: BlockManager stopped
2025-05-12 18:10:26,636 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 18:10:26,641 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 18:10:26,676 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 18:10:26,814 - SparkScheduler - INFO - [discover_feed_generator] Discover feed generation completed
2025-05-12 18:10:26,878 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 18:10:26,879 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-640ed239-bb65-464b-b100-8642566e096a/pyspark-638269f2-6a0d-44f9-ab5c-1fd396ac3155
2025-05-12 18:10:26,890 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-640ed239-bb65-464b-b100-8642566e096a
2025-05-12 18:10:26,901 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:10:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b877d8f-0faa-4e54-9e7e-099b52a7ad1b
2025-05-12 18:10:26,995 - SparkScheduler - ERROR - [discover_feed_generator] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 18:10:26,996 - SparkScheduler - ERROR - [discover_feed_generator] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 18:10:26,997 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 18:10:26,997 - SparkScheduler - ERROR - [discover_feed_generator] :: resolving dependencies :: org.apache.spark#spark-submit-parent-4929f38a-c6da-4e44-9d27-2d07fbe443bd;1.0
2025-05-12 18:10:26,997 - SparkScheduler - ERROR - [discover_feed_generator] confs: [default]
2025-05-12 18:10:26,998 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 18:10:26,998 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 18:10:26,999 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#bson;4.0.5 in central
2025-05-12 18:10:26,999 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 18:10:26,999 - SparkScheduler - ERROR - [discover_feed_generator] :: resolution report :: resolve 281ms :: artifacts dl 14ms
2025-05-12 18:10:27,000 - SparkScheduler - ERROR - [discover_feed_generator] :: modules in use:
2025-05-12 18:10:27,000 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 18:10:27,001 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 18:10:27,001 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 18:10:27,002 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 18:10:27,002 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:10:27,003 - SparkScheduler - ERROR - [discover_feed_generator] |                  |            modules            ||   artifacts   |
2025-05-12 18:10:27,003 - SparkScheduler - ERROR - [discover_feed_generator] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 18:10:27,005 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:10:27,006 - SparkScheduler - ERROR - [discover_feed_generator] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 18:10:27,006 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:10:27,007 - SparkScheduler - ERROR - [discover_feed_generator] :: retrieving :: org.apache.spark#spark-submit-parent-4929f38a-c6da-4e44-9d27-2d07fbe443bd
2025-05-12 18:10:27,007 - SparkScheduler - ERROR - [discover_feed_generator] confs: [default]
2025-05-12 18:10:27,008 - SparkScheduler - ERROR - [discover_feed_generator] 0 artifacts copied, 4 already retrieved (0kB/10ms)
2025-05-12 18:10:27,009 - SparkScheduler - INFO - Job discover_feed_generator completed successfully
2025-05-12 18:10:27,009 - SparkScheduler - INFO - Job discover_feed_generator duration: 20.30 seconds
2025-05-12 18:25:58,692 - SparkScheduler - INFO - Starting job: trend_analysis - Analyze trending hashtags and topics
2025-05-12 18:25:58,693 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/trend_analysis.py
2025-05-12 18:26:00,488 - SparkScheduler - INFO - [trend_analysis] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 18:26:01,371 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 18:26:02,742 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 18:26:02,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO ResourceUtils: ==============================================================
2025-05-12 18:26:02,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 18:26:02,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO ResourceUtils: ==============================================================
2025-05-12 18:26:02,781 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO SparkContext: Submitted application: MiniTwitterTrendAnalysis
2025-05-12 18:26:02,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 18:26:02,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 18:26:02,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 18:26:02,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO SecurityManager: Changing view acls to: spark
2025-05-12 18:26:02,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 18:26:02,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO SecurityManager: Changing view acls groups to:
2025-05-12 18:26:02,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 18:26:02,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 18:26:03,137 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO Utils: Successfully started service 'sparkDriver' on port 36071.
2025-05-12 18:26:03,163 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 18:26:03,198 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 18:26:03,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 18:26:03,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 18:26:03,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 18:26:03,266 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fa30977c-b694-4e8f-8089-8a28b5e4c451
2025-05-12 18:26:03,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 18:26:03,322 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 18:26:03,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 18:26:03,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:36071/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747074362734
2025-05-12 18:26:03,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:36071/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747074362734
2025-05-12 18:26:03,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:36071/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747074362734
2025-05-12 18:26:03,559 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:36071/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747074362734
2025-05-12 18:26:03,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:36071/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747074362734
2025-05-12 18:26:03,563 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-9e5ef772-ad8b-40fc-93e0-a2c6e2d153b1/userFiles-839c56e8-1c65-48da-a0af-bd319f2bf609/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 18:26:03,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:36071/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747074362734
2025-05-12 18:26:03,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-9e5ef772-ad8b-40fc-93e0-a2c6e2d153b1/userFiles-839c56e8-1c65-48da-a0af-bd319f2bf609/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 18:26:03,594 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:36071/files/org.mongodb_bson-4.0.5.jar with timestamp 1747074362734
2025-05-12 18:26:03,595 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-9e5ef772-ad8b-40fc-93e0-a2c6e2d153b1/userFiles-839c56e8-1c65-48da-a0af-bd319f2bf609/org.mongodb_bson-4.0.5.jar
2025-05-12 18:26:03,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:36071/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747074362734
2025-05-12 18:26:03,609 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-9e5ef772-ad8b-40fc-93e0-a2c6e2d153b1/userFiles-839c56e8-1c65-48da-a0af-bd319f2bf609/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 18:26:03,701 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 18:26:03,762 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 33 ms (0 ms spent in bootstraps)
2025-05-12 18:26:03,903 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512182603-0004
2025-05-12 18:26:03,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512182603-0004/0 on worker-20250512175431-172.23.0.8-44645 (172.23.0.8:44645) with 2 core(s)
2025-05-12 18:26:03,912 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512182603-0004/0 on hostPort 172.23.0.8:44645 with 2 core(s), 1024.0 MiB RAM
2025-05-12 18:26:03,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45657.
2025-05-12 18:26:03,916 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO NettyBlockTransferService: Server created on 1939bc4966ae:45657
2025-05-12 18:26:03,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 18:26:03,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1939bc4966ae, 45657, None)
2025-05-12 18:26:03,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO BlockManagerMasterEndpoint: Registering block manager 1939bc4966ae:45657 with 434.4 MiB RAM, BlockManagerId(driver, 1939bc4966ae, 45657, None)
2025-05-12 18:26:03,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1939bc4966ae, 45657, None)
2025-05-12 18:26:03,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1939bc4966ae, 45657, None)
2025-05-12 18:26:03,967 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512182603-0004/0 is now RUNNING
2025-05-12 18:26:04,576 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:04 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 18:26:05,121 - SparkScheduler - INFO - [trend_analysis] Starting Mini Twitter Analytics...
2025-05-12 18:26:05,151 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 18:26:05,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:05 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 18:26:06,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:26:06,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:26:06,730 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1939bc4966ae:45657 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:06,739 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:06 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:08,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:33002) with ID 0,  ResourceProfileId 0
2025-05-12 18:26:09,065 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:09 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:32885 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 32885, None)
2025-05-12 18:26:10,647 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:10 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:10,685 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:10 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:10,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:10 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:10,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:10 INFO connection: Opened connection [connectionId{localValue:1, serverValue:1683}] to mongodb:27017
2025-05-12 18:26:10,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:10 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3269209}
2025-05-12 18:26:10,740 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:10 INFO connection: Opened connection [connectionId{localValue:2, serverValue:1684}] to mongodb:27017
2025-05-12 18:26:11,063 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 18:26:11,971 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO CodeGenerator: Code generated in 226.428685 ms
2025-05-12 18:26:12,084 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:12,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:12,088 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO connection: Opened connection [connectionId{localValue:3, serverValue:1686}] to mongodb:27017
2025-05-12 18:26:12,090 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1514697}
2025-05-12 18:26:12,097 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO connection: Opened connection [connectionId{localValue:4, serverValue:1687}] to mongodb:27017
2025-05-12 18:26:12,164 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO DAGScheduler: Registering RDD 5 (rdd at MongoSpark.scala:169) as input to shuffle 0
2025-05-12 18:26:12,170 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO DAGScheduler: Got map stage job 0 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 18:26:12,170 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (rdd at MongoSpark.scala:169)
2025-05-12 18:26:12,171 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:12,173 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:12,178 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:12 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:11,791 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 18:26:11,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 18:26:11,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1939bc4966ae:45657 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:11,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:11,829 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:11,831 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 18:26:11,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:16,170 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:32885 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:17,088 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:32885 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:19,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:19 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:19,174 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:19 INFO connection: Closed connection [connectionId{localValue:2, serverValue:1684}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:19,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8121 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:19,987 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 18:26:19,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:19 INFO DAGScheduler: ShuffleMapStage 0 (rdd at MongoSpark.scala:169) finished in 7.798 s
2025-05-12 18:26:19,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:19 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:19,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:19 INFO DAGScheduler: running: Set()
2025-05-12 18:26:20,000 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:20,001 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:20,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:20,104 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO CodeGenerator: Code generated in 22.625559 ms
2025-05-12 18:26:20,107 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:20,147 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO CodeGenerator: Code generated in 25.465361 ms
2025-05-12 18:26:20,249 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 18:26:20,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO DAGScheduler: Got job 1 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 18:26:20,256 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:120)
2025-05-12 18:26:20,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-12 18:26:20,258 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:20,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:20,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 18:26:20,295 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 18:26:20,297 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1939bc4966ae:45657 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:20,298 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:20,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:20,301 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 18:26:20,309 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:20,342 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1939bc4966ae:45657 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:20,347 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:32885 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:20,364 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:32885 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:20,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:20,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO connection: Closed connection [connectionId{localValue:4, serverValue:1687}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:20,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.23.0.8:33002
2025-05-12 18:26:21,074 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 770 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:21,075 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 18:26:21,077 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:120) finished in 0.802 s
2025-05-12 18:26:21,079 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:21,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 18:26:21,083 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Job 1 finished: foreachPartition at MongoSpark.scala:120, took 0.833145 s
2025-05-12 18:26:21,095 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 18:26:21,108 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 1939bc4966ae:45657 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 18:26:21,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.23.0.8:32885 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1939bc4966ae:45657 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:21,123 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO SparkContext: Created broadcast 3 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:21,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:21,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:21,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:21,194 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Opened connection [connectionId{localValue:5, serverValue:1692}] to mongodb:27017
2025-05-12 18:26:21,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1161607}
2025-05-12 18:26:21,200 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Opened connection [connectionId{localValue:6, serverValue:1693}] to mongodb:27017
2025-05-12 18:26:21,269 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 18:26:21,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO CodeGenerator: Code generated in 60.222457 ms
2025-05-12 18:26:21,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:21,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:21,391 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:21,392 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Opened connection [connectionId{localValue:7, serverValue:1694}] to mongodb:27017
2025-05-12 18:26:21,393 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=931536}
2025-05-12 18:26:21,397 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Opened connection [connectionId{localValue:8, serverValue:1695}] to mongodb:27017
2025-05-12 18:26:21,415 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Registering RDD 18 (rdd at MongoSpark.scala:169) as input to shuffle 1
2025-05-12 18:26:21,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Got map stage job 2 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 18:26:21,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (rdd at MongoSpark.scala:169)
2025-05-12 18:26:21,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:21,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:21,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:21,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.8 KiB, free 434.4 MiB)
2025-05-12 18:26:21,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 18:26:21,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1939bc4966ae:45657 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:21,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Closed connection [connectionId{localValue:6, serverValue:1693}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:21,438 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:21,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Closed connection [connectionId{localValue:8, serverValue:1695}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:21,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1939bc4966ae:45657 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:21,440 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:21,441 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:21,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
2025-05-12 18:26:21,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:21,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:32885 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,580 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 136 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:21,581 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
2025-05-12 18:26:21,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: ShuffleMapStage 3 (rdd at MongoSpark.scala:169) finished in 0.162 s
2025-05-12 18:26:21,583 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:21,584 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: running: Set()
2025-05-12 18:26:21,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:21,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:21,591 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:21,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:21,628 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 18:26:21,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Got job 3 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 18:26:21,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Final stage: ResultStage 5 (foreachPartition at MongoSpark.scala:120)
2025-05-12 18:26:21,631 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
2025-05-12 18:26:21,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:21,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:21,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 18:26:21,649 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.3 MiB)
2025-05-12 18:26:21,651 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 1939bc4966ae:45657 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,653 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1939bc4966ae:45657 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,654 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:21,657 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:21,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.23.0.8:32885 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
2025-05-12 18:26:21,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:21,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:32885 (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,715 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.23.0.8:33002
2025-05-12 18:26:21,776 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 113 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:21,777 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
2025-05-12 18:26:21,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: ResultStage 5 (foreachPartition at MongoSpark.scala:120) finished in 0.141 s
2025-05-12 18:26:21,780 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:21,782 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
2025-05-12 18:26:21,782 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO DAGScheduler: Job 3 finished: foreachPartition at MongoSpark.scala:120, took 0.153168 s
2025-05-12 18:26:21,803 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 18:26:21,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 18:26:21,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 1939bc4966ae:45657 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:21,832 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 1939bc4966ae:45657 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:21,849 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.23.0.8:32885 in memory (size: 24.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:21,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:21,914 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:21,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:21,917 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Opened connection [connectionId{localValue:9, serverValue:1696}] to mongodb:27017
2025-05-12 18:26:21,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=975761}
2025-05-12 18:26:21,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Opened connection [connectionId{localValue:10, serverValue:1697}] to mongodb:27017
2025-05-12 18:26:21,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:21,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:21 INFO connection: Closed connection [connectionId{localValue:10, serverValue:1697}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:22,107 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 18:26:22,144 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:22,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO CodeGenerator: Code generated in 74.515326 ms
2025-05-12 18:26:22,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:22,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:22,329 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO connection: Opened connection [connectionId{localValue:11, serverValue:1698}] to mongodb:27017
2025-05-12 18:26:22,333 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1482058}
2025-05-12 18:26:22,338 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO connection: Opened connection [connectionId{localValue:12, serverValue:1699}] to mongodb:27017
2025-05-12 18:26:22,340 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:22,346 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO connection: Closed connection [connectionId{localValue:12, serverValue:1699}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:22,348 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:22,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:22,355 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:22,356 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO connection: Opened connection [connectionId{localValue:13, serverValue:1700}] to mongodb:27017
2025-05-12 18:26:22,359 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1445607}
2025-05-12 18:26:22,367 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO connection: Opened connection [connectionId{localValue:14, serverValue:1701}] to mongodb:27017
2025-05-12 18:26:22,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:22,385 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO connection: Closed connection [connectionId{localValue:14, serverValue:1701}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:22,386 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:22,388 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:22,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:22,395 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Registering RDD 31 (rdd at MongoSpark.scala:169) as input to shuffle 2
2025-05-12 18:26:22,396 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Got map stage job 4 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 18:26:22,397 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (rdd at MongoSpark.scala:169)
2025-05-12 18:26:22,397 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:22,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:22,399 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:22,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 18:26:22,412 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 18:26:22,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 1939bc4966ae:45657 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:22,414 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 1939bc4966ae:45657 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:22,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:22,420 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:22,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
2025-05-12 18:26:22,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:22,460 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.23.0.8:32885 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:22,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 269 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:22,696 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
2025-05-12 18:26:22,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: ShuffleMapStage 6 (rdd at MongoSpark.scala:169) finished in 0.295 s
2025-05-12 18:26:22,698 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:22,699 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: running: Set()
2025-05-12 18:26:22,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:22,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:22,704 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:22,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO CodeGenerator: Code generated in 8.353166 ms
2025-05-12 18:26:22,721 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:22,744 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO CodeGenerator: Code generated in 16.480992 ms
2025-05-12 18:26:22,769 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 18:26:22,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Got job 5 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 18:26:22,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at MongoSpark.scala:120)
2025-05-12 18:26:22,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
2025-05-12 18:26:22,773 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:22,774 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:22,785 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.8 KiB, free 434.3 MiB)
2025-05-12 18:26:22,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)
2025-05-12 18:26:22,796 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 1939bc4966ae:45657 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:22,799 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 1939bc4966ae:45657 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:22,800 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:22,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[38] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:22,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
2025-05-12 18:26:22,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.23.0.8:32885 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:22,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:22,828 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.23.0.8:32885 (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:22,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.23.0.8:33002
2025-05-12 18:26:22,991 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 191 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:22,992 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
2025-05-12 18:26:22,993 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: ResultStage 8 (foreachPartition at MongoSpark.scala:120) finished in 0.216 s
2025-05-12 18:26:22,994 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:22,995 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
2025-05-12 18:26:22,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO DAGScheduler: Job 5 finished: foreachPartition at MongoSpark.scala:120, took 0.224913 s
2025-05-12 18:26:22,999 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 18:26:23,006 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 18:26:23,008 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 1939bc4966ae:45657 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:23,011 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO SparkContext: Created broadcast 9 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:23,017 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 1939bc4966ae:45657 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,018 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.23.0.8:32885 in memory (size: 27.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,076 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 18:26:23,157 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:23,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:23,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:23,161 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Opened connection [connectionId{localValue:16, serverValue:1703}] to mongodb:27017
2025-05-12 18:26:23,162 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=783489}
2025-05-12 18:26:23,164 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Opened connection [connectionId{localValue:17, serverValue:1704}] to mongodb:27017
2025-05-12 18:26:23,165 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:23,165 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Closed connection [connectionId{localValue:17, serverValue:1704}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:23,166 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:23,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:23,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:23,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Opened connection [connectionId{localValue:18, serverValue:1705}] to mongodb:27017
2025-05-12 18:26:23,170 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=830106}
2025-05-12 18:26:23,172 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Opened connection [connectionId{localValue:19, serverValue:1706}] to mongodb:27017
2025-05-12 18:26:23,179 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:23,180 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Closed connection [connectionId{localValue:19, serverValue:1706}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:23,180 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:23,181 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:23,182 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:23,185 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
2025-05-12 18:26:23,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:23,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:23,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:23,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:23,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:23,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 47.9 KiB, free 434.4 MiB)
2025-05-12 18:26:23,204 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 18:26:23,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 1939bc4966ae:45657 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:23,206 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 1939bc4966ae:45657 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,207 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:23,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:23,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
2025-05-12 18:26:23,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:23,245 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.23.0.8:32885 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 109 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:23,319 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
2025-05-12 18:26:23,324 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.132 s
2025-05-12 18:26:23,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:23,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: running: Set()
2025-05-12 18:26:23,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:23,326 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:23,331 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:23,356 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO CodeGenerator: Code generated in 7.615639 ms
2025-05-12 18:26:23,364 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO CodeGenerator: Code generated in 5.665393 ms
2025-05-12 18:26:23,366 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:23,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO CodeGenerator: Code generated in 11.527902 ms
2025-05-12 18:26:23,399 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 18:26:23,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:23,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:23,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
2025-05-12 18:26:23,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:23,403 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:23,408 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 18:26:23,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 18:26:23,418 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 1939bc4966ae:45657 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,419 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 1939bc4966ae:45657 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:23,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.23.0.8:32885 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:23,424 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
2025-05-12 18:26:23,426 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:23,453 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.23.0.8:32885 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.23.0.8:33002
2025-05-12 18:26:23,549 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 124 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:23,552 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
2025-05-12 18:26:23,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.145 s
2025-05-12 18:26:23,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:23,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
2025-05-12 18:26:23,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.152290 s
2025-05-12 18:26:23,611 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 18:26:23,658 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:23,659 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:23,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:23,662 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Opened connection [connectionId{localValue:21, serverValue:1708}] to mongodb:27017
2025-05-12 18:26:23,664 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1322684}
2025-05-12 18:26:23,667 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Opened connection [connectionId{localValue:22, serverValue:1709}] to mongodb:27017
2025-05-12 18:26:23,668 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:23,669 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Closed connection [connectionId{localValue:22, serverValue:1709}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:23,670 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:23,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:23,672 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:23,673 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Opened connection [connectionId{localValue:23, serverValue:1710}] to mongodb:27017
2025-05-12 18:26:23,674 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=954384}
2025-05-12 18:26:23,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Opened connection [connectionId{localValue:24, serverValue:1711}] to mongodb:27017
2025-05-12 18:26:23,684 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:23,685 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO connection: Closed connection [connectionId{localValue:24, serverValue:1711}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:23,686 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:23,687 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:23,688 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:23,691 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Registering RDD 54 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4
2025-05-12 18:26:23,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:23,693 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:23,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:23,694 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:23,695 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:23,699 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 47.9 KiB, free 434.3 MiB)
2025-05-12 18:26:23,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 434.3 MiB)
2025-05-12 18:26:23,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 1939bc4966ae:45657 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,714 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:23,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:23,718 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
2025-05-12 18:26:23,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:23,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.23.0.8:32885 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,729 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 1939bc4966ae:45657 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.23.0.8:32885 (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,832 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 113 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:23,833 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
2025-05-12 18:26:23,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.137 s
2025-05-12 18:26:23,834 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:23,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: running: Set()
2025-05-12 18:26:23,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:23,836 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:23,844 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:23,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:23,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 18:26:23,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:23,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:23,882 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
2025-05-12 18:26:23,883 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:23,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:23,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 52.3 KiB, free 434.3 MiB)
2025-05-12 18:26:23,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.3 MiB)
2025-05-12 18:26:23,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 1939bc4966ae:45657 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:23,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 1939bc4966ae:45657 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:23,907 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
2025-05-12 18:26:23,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:23,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.23.0.8:32885 in memory (size: 21.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,947 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.23.0.8:32885 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:23,964 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.23.0.8:33002
2025-05-12 18:26:23,994 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 86 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:23,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
2025-05-12 18:26:23,996 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.110 s
2025-05-12 18:26:23,997 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:23,997 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
2025-05-12 18:26:23,998 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:23 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.117728 s
2025-05-12 18:26:24,089 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoRelation: requiredColumns: hashtags, created_at, filters: IsNotNull(created_at), IsNotNull(hashtags)
2025-05-12 18:26:24,141 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:24,179 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:24,183 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:24,184 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:24,186 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Opened connection [connectionId{localValue:26, serverValue:1713}] to mongodb:27017
2025-05-12 18:26:24,188 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1383567}
2025-05-12 18:26:24,193 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Opened connection [connectionId{localValue:27, serverValue:1714}] to mongodb:27017
2025-05-12 18:26:24,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:24,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Closed connection [connectionId{localValue:27, serverValue:1714}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:24,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:24,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:24,202 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:24,206 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Opened connection [connectionId{localValue:28, serverValue:1715}] to mongodb:27017
2025-05-12 18:26:24,207 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1214890}
2025-05-12 18:26:24,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Opened connection [connectionId{localValue:29, serverValue:1716}] to mongodb:27017
2025-05-12 18:26:24,223 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:24,224 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Closed connection [connectionId{localValue:29, serverValue:1716}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:24,225 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:24,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:24,228 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:24,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5
2025-05-12 18:26:24,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:24,233 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:24,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:24,234 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:24,235 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:24,243 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 55.9 KiB, free 434.3 MiB)
2025-05-12 18:26:24,261 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.2 MiB)
2025-05-12 18:26:24,263 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 1939bc4966ae:45657 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,267 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:24,268 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:24,275 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 1939bc4966ae:45657 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,293 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
2025-05-12 18:26:24,294 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:24,307 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.23.0.8:32885 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,318 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.23.0.8:32885 (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,405 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 135 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:24,406 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
2025-05-12 18:26:24,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.168 s
2025-05-12 18:26:24,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:24,408 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: running: Set()
2025-05-12 18:26:24,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:24,409 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:24,416 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:24,440 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:24,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO CodeGenerator: Code generated in 22.185618 ms
2025-05-12 18:26:24,498 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 18:26:24,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:24,501 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:24,503 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
2025-05-12 18:26:24,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:24,505 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:24,510 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.3 MiB)
2025-05-12 18:26:24,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 434.2 MiB)
2025-05-12 18:26:24,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 1939bc4966ae:45657 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 1939bc4966ae:45657 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,522 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:24,529 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:24,531 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
2025-05-12 18:26:24,541 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:24,542 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.23.0.8:32885 in memory (size: 23.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,579 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.23.0.8:32885 (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,610 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.23.0.8:33002
2025-05-12 18:26:24,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 175 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:24,706 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
2025-05-12 18:26:24,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.197 s
2025-05-12 18:26:24,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:24,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
2025-05-12 18:26:24,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.206729 s
2025-05-12 18:26:24,718 - SparkScheduler - INFO - [trend_analysis] Hashtag analysis complete: {'daily': 5, 'weekly': 5, 'hourly': 10}
2025-05-12 18:26:24,738 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 18:26:24,749 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 18:26:24,750 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 1939bc4966ae:45657 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:24,752 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO SparkContext: Created broadcast 16 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:24,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 1939bc4966ae:45657 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.23.0.8:32885 in memory (size: 25.7 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:24,778 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:24,779 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:24,783 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Opened connection [connectionId{localValue:31, serverValue:1718}] to mongodb:27017
2025-05-12 18:26:24,784 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1163805}
2025-05-12 18:26:24,789 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Opened connection [connectionId{localValue:32, serverValue:1719}] to mongodb:27017
2025-05-12 18:26:24,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:24,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Closed connection [connectionId{localValue:32, serverValue:1719}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:24,815 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:24,821 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:24,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:24,823 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO connection: Opened connection [connectionId{localValue:33, serverValue:1720}] to mongodb:27017
2025-05-12 18:26:24,867 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:26:24,870 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Got job 12 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:26:24,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:26:24,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:24,872 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:24,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:26:24,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:26:24,886 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:26:24,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 1939bc4966ae:45657 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,896 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:24,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[74] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:24,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
2025-05-12 18:26:24,905 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:26:24,947 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.23.0.8:32885 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:24,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:24 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.23.0.8:32885 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:25,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 161 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:25,067 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
2025-05-12 18:26:25,068 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: ResultStage 18 (treeAggregate at MongoInferSchema.scala:88) finished in 0.197 s
2025-05-12 18:26:25,069 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:25,071 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
2025-05-12 18:26:25,086 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Job 12 finished: treeAggregate at MongoInferSchema.scala:88, took 0.213399 s
2025-05-12 18:26:25,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:25,279 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:25,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:25,283 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Opened connection [connectionId{localValue:34, serverValue:1722}] to mongodb:27017
2025-05-12 18:26:25,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1594026}
2025-05-12 18:26:25,288 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Opened connection [connectionId{localValue:35, serverValue:1723}] to mongodb:27017
2025-05-12 18:26:25,289 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:25,290 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Closed connection [connectionId{localValue:35, serverValue:1723}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:25,320 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 18:26:25,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO CodeGenerator: Code generated in 58.253028 ms
2025-05-12 18:26:25,417 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:25,421 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:25,422 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:25,425 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Opened connection [connectionId{localValue:36, serverValue:1724}] to mongodb:27017
2025-05-12 18:26:25,427 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1148320}
2025-05-12 18:26:25,432 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Opened connection [connectionId{localValue:37, serverValue:1725}] to mongodb:27017
2025-05-12 18:26:25,435 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:25,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Closed connection [connectionId{localValue:37, serverValue:1725}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:25,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:25,440 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:25,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:25,444 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Opened connection [connectionId{localValue:38, serverValue:1726}] to mongodb:27017
2025-05-12 18:26:25,446 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1627213}
2025-05-12 18:26:25,452 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Opened connection [connectionId{localValue:39, serverValue:1727}] to mongodb:27017
2025-05-12 18:26:25,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:25,464 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO connection: Closed connection [connectionId{localValue:39, serverValue:1727}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:25,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:25,466 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:25,467 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:25,472 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Registering RDD 79 (rdd at MongoSpark.scala:169) as input to shuffle 6
2025-05-12 18:26:25,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Got map stage job 13 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 18:26:25,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (rdd at MongoSpark.scala:169)
2025-05-12 18:26:25,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:25,480 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:25,481 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:25,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 43.5 KiB, free 434.3 MiB)
2025-05-12 18:26:25,495 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 434.3 MiB)
2025-05-12 18:26:25,497 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 1939bc4966ae:45657 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:25,504 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 1939bc4966ae:45657 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:25,509 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.23.0.8:32885 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:25,512 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:25,514 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:25,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
2025-05-12 18:26:25,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:25,561 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.23.0.8:32885 (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:25,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 289 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:25,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
2025-05-12 18:26:25,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: ShuffleMapStage 19 (rdd at MongoSpark.scala:169) finished in 0.329 s
2025-05-12 18:26:25,811 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:25,813 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: running: Set()
2025-05-12 18:26:25,814 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:25,815 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:25,824 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:25,832 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:25,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO CodeGenerator: Code generated in 20.406138 ms
2025-05-12 18:26:25,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO CodeGenerator: Code generated in 11.036592 ms
2025-05-12 18:26:25,939 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO SparkContext: Starting job: rdd at MongoSpark.scala:169
2025-05-12 18:26:25,941 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Got job 14 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 18:26:25,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Final stage: ResultStage 21 (rdd at MongoSpark.scala:169)
2025-05-12 18:26:25,943 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
2025-05-12 18:26:25,944 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:25,946 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[84] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:25,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 45.8 KiB, free 434.3 MiB)
2025-05-12 18:26:25,961 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 434.3 MiB)
2025-05-12 18:26:25,963 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 1939bc4966ae:45657 (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:25,965 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 1939bc4966ae:45657 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:25,969 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:25,972 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[84] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:25,973 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
2025-05-12 18:26:25,974 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.23.0.8:32885 in memory (size: 20.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:25,975 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:25 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:26,028 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.23.0.8:32885 (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.23.0.8:33002
2025-05-12 18:26:26,159 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 185 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:26,160 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
2025-05-12 18:26:26,165 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: ResultStage 21 (rdd at MongoSpark.scala:169) finished in 0.213 s
2025-05-12 18:26:26,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:26,167 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
2025-05-12 18:26:26,169 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Job 14 finished: rdd at MongoSpark.scala:169, took 0.226901 s
2025-05-12 18:26:26,189 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Registering RDD 85 (rdd at MongoSpark.scala:169) as input to shuffle 7
2025-05-12 18:26:26,190 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Got map stage job 15 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 18:26:26,191 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (rdd at MongoSpark.scala:169)
2025-05-12 18:26:26,192 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
2025-05-12 18:26:26,196 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:26,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[85] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:26,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 46.0 KiB, free 434.3 MiB)
2025-05-12 18:26:26,208 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 21.2 KiB, free 434.3 MiB)
2025-05-12 18:26:26,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 1939bc4966ae:45657 (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 1939bc4966ae:45657 in memory (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:26,213 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[85] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:26,215 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
2025-05-12 18:26:26,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 15) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 18:26:26,226 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.23.0.8:32885 in memory (size: 21.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.23.0.8:32885 (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,325 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 15) in 111 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:26,327 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
2025-05-12 18:26:26,328 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: ShuffleMapStage 23 (rdd at MongoSpark.scala:169) finished in 0.132 s
2025-05-12 18:26:26,330 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:26,335 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: running: Set()
2025-05-12 18:26:26,336 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:26,337 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:26,339 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:26,362 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO CodeGenerator: Code generated in 14.97678 ms
2025-05-12 18:26:26,398 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 18:26:26,400 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Got job 16 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 18:26:26,401 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Final stage: ResultStage 26 (foreachPartition at MongoSpark.scala:120)
2025-05-12 18:26:26,402 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
2025-05-12 18:26:26,404 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:26,408 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[91] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:26,413 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 52.0 KiB, free 434.3 MiB)
2025-05-12 18:26:26,425 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 22.9 KiB, free 434.3 MiB)
2025-05-12 18:26:26,428 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 1939bc4966ae:45657 (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,431 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:26,433 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[91] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:26,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
2025-05-12 18:26:26,441 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 16) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:26,442 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 1939bc4966ae:45657 in memory (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,445 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.23.0.8:32885 in memory (size: 21.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.23.0.8:32885 (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.23.0.8:33002
2025-05-12 18:26:26,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 16) in 200 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:26,641 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
2025-05-12 18:26:26,642 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: ResultStage 26 (foreachPartition at MongoSpark.scala:120) finished in 0.236 s
2025-05-12 18:26:26,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:26,643 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
2025-05-12 18:26:26,644 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Job 16 finished: foreachPartition at MongoSpark.scala:120, took 0.244017 s
2025-05-12 18:26:26,648 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 18:26:26,654 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 18:26:26,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 1939bc4966ae:45657 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:26,667 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO SparkContext: Created broadcast 22 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:26,671 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.23.0.8:32885 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,676 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 1939bc4966ae:45657 in memory (size: 22.9 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,700 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:26,702 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:26,703 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:26,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Opened connection [connectionId{localValue:40, serverValue:1728}] to mongodb:27017
2025-05-12 18:26:26,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2645368}
2025-05-12 18:26:26,719 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Opened connection [connectionId{localValue:41, serverValue:1729}] to mongodb:27017
2025-05-12 18:26:26,722 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:26,725 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Closed connection [connectionId{localValue:41, serverValue:1729}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:26,768 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 18:26:26,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO CodeGenerator: Code generated in 56.281381 ms
2025-05-12 18:26:26,862 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:26,863 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:26,864 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:26,866 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Opened connection [connectionId{localValue:42, serverValue:1730}] to mongodb:27017
2025-05-12 18:26:26,868 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1418929}
2025-05-12 18:26:26,871 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Opened connection [connectionId{localValue:43, serverValue:1731}] to mongodb:27017
2025-05-12 18:26:26,873 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:26,874 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Closed connection [connectionId{localValue:43, serverValue:1731}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:26,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:26,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:26,878 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:26,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Opened connection [connectionId{localValue:44, serverValue:1732}] to mongodb:27017
2025-05-12 18:26:26,881 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1286267}
2025-05-12 18:26:26,884 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Opened connection [connectionId{localValue:45, serverValue:1733}] to mongodb:27017
2025-05-12 18:26:26,891 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:26,892 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO connection: Closed connection [connectionId{localValue:45, serverValue:1733}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:26,893 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:26,894 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:26,895 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:26,898 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Registering RDD 97 (rdd at MongoSpark.scala:169) as input to shuffle 8
2025-05-12 18:26:26,899 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Got map stage job 17 (rdd at MongoSpark.scala:169) with 1 output partitions
2025-05-12 18:26:26,900 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (rdd at MongoSpark.scala:169)
2025-05-12 18:26:26,901 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:26,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:26,902 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[97] at rdd at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:26,904 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 39.6 KiB, free 434.4 MiB)
2025-05-12 18:26:26,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 18:26:26,915 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 1939bc4966ae:45657 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:26,916 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 1939bc4966ae:45657 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:26,918 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:26,921 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[97] at rdd at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:26,929 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
2025-05-12 18:26:26,934 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 17) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:26,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:26 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.23.0.8:32885 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 17) in 136 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:27,057 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
2025-05-12 18:26:27,058 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: ShuffleMapStage 27 (rdd at MongoSpark.scala:169) finished in 0.157 s
2025-05-12 18:26:27,059 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:27,060 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: running: Set()
2025-05-12 18:26:27,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:27,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:27,066 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:27,072 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:27,096 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:120
2025-05-12 18:26:27,098 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Got job 18 (foreachPartition at MongoSpark.scala:120) with 1 output partitions
2025-05-12 18:26:27,099 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Final stage: ResultStage 29 (foreachPartition at MongoSpark.scala:120)
2025-05-12 18:26:27,100 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
2025-05-12 18:26:27,101 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:27,102 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[104] at map at MongoSpark.scala:169), which has no missing parents
2025-05-12 18:26:27,104 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.9 KiB, free 434.3 MiB)
2025-05-12 18:26:27,112 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 434.3 MiB)
2025-05-12 18:26:27,113 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 1939bc4966ae:45657 (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:27,117 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[104] at map at MongoSpark.scala:169) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:27,118 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 1939bc4966ae:45657 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,120 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
2025-05-12 18:26:27,121 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 18) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:27,122 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.23.0.8:32885 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,145 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.23.0.8:32885 (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,164 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.23.0.8:33002
2025-05-12 18:26:27,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 18) in 109 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:27,227 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
2025-05-12 18:26:27,228 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: ResultStage 29 (foreachPartition at MongoSpark.scala:120) finished in 0.126 s
2025-05-12 18:26:27,229 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:27,230 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
2025-05-12 18:26:27,232 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Job 18 finished: foreachPartition at MongoSpark.scala:120, took 0.131604 s
2025-05-12 18:26:27,237 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 200.0 B, free 434.3 MiB)
2025-05-12 18:26:27,245 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 410.0 B, free 434.3 MiB)
2025-05-12 18:26:27,249 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 1939bc4966ae:45657 (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:27,255 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO SparkContext: Created broadcast 25 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:27,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 1939bc4966ae:45657 in memory (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,262 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.23.0.8:32885 in memory (size: 22.2 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,352 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MongoRelation: requiredColumns: user_id, created_at, filters:
2025-05-12 18:26:27,423 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:27,453 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO CodeGenerator: Code generated in 21.652308 ms
2025-05-12 18:26:27,459 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:27,461 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:27,462 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:27,463 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO connection: Opened connection [connectionId{localValue:47, serverValue:1735}] to mongodb:27017
2025-05-12 18:26:27,465 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1292419}
2025-05-12 18:26:27,467 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO connection: Opened connection [connectionId{localValue:48, serverValue:1736}] to mongodb:27017
2025-05-12 18:26:27,469 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:27,474 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO connection: Closed connection [connectionId{localValue:48, serverValue:1736}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:27,475 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:27,476 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:27,477 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:27,478 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO connection: Opened connection [connectionId{localValue:49, serverValue:1737}] to mongodb:27017
2025-05-12 18:26:27,479 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1288853}
2025-05-12 18:26:27,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO connection: Opened connection [connectionId{localValue:50, serverValue:1738}] to mongodb:27017
2025-05-12 18:26:27,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:27,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO connection: Closed connection [connectionId{localValue:50, serverValue:1738}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:27,493 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:27,494 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:27,495 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:27,498 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Registering RDD 110 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
2025-05-12 18:26:27,499 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Got map stage job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:27,500 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:27,501 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:27,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:27,502 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:27,506 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 36.4 KiB, free 434.4 MiB)
2025-05-12 18:26:27,515 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 434.3 MiB)
2025-05-12 18:26:27,517 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 1939bc4966ae:45657 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,520 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:27,521 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 1939bc4966ae:45657 in memory (size: 410.0 B, free: 434.4 MiB)
2025-05-12 18:26:27,523 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:27,526 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
2025-05-12 18:26:27,532 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 19) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:27,553 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.23.0.8:32885 (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,635 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 19) in 114 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:27,636 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
2025-05-12 18:26:27,637 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: ShuffleMapStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.135 s
2025-05-12 18:26:27,638 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:27,639 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: running: Set()
2025-05-12 18:26:27,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:27,640 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:27,646 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:27,660 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:27,677 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO CodeGenerator: Code generated in 13.370831 ms
2025-05-12 18:26:27,689 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Registering RDD 113 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
2025-05-12 18:26:27,690 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:27,690 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:27,691 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
2025-05-12 18:26:27,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:27,692 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:27,697 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 45.0 KiB, free 434.3 MiB)
2025-05-12 18:26:27,705 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.3 MiB)
2025-05-12 18:26:27,707 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 1939bc4966ae:45657 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:27,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 1939bc4966ae:45657 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,710 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:27,711 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
2025-05-12 18:26:27,713 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 20) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
2025-05-12 18:26:27,723 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.23.0.8:32885 in memory (size: 16.9 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,750 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.23.0.8:32885 (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,801 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.23.0.8:33002
2025-05-12 18:26:27,854 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 20) in 143 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:27,855 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
2025-05-12 18:26:27,857 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0) finished in 0.163 s
2025-05-12 18:26:27,858 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:27,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: running: Set()
2025-05-12 18:26:27,859 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:27,860 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:27,889 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO CodeGenerator: Code generated in 13.264476 ms
2025-05-12 18:26:27,906 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 18:26:27,908 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:27,909 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Final stage: ResultStage 35 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:27,910 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
2025-05-12 18:26:27,911 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:27,912 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:27,913 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 11.1 KiB, free 434.3 MiB)
2025-05-12 18:26:27,923 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)
2025-05-12 18:26:27,927 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 1939bc4966ae:45657 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,928 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 1939bc4966ae:45657 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,931 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:27,937 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:27,938 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
2025-05-12 18:26:27,939 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 21) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:27,940 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.23.0.8:32885 in memory (size: 20.4 KiB, free: 434.4 MiB)
2025-05-12 18:26:27,984 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:27 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.23.0.8:32885 (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,013 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.23.0.8:33002
2025-05-12 18:26:28,074 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 21) in 138 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:28,078 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
2025-05-12 18:26:28,079 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: ResultStage 35 (count at NativeMethodAccessorImpl.java:0) finished in 0.165 s
2025-05-12 18:26:28,080 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:28,081 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
2025-05-12 18:26:28,082 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.170428 s
2025-05-12 18:26:28,122 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoRelation: requiredColumns: user_id, filters:
2025-05-12 18:26:28,193 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:28,195 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:28,197 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:28,199 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Opened connection [connectionId{localValue:52, serverValue:1740}] to mongodb:27017
2025-05-12 18:26:28,201 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1407282}
2025-05-12 18:26:28,205 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Opened connection [connectionId{localValue:53, serverValue:1741}] to mongodb:27017
2025-05-12 18:26:28,207 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:28,209 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Closed connection [connectionId{localValue:53, serverValue:1741}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:28,210 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:28,211 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:28,212 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:28,216 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Opened connection [connectionId{localValue:54, serverValue:1742}] to mongodb:27017
2025-05-12 18:26:28,217 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1195827}
2025-05-12 18:26:28,221 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Opened connection [connectionId{localValue:55, serverValue:1743}] to mongodb:27017
2025-05-12 18:26:28,236 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:28,237 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Closed connection [connectionId{localValue:55, serverValue:1743}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:28,238 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:28,239 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:28,241 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:28,247 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Registering RDD 121 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 11
2025-05-12 18:26:28,251 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Got map stage job 22 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:28,252 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:28,253 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:28,254 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:28,254 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[121] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:28,257 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 39.7 KiB, free 434.3 MiB)
2025-05-12 18:26:28,270 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
2025-05-12 18:26:28,273 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 1939bc4966ae:45657 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,277 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 1939bc4966ae:45657 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,280 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:28,282 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[121] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:28,284 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
2025-05-12 18:26:28,286 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.23.0.8:32885 in memory (size: 5.5 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,288 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 22) (172.23.0.8, executor 0, partition 0, ANY, 4601 bytes) taskResourceAssignments Map()
2025-05-12 18:26:28,316 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.23.0.8:32885 (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,377 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 22) in 96 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:28,378 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
2025-05-12 18:26:28,379 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: ShuffleMapStage 36 (count at NativeMethodAccessorImpl.java:0) finished in 0.125 s
2025-05-12 18:26:28,380 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: looking for newly runnable stages
2025-05-12 18:26:28,381 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: running: Set()
2025-05-12 18:26:28,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: waiting: Set()
2025-05-12 18:26:28,382 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: failed: Set()
2025-05-12 18:26:28,390 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO ShufflePartitionsUtil: For shuffle(11), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
2025-05-12 18:26:28,407 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2025-05-12 18:26:28,434 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
2025-05-12 18:26:28,436 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Got job 23 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
2025-05-12 18:26:28,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Final stage: ResultStage 38 (count at NativeMethodAccessorImpl.java:0)
2025-05-12 18:26:28,437 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
2025-05-12 18:26:28,438 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:28,439 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[126] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-12 18:26:28,443 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 46.3 KiB, free 434.3 MiB)
2025-05-12 18:26:28,453 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 434.3 MiB)
2025-05-12 18:26:28,468 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 1939bc4966ae:45657 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,470 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 1939bc4966ae:45657 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,482 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:28,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[126] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:28,483 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
2025-05-12 18:26:28,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 23) (172.23.0.8, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
2025-05-12 18:26:28,485 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.23.0.8:32885 in memory (size: 18.6 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,492 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.23.0.8:32885 (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,518 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 172.23.0.8:33002
2025-05-12 18:26:28,554 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 23) in 94 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:28,555 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
2025-05-12 18:26:28,556 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: ResultStage 38 (count at NativeMethodAccessorImpl.java:0) finished in 0.116 s
2025-05-12 18:26:28,557 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:28,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
2025-05-12 18:26:28,558 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Job 23 finished: count at NativeMethodAccessorImpl.java:0, took 0.122313 s
2025-05-12 18:26:28,559 - SparkScheduler - INFO - [trend_analysis] User activity analysis complete: {'daily_activity_records': 2, 'active_users': 2}
2025-05-12 18:26:28,575 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 192.0 B, free 434.3 MiB)
2025-05-12 18:26:28,585 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.3 MiB)
2025-05-12 18:26:28,588 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 1939bc4966ae:45657 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:28,589 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 1939bc4966ae:45657 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Created broadcast 31 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:28,598 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:28,599 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:28,600 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.23.0.8:32885 in memory (size: 21.1 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,601 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:28,602 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Opened connection [connectionId{localValue:57, serverValue:1745}] to mongodb:27017
2025-05-12 18:26:28,603 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1645003}
2025-05-12 18:26:28,603 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Opened connection [connectionId{localValue:58, serverValue:1746}] to mongodb:27017
2025-05-12 18:26:28,604 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:28,604 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Closed connection [connectionId{localValue:58, serverValue:1746}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:28,606 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:28,608 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:28,610 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:28,625 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:26:28,627 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Got job 24 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:26:28,628 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Final stage: ResultStage 39 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:26:28,629 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:28,630 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:28,631 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[131] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:26:28,632 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:26:28,633 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:26:28,633 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 1939bc4966ae:45657 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,634 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:28,635 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[131] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:28,635 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
2025-05-12 18:26:28,645 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 24) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:26:28,667 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.23.0.8:32885 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,687 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.23.0.8:32885 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:28,708 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 24) in 64 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:28,709 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
2025-05-12 18:26:28,712 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: ResultStage 39 (treeAggregate at MongoInferSchema.scala:88) finished in 0.080 s
2025-05-12 18:26:28,715 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:28,716 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
2025-05-12 18:26:28,717 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Job 24 finished: treeAggregate at MongoInferSchema.scala:88, took 0.085635 s
2025-05-12 18:26:28,732 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:26:28,741 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:26:28,755 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 1939bc4966ae:45657 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:28,756 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Created broadcast 33 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:28,757 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:28,759 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:28,762 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 1939bc4966ae:45657 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,763 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:28,763 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.23.0.8:32885 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,764 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Opened connection [connectionId{localValue:60, serverValue:1748}] to mongodb:27017
2025-05-12 18:26:28,765 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1016221}
2025-05-12 18:26:28,766 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Opened connection [connectionId{localValue:61, serverValue:1749}] to mongodb:27017
2025-05-12 18:26:28,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:28,767 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO connection: Closed connection [connectionId{localValue:61, serverValue:1749}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:28,770 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:28,771 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:28,772 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:28,786 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:26:28,790 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Got job 25 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:26:28,791 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Final stage: ResultStage 40 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:26:28,792 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:28,793 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:28,794 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[136] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:26:28,804 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:26:28,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:26:28,805 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 1939bc4966ae:45657 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,806 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:28,807 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[136] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:28,808 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
2025-05-12 18:26:28,810 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 25) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:26:28,835 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.23.0.8:32885 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:28,861 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.23.0.8:32885 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:28,874 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 25) in 65 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:28,875 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
2025-05-12 18:26:28,876 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: ResultStage 40 (treeAggregate at MongoInferSchema.scala:88) finished in 0.083 s
2025-05-12 18:26:28,877 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:28,879 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
2025-05-12 18:26:28,880 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO DAGScheduler: Job 25 finished: treeAggregate at MongoInferSchema.scala:88, took 0.090790 s
2025-05-12 18:26:28,945 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO SparkUI: Stopped Spark web UI at http://1939bc4966ae:4040
2025-05-12 18:26:28,950 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 18:26:28,952 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 18:26:28,990 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 18:26:29,037 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO MemoryStore: MemoryStore cleared
2025-05-12 18:26:29,040 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO BlockManager: BlockManager stopped
2025-05-12 18:26:29,052 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 18:26:29,061 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 18:26:29,108 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 18:26:29,506 - SparkScheduler - INFO - [trend_analysis] Traceback (most recent call last):
2025-05-12 18:26:29,506 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 210, in <module>
2025-05-12 18:26:29,508 - SparkScheduler - INFO - [trend_analysis] main()
2025-05-12 18:26:29,509 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 199, in main
2025-05-12 18:26:29,511 - SparkScheduler - INFO - [trend_analysis] engagement_results = analyze_engagement_metrics(spark)
2025-05-12 18:26:29,511 - SparkScheduler - INFO - [trend_analysis] File "/opt/spark-jobs/trend_analysis.py", line 160, in analyze_engagement_metrics
2025-05-12 18:26:29,513 - SparkScheduler - INFO - [trend_analysis] .join(likes_df, tweets_df["id"] == likes_df["tweet_id"], "left")
2025-05-12 18:26:29,513 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1965, in __getitem__
2025-05-12 18:26:29,513 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
2025-05-12 18:26:29,514 - SparkScheduler - INFO - [trend_analysis] File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
2025-05-12 18:26:29,520 - SparkScheduler - INFO - [trend_analysis] pyspark.sql.utils.AnalysisException: Cannot resolve column name "tweet_id" among ()
2025-05-12 18:26:29,573 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 18:26:29,574 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e5ef772-ad8b-40fc-93e0-a2c6e2d153b1
2025-05-12 18:26:29,582 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-ebea3c01-9fc9-4ff9-b766-9a50182d9782
2025-05-12 18:26:29,590 - SparkScheduler - INFO - [trend_analysis] 25/05/12 18:26:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e5ef772-ad8b-40fc-93e0-a2c6e2d153b1/pyspark-3aade560-9cd4-46a7-a33d-9e1462828840
2025-05-12 18:26:29,634 - SparkScheduler - ERROR - [trend_analysis] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 18:26:29,634 - SparkScheduler - ERROR - [trend_analysis] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 18:26:29,635 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 18:26:29,635 - SparkScheduler - ERROR - [trend_analysis] :: resolving dependencies :: org.apache.spark#spark-submit-parent-8268e695-c089-4cfb-b4fd-f3ff3c5a41ec;1.0
2025-05-12 18:26:29,635 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 18:26:29,636 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 18:26:29,636 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 18:26:29,636 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#bson;4.0.5 in central
2025-05-12 18:26:29,637 - SparkScheduler - ERROR - [trend_analysis] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 18:26:29,637 - SparkScheduler - ERROR - [trend_analysis] :: resolution report :: resolve 266ms :: artifacts dl 15ms
2025-05-12 18:26:29,637 - SparkScheduler - ERROR - [trend_analysis] :: modules in use:
2025-05-12 18:26:29,638 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 18:26:29,638 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 18:26:29,638 - SparkScheduler - ERROR - [trend_analysis] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 18:26:29,639 - SparkScheduler - ERROR - [trend_analysis] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 18:26:29,639 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 18:26:29,639 - SparkScheduler - ERROR - [trend_analysis] |                  |            modules            ||   artifacts   |
2025-05-12 18:26:29,640 - SparkScheduler - ERROR - [trend_analysis] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 18:26:29,640 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 18:26:29,640 - SparkScheduler - ERROR - [trend_analysis] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 18:26:29,640 - SparkScheduler - ERROR - [trend_analysis] ---------------------------------------------------------------------
2025-05-12 18:26:29,641 - SparkScheduler - ERROR - [trend_analysis] :: retrieving :: org.apache.spark#spark-submit-parent-8268e695-c089-4cfb-b4fd-f3ff3c5a41ec
2025-05-12 18:26:29,641 - SparkScheduler - ERROR - [trend_analysis] confs: [default]
2025-05-12 18:26:29,641 - SparkScheduler - ERROR - [trend_analysis] 0 artifacts copied, 4 already retrieved (0kB/12ms)
2025-05-12 18:26:29,644 - SparkScheduler - ERROR - Job trend_analysis failed with exit code 1
2025-05-12 18:26:29,645 - SparkScheduler - INFO - Job trend_analysis duration: 30.95 seconds
2025-05-12 18:26:29,646 - SparkScheduler - INFO - Starting job: discover_feed_generator - Generate discover feed recommendations
2025-05-12 18:26:29,647 - SparkScheduler - INFO - Running command: /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --conf spark.mongodb.input.uri=mongodb://mongodb:27017/mini_twitter --conf spark.mongodb.output.uri=mongodb://mongodb:27017/mini_twitter_analytics --conf spark.driver.memory=1g --conf spark.executor.memory=1g /opt/spark-jobs/discover_feed_generator.py
2025-05-12 18:26:32,059 - SparkScheduler - INFO - [discover_feed_generator] :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
2025-05-12 18:26:32,793 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-12 18:26:34,039 - SparkScheduler - INFO - [discover_feed_generator] Starting discover feed generation...
2025-05-12 18:26:34,132 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SparkContext: Running Spark version 3.3.4
2025-05-12 18:26:34,164 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO ResourceUtils: ==============================================================
2025-05-12 18:26:34,164 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO ResourceUtils: No custom resources configured for spark.driver.
2025-05-12 18:26:34,165 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO ResourceUtils: ==============================================================
2025-05-12 18:26:34,166 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SparkContext: Submitted application: DiscoverFeedGenerator
2025-05-12 18:26:34,197 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-12 18:26:34,207 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO ResourceProfile: Limiting resource is cpu
2025-05-12 18:26:34,208 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
2025-05-12 18:26:34,259 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SecurityManager: Changing view acls to: spark
2025-05-12 18:26:34,260 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SecurityManager: Changing modify acls to: spark
2025-05-12 18:26:34,261 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SecurityManager: Changing view acls groups to:
2025-05-12 18:26:34,262 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SecurityManager: Changing modify acls groups to:
2025-05-12 18:26:34,263 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
2025-05-12 18:26:34,518 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO Utils: Successfully started service 'sparkDriver' on port 39429.
2025-05-12 18:26:34,550 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SparkEnv: Registering MapOutputTracker
2025-05-12 18:26:34,589 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SparkEnv: Registering BlockManagerMaster
2025-05-12 18:26:34,611 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-12 18:26:34,613 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-12 18:26:34,625 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-12 18:26:34,643 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e34e7890-e0b0-4786-ad65-f43504b4f7d6
2025-05-12 18:26:34,660 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
2025-05-12 18:26:34,678 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO SparkEnv: Registering OutputCommitCoordinator
2025-05-12 18:26:34,993 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-12 18:26:35,037 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:39429/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747074394125
2025-05-12 18:26:35,038 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:39429/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747074394125
2025-05-12 18:26:35,039 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:39429/jars/org.mongodb_bson-4.0.5.jar with timestamp 1747074394125
2025-05-12 18:26:35,039 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:39429/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747074394125
2025-05-12 18:26:35,044 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://1939bc4966ae:39429/files/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1747074394125
2025-05-12 18:26:35,047 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-46ab3f4e-3b60-43c0-a815-b5eb3effddfa/userFiles-b27b1db7-8cbb-47a4-87ab-410e9df7bfa8/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
2025-05-12 18:26:35,065 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://1939bc4966ae:39429/files/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1747074394125
2025-05-12 18:26:35,066 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-46ab3f4e-3b60-43c0-a815-b5eb3effddfa/userFiles-b27b1db7-8cbb-47a4-87ab-410e9df7bfa8/org.mongodb_mongodb-driver-sync-4.0.5.jar
2025-05-12 18:26:35,077 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://1939bc4966ae:39429/files/org.mongodb_bson-4.0.5.jar with timestamp 1747074394125
2025-05-12 18:26:35,077 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-46ab3f4e-3b60-43c0-a815-b5eb3effddfa/userFiles-b27b1db7-8cbb-47a4-87ab-410e9df7bfa8/org.mongodb_bson-4.0.5.jar
2025-05-12 18:26:35,092 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://1939bc4966ae:39429/files/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1747074394125
2025-05-12 18:26:35,092 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-46ab3f4e-3b60-43c0-a815-b5eb3effddfa/userFiles-b27b1db7-8cbb-47a4-87ab-410e9df7bfa8/org.mongodb_mongodb-driver-core-4.0.5.jar
2025-05-12 18:26:35,263 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2025-05-12 18:26:35,331 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.7:7077 after 33 ms (0 ms spent in bootstraps)
2025-05-12 18:26:35,451 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250512182635-0005
2025-05-12 18:26:35,454 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250512182635-0005/0 on worker-20250512175431-172.23.0.8-44645 (172.23.0.8:44645) with 2 core(s)
2025-05-12 18:26:35,459 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20250512182635-0005/0 on hostPort 172.23.0.8:44645 with 2 core(s), 1024.0 MiB RAM
2025-05-12 18:26:35,462 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46265.
2025-05-12 18:26:35,463 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO NettyBlockTransferService: Server created on 1939bc4966ae:46265
2025-05-12 18:26:35,465 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-12 18:26:35,475 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1939bc4966ae, 46265, None)
2025-05-12 18:26:35,480 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO BlockManagerMasterEndpoint: Registering block manager 1939bc4966ae:46265 with 434.4 MiB RAM, BlockManagerId(driver, 1939bc4966ae, 46265, None)
2025-05-12 18:26:35,484 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1939bc4966ae, 46265, None)
2025-05-12 18:26:35,487 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1939bc4966ae, 46265, None)
2025-05-12 18:26:35,513 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250512182635-0005/0 is now RUNNING
2025-05-12 18:26:35,864 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:35 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-05-12 18:26:36,210 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-12 18:26:36,215 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:36 INFO SharedState: Warehouse path is 'file:/opt/spark-jobs/spark-warehouse'.
2025-05-12 18:26:37,700 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:26:37,770 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:26:37,775 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1939bc4966ae:46265 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:37,783 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:37 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:37,940 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:37 INFO cluster: Cluster created with settings {hosts=[mongodb:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2025-05-12 18:26:38,007 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO MongoClientCache: Creating MongoClient: [mongodb:27017]
2025-05-12 18:26:38,027 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out
2025-05-12 18:26:38,064 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO connection: Opened connection [connectionId{localValue:1, serverValue:1752}] to mongodb:27017
2025-05-12 18:26:38,077 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=mongodb:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=9547782}
2025-05-12 18:26:38,113 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO connection: Opened connection [connectionId{localValue:2, serverValue:1753}] to mongodb:27017
2025-05-12 18:26:38,871 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:26:38,907 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO DAGScheduler: Got job 0 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:26:38,911 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:26:38,914 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:38,919 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:38,946 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:26:38,998 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:26:39,009 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:26:39,012 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1939bc4966ae:46265 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:39,017 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:39,068 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:39,071 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
2025-05-12 18:26:39,866 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.8:53702) with ID 0,  ResourceProfileId 0
2025-05-12 18:26:39,987 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.8:37101 with 434.4 MiB RAM, BlockManagerId(0, 172.23.0.8, 37101, None)
2025-05-12 18:26:40,358 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:26:40,644 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.23.0.8:37101 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:40,863 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.8:37101 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:41,141 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 816 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:41,142 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
2025-05-12 18:26:41,148 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:41 INFO DAGScheduler: ResultStage 0 (treeAggregate at MongoInferSchema.scala:88) finished in 2.170 s
2025-05-12 18:26:41,152 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:41,153 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
2025-05-12 18:26:41,196 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:41 INFO DAGScheduler: Job 0 finished: treeAggregate at MongoInferSchema.scala:88, took 2.323878 s
2025-05-12 18:26:42,214 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1939bc4966ae:46265 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:42,227 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.23.0.8:37101 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:43,155 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:26:43,161 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:26:43,163 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1939bc4966ae:46265 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:43,164 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO SparkContext: Created broadcast 2 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:43,193 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:26:43,195 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Got job 1 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:26:43,195 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Final stage: ResultStage 1 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:26:43,196 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:43,197 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:43,198 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:26:43,202 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:26:43,208 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:26:43,209 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1939bc4966ae:46265 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:43,210 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:43,212 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:43,212 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
2025-05-12 18:26:43,216 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:26:43,241 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.23.0.8:37101 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:43,269 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.23.0.8:37101 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:43,287 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 71 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:43,288 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
2025-05-12 18:26:43,289 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: ResultStage 1 (treeAggregate at MongoInferSchema.scala:88) finished in 0.089 s
2025-05-12 18:26:43,290 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:43,290 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
2025-05-12 18:26:43,291 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Job 1 finished: treeAggregate at MongoInferSchema.scala:88, took 0.097047 s
2025-05-12 18:26:43,307 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 192.0 B, free 434.4 MiB)
2025-05-12 18:26:43,320 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 401.0 B, free 434.4 MiB)
2025-05-12 18:26:43,322 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1939bc4966ae:46265 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:43,326 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO SparkContext: Created broadcast 4 from broadcast at MongoSpark.scala:530
2025-05-12 18:26:43,327 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1939bc4966ae:46265 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:43,332 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.23.0.8:37101 in memory (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:43,359 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88
2025-05-12 18:26:43,361 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Got job 2 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions
2025-05-12 18:26:43,362 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Final stage: ResultStage 2 (treeAggregate at MongoInferSchema.scala:88)
2025-05-12 18:26:43,362 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Parents of final stage: List()
2025-05-12 18:26:43,363 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Missing parents: List()
2025-05-12 18:26:43,364 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents
2025-05-12 18:26:43,370 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.1 KiB, free 434.4 MiB)
2025-05-12 18:26:43,379 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
2025-05-12 18:26:43,381 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1939bc4966ae:46265 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:43,382 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
2025-05-12 18:26:43,385 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))
2025-05-12 18:26:43,386 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
2025-05-12 18:26:43,390 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.23.0.8, executor 0, partition 0, ANY, 4612 bytes) taskResourceAssignments Map()
2025-05-12 18:26:43,418 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.23.0.8:37101 (size: 4.0 KiB, free: 434.4 MiB)
2025-05-12 18:26:43,440 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.23.0.8:37101 (size: 401.0 B, free: 434.4 MiB)
2025-05-12 18:26:43,459 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 69 ms on 172.23.0.8 (executor 0) (1/1)
2025-05-12 18:26:43,460 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
2025-05-12 18:26:43,461 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: ResultStage 2 (treeAggregate at MongoInferSchema.scala:88) finished in 0.095 s
2025-05-12 18:26:43,462 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-12 18:26:43,463 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
2025-05-12 18:26:43,463 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO DAGScheduler: Job 2 finished: treeAggregate at MongoInferSchema.scala:88, took 0.103515 s
2025-05-12 18:26:43,716 - SparkScheduler - INFO - [discover_feed_generator] Error generating discover feed: Column 'likes_count' does not exist. Did you mean one of the following? [content, created_at, user_id, _id, hashtags, id, media_urls, retweeted_from, original_user_id]; line 1 pos 10;
2025-05-12 18:26:43,717 - SparkScheduler - INFO - [discover_feed_generator] 'Project [_id#0, content#1, created_at#2, hashtags#3, id#4, media_urls#5, original_user_id#6, retweeted_from#7, user_id#8, ((('COALESCE('likes_count, 0) * 1) + ('COALESCE('comments_count, 0) * 2)) + ('COALESCE('retweet_count, 0) * 3)) AS engagement_score#18]
2025-05-12 18:26:43,717 - SparkScheduler - INFO - [discover_feed_generator] +- Relation [_id#0,content#1,created_at#2,hashtags#3,id#4,media_urls#5,original_user_id#6,retweeted_from#7,user_id#8] MongoRelation(MongoRDD[0] at RDD at MongoRDD.scala:51,Some(StructType(StructField(_id,StructType(StructField(oid,StringType,true)),true),StructField(content,StringType,true),StructField(created_at,TimestampType,true),StructField(hashtags,ArrayType(StringType,true),true),StructField(id,StringType,true),StructField(media_urls,ArrayType(StringType,true),true),StructField(original_user_id,StringType,true),StructField(retweeted_from,StringType,true),StructField(user_id,StringType,true))))
2025-05-12 18:26:43,718 - SparkScheduler - INFO - [discover_feed_generator] 
2025-05-12 18:26:43,731 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO SparkUI: Stopped Spark web UI at http://1939bc4966ae:4040
2025-05-12 18:26:43,735 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO StandaloneSchedulerBackend: Shutting down all executors
2025-05-12 18:26:43,736 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2025-05-12 18:26:43,770 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-12 18:26:43,807 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MongoClientCache: Closing MongoClient: [mongodb:27017]
2025-05-12 18:26:43,811 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO connection: Closed connection [connectionId{localValue:2, serverValue:1753}] to mongodb:27017 because the pool has been closed.
2025-05-12 18:26:43,814 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO MemoryStore: MemoryStore cleared
2025-05-12 18:26:43,814 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManager: BlockManager stopped
2025-05-12 18:26:43,819 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO BlockManagerMaster: BlockManagerMaster stopped
2025-05-12 18:26:43,824 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-12 18:26:43,845 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO SparkContext: Successfully stopped SparkContext
2025-05-12 18:26:43,916 - SparkScheduler - INFO - [discover_feed_generator] Discover feed generation completed
2025-05-12 18:26:43,979 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO ShutdownHookManager: Shutdown hook called
2025-05-12 18:26:43,981 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-46ab3f4e-3b60-43c0-a815-b5eb3effddfa/pyspark-dab217fc-29cb-4748-905e-521fcb1c56ff
2025-05-12 18:26:43,992 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-46ab3f4e-3b60-43c0-a815-b5eb3effddfa
2025-05-12 18:26:44,002 - SparkScheduler - INFO - [discover_feed_generator] 25/05/12 18:26:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0a242e4-6b15-4f5b-8781-2b896cb8f7bb
2025-05-12 18:26:44,093 - SparkScheduler - ERROR - [discover_feed_generator] Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
2025-05-12 18:26:44,094 - SparkScheduler - ERROR - [discover_feed_generator] The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
2025-05-12 18:26:44,094 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
2025-05-12 18:26:44,095 - SparkScheduler - ERROR - [discover_feed_generator] :: resolving dependencies :: org.apache.spark#spark-submit-parent-4ca5fdf4-c946-4473-bd85-1415cabc1ebb;1.0
2025-05-12 18:26:44,096 - SparkScheduler - ERROR - [discover_feed_generator] confs: [default]
2025-05-12 18:26:44,097 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
2025-05-12 18:26:44,098 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#mongodb-driver-sync;4.0.5 in central
2025-05-12 18:26:44,099 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#bson;4.0.5 in central
2025-05-12 18:26:44,100 - SparkScheduler - ERROR - [discover_feed_generator] found org.mongodb#mongodb-driver-core;4.0.5 in central
2025-05-12 18:26:44,101 - SparkScheduler - ERROR - [discover_feed_generator] :: resolution report :: resolve 264ms :: artifacts dl 15ms
2025-05-12 18:26:44,101 - SparkScheduler - ERROR - [discover_feed_generator] :: modules in use:
2025-05-12 18:26:44,103 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#bson;4.0.5 from central in [default]
2025-05-12 18:26:44,103 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
2025-05-12 18:26:44,104 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
2025-05-12 18:26:44,104 - SparkScheduler - ERROR - [discover_feed_generator] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
2025-05-12 18:26:44,105 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:26:44,105 - SparkScheduler - ERROR - [discover_feed_generator] |                  |            modules            ||   artifacts   |
2025-05-12 18:26:44,106 - SparkScheduler - ERROR - [discover_feed_generator] |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
2025-05-12 18:26:44,107 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:26:44,107 - SparkScheduler - ERROR - [discover_feed_generator] |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
2025-05-12 18:26:44,108 - SparkScheduler - ERROR - [discover_feed_generator] ---------------------------------------------------------------------
2025-05-12 18:26:44,108 - SparkScheduler - ERROR - [discover_feed_generator] :: retrieving :: org.apache.spark#spark-submit-parent-4ca5fdf4-c946-4473-bd85-1415cabc1ebb
2025-05-12 18:26:44,109 - SparkScheduler - ERROR - [discover_feed_generator] confs: [default]
2025-05-12 18:26:44,110 - SparkScheduler - ERROR - [discover_feed_generator] 0 artifacts copied, 4 already retrieved (0kB/11ms)
2025-05-12 18:26:44,111 - SparkScheduler - INFO - Job discover_feed_generator completed successfully
2025-05-12 18:26:44,111 - SparkScheduler - INFO - Job discover_feed_generator duration: 14.46 seconds
